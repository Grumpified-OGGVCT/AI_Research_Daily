[
  {
    "arxiv_id": "2510.20692v1",
    "title": "Exploring Large Language Models for Access Control Policy Synthesis and   Summarization",
    "summary": "Cloud computing is ubiquitous, with a growing number of services being hosted on the cloud every day. Typical cloud compute systems allow administrators to write policies implementing access control rules which specify how access to private data is governed. These policies must be manually written, and due to their complexity can often be error prone. Moreover, existing policies often implement complex access control specifications and thus can be difficult to precisely analyze in determining their behavior works exactly as intended. Recently, Large Language Models (LLMs) have shown great success in automated code synthesis and summarization. Given this success, they could potentially be used for automatically generating access control policies or aid in understanding existing policies. In this paper, we explore the effectiveness of LLMs for access control policy synthesis and summarization. Specifically, we first investigate diverse LLMs for access control policy synthesis, finding that: although LLMs can effectively generate syntactically correct policies, they have permissiveness issues, generating policies equivalent to the given specification 45.8% of the time for non-reasoning LLMs, and 93.7% of the time for reasoning LLMs. We then investigate how LLMs can be used to analyze policies by introducing a novel semantic-based request summarization approach which leverages LLMs to generate a precise characterization of the requests allowed by a policy. Our results show that while there are significant hurdles in leveraging LLMs for automated policy generation, LLMs show promising results when combined with symbolic approaches in analyzing existing policies.",
    "authors": [
      "Adarsh Vatsa",
      "Bethel Hall",
      "William Eiers"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.FL",
      "D.4.6; D.2.4; I.2.2; I.2.7; F.3.1; F.4.3"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20692v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20692v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.9
  },
  {
    "arxiv_id": "2510.20639v1",
    "title": "Better Tokens for Better 3D: Advancing Vision-Language Modeling in 3D   Medical Imaging",
    "summary": "Recent progress in vision-language modeling for 3D medical imaging has been fueled by large-scale computed tomography (CT) corpora with paired free-text reports, stronger architectures, and powerful pretrained models. This has enabled applications such as automated report generation and text-conditioned 3D image synthesis. Yet, current approaches struggle with high-resolution, long-sequence volumes: contrastive pretraining often yields vision encoders that are misaligned with clinical language, and slice-wise tokenization blurs fine anatomy, reducing diagnostic performance on downstream tasks. We introduce BTB3D (Better Tokens for Better 3D), a causal convolutional encoder-decoder that unifies 2D and 3D training and inference while producing compact, frequency-aware volumetric tokens. A three-stage training curriculum enables (i) local reconstruction, (ii) overlapping-window tiling, and (iii) long-context decoder refinement, during which the model learns from short slice excerpts yet generalizes to scans exceeding 300 slices without additional memory overhead. BTB3D sets a new state-of-the-art on two key tasks: it improves BLEU scores and increases clinical F1 by 40% over CT2Rep, CT-CHAT, and Merlin for report generation; and it reduces FID by 75% and halves FVD compared to GenerateCT and MedSyn for text-to-CT synthesis, producing anatomically consistent 512*512*241 volumes. These results confirm that precise three-dimensional tokenization, rather than larger language backbones alone, is essential for scalable vision-language modeling in 3D medical imaging. The codebase is available at: https://github.com/ibrahimethemhamamci/BTB3D",
    "authors": [
      "Ibrahim Ethem Hamamci",
      "Sezgin Er",
      "Suprosanna Shit",
      "Hadrien Reynaud",
      "Dong Yang",
      "Pengfei Guo",
      "Marc Edgar",
      "Daguang Xu",
      "Bernhard Kainz",
      "Bjoern Menze"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20639v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20639v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.89
  },
  {
    "arxiv_id": "2510.20518v1",
    "title": "Adversary-Aware Private Inference over Wireless Channels",
    "summary": "AI-based sensing at wireless edge devices has the potential to significantly enhance Artificial Intelligence (AI) applications, particularly for vision and perception tasks such as in autonomous driving and environmental monitoring. AI systems rely both on efficient model learning and inference. In the inference phase, features extracted from sensing data are utilized for prediction tasks (e.g., classification or regression). In edge networks, sensors and model servers are often not co-located, which requires communication of features. As sensitive personal data can be reconstructed by an adversary, transformation of the features are required to reduce the risk of privacy violations. While differential privacy mechanisms provide a means of protecting finite datasets, protection of individual features has not been addressed. In this paper, we propose a novel framework for privacy-preserving AI-based sensing, where devices apply transformations of extracted features before transmission to a model server.",
    "authors": [
      "Mohamed Seif",
      "Malcolm Egan",
      "Andrea J. Goldsmith",
      "H. Vincent Poor"
    ],
    "categories": [
      "cs.IT",
      "cs.CR",
      "cs.LG",
      "math.IT"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20518v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20518v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.89
  },
  {
    "arxiv_id": "2510.20818v1",
    "title": "VAMOS: A Hierarchical Vision-Language-Action Model for   Capability-Modulated and Steerable Navigation",
    "summary": "A fundamental challenge in robot navigation lies in learning policies that generalize across diverse environments while conforming to the unique physical constraints and capabilities of a specific embodiment (e.g., quadrupeds can walk up stairs, but rovers cannot). We propose VAMOS, a hierarchical VLA that decouples semantic planning from embodiment grounding: a generalist planner learns from diverse, open-world data, while a specialist affordance model learns the robot's physical constraints and capabilities in safe, low-cost simulation. We enabled this separation by carefully designing an interface that lets a high-level planner propose candidate paths directly in image space that the affordance model then evaluates and re-ranks. Our real-world experiments show that VAMOS achieves higher success rates in both indoor and complex outdoor navigation than state-of-the-art model-based and end-to-end learning methods. We also show that our hierarchical design enables cross-embodied navigation across legged and wheeled robots and is easily steerable using natural language. Real-world ablations confirm that the specialist model is key to embodiment grounding, enabling a single high-level planner to be deployed across physically distinct wheeled and legged robots. Finally, this model significantly enhances single-robot reliability, achieving 3X higher success rates by rejecting physically infeasible plans. Website: https://vamos-vla.github.io/",
    "authors": [
      "Mateo Guaman Castro",
      "Sidharth Rajagopal",
      "Daniel Gorbatov",
      "Matt Schmittle",
      "Rohan Baijal",
      "Octi Zhang",
      "Rosario Scalise",
      "Sidharth Talia",
      "Emma Romig",
      "Celso de Melo",
      "Byron Boots",
      "Abhishek Gupta"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20818v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20818v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.88
  },
  {
    "arxiv_id": "2510.20543v1",
    "title": "The Dog the Cat Chased Stumped the Model: Measuring When Language Models   Abandon Structure for Shortcuts",
    "summary": "When language models correctly parse \"The cat that the dog chased meowed,\" are they analyzing syntax or simply familiar with dogs chasing cats? Despite extensive benchmarking, we lack methods to distinguish structural understanding from semantic pattern matching. We introduce CenterBench, a dataset of 9,720 comprehension questions on center-embedded sentences (like \"The cat [that the dog chased] meowed\") where relative clauses nest recursively, creating processing demands from simple to deeply nested structures. Each sentence has a syntactically identical but semantically implausible counterpart (e.g., mailmen prescribe medicine, doctors deliver mail) and six comprehension questions testing surface understanding, syntactic dependencies, and causal reasoning. Testing six models reveals that performance gaps between plausible and implausible sentences widen systematically with complexity, with models showing median gaps up to 26.8 percentage points, quantifying when they abandon structural analysis for semantic associations. Notably, semantic plausibility harms performance on questions about resulting actions, where following causal relationships matters more than semantic coherence. Reasoning models improve accuracy but their traces show semantic shortcuts, overthinking, and answer refusal. Unlike models whose plausibility advantage systematically widens with complexity, humans shows variable semantic effects. CenterBench provides the first framework to identify when models shift from structural analysis to pattern matching.",
    "authors": [
      "Sangmitra Madhusudan",
      "Kaige Chen",
      "Ali Emami"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20543v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20543v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.86
  },
  {
    "arxiv_id": "2510.20460v1",
    "title": "Systematic Evaluation of Uncertainty Estimation Methods in Large   Language Models",
    "summary": "Large language models (LLMs) produce outputs with varying levels of uncertainty, and, just as often, varying levels of correctness; making their practical reliability far from guaranteed. To quantify this uncertainty, we systematically evaluate four approaches for confidence estimation in LLM outputs: VCE, MSP, Sample Consistency, and CoCoA (Vashurin et al., 2025). For the evaluation of the approaches, we conduct experiments on four question-answering tasks using a state-of-the-art open-source LLM. Our results show that each uncertainty metric captures a different facet of model confidence and that the hybrid CoCoA approach yields the best reliability overall, improving both calibration and discrimination of correct answers. We discuss the trade-offs of each method and provide recommendations for selecting uncertainty measures in LLM applications.",
    "authors": [
      "Christian Hobelsberger",
      "Theresa Winner",
      "Andreas Nawroth",
      "Oliver Mitevski",
      "Anna-Carolina Haensch"
    ],
    "categories": [
      "cs.CL",
      "stat.AP",
      "stat.ME"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20460v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20460v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.84
  },
  {
    "arxiv_id": "2510.20683v1",
    "title": "A Scalable, Causal, and Energy Efficient Framework for Neural Decoding   with Spiking Neural Networks",
    "summary": "Brain-computer interfaces (BCIs) promise to enable vital functions, such as speech and prosthetic control, for individuals with neuromotor impairments. Central to their success are neural decoders, models that map neural activity to intended behavior. Current learning-based decoding approaches fall into two classes: simple, causal models that lack generalization, or complex, non-causal models that generalize and scale offline but struggle in real-time settings. Both face a common challenge, their reliance on power-hungry artificial neural network backbones, which makes integration into real-world, resource-limited systems difficult. Spiking neural networks (SNNs) offer a promising alternative. Because they operate causally these models are suitable for real-time use, and their low energy demands make them ideal for battery-constrained environments. To this end, we introduce Spikachu: a scalable, causal, and energy-efficient neural decoding framework based on SNNs. Our approach processes binned spikes directly by projecting them into a shared latent space, where spiking modules, adapted to the timing of the input, extract relevant features; these latent representations are then integrated and decoded to generate behavioral predictions. We evaluate our approach on 113 recording sessions from 6 non-human primates, totaling 43 hours of recordings. Our method outperforms causal baselines when trained on single sessions using between 2.26 and 418.81 times less energy. Furthermore, we demonstrate that scaling up training to multiple sessions and subjects improves performance and enables few-shot transfer to unseen sessions, subjects, and tasks. Overall, Spikachu introduces a scalable, online-compatible neural decoding framework based on SNNs, whose performance is competitive relative to state-of-the-art models while consuming orders of magnitude less energy.",
    "authors": [
      "Georgios Mentzelopoulos",
      "Ioannis Asmanis",
      "Konrad P. Kording",
      "Eva L. Dyer",
      "Kostas Daniilidis",
      "Flavia Vitale"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20683v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20683v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.78
  },
  {
    "arxiv_id": "2510.20606v1",
    "title": "Strategic Costs of Perceived Bias in Fair Selection",
    "summary": "Meritocratic systems, from admissions to hiring, aim to impartially reward skill and effort. Yet persistent disparities across race, gender, and class challenge this ideal. Some attribute these gaps to structural inequality; others to individual choice. We develop a game-theoretic model in which candidates from different socioeconomic groups differ in their perceived post-selection value--shaped by social context and, increasingly, by AI-powered tools offering personalized career or salary guidance. Each candidate strategically chooses effort, balancing its cost against expected reward; effort translates into observable merit, and selection is based solely on merit. We characterize the unique Nash equilibrium in the large-agent limit and derive explicit formulas showing how valuation disparities and institutional selectivity jointly determine effort, representation, social welfare, and utility. We further propose a cost-sensitive optimization framework that quantifies how modifying selectivity or perceived value can reduce disparities without compromising institutional goals. Our analysis reveals a perception-driven bias: when perceptions of post-selection value differ across groups, these differences translate into rational differences in effort, propagating disparities backward through otherwise \"fair\" selection processes. While the model is static, it captures one stage of a broader feedback cycle linking perceptions, incentives, and outcome--bridging rational-choice and structural explanations of inequality by showing how techno-social environments shape individual incentives in meritocratic systems.",
    "authors": [
      "L. Elisa Celis",
      "Lingxiao Huang",
      "Milind Sohoni",
      "Nisheeth K. Vishnoi"
    ],
    "categories": [
      "cs.GT",
      "cs.CY",
      "cs.LG",
      "econ.TH"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20606v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20606v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.77
  },
  {
    "arxiv_id": "2510.20605v1",
    "title": "OnlineSplatter: Pose-Free Online 3D Reconstruction for Free-Moving   Objects",
    "summary": "Free-moving object reconstruction from monocular video remains challenging, particularly without reliable pose or depth cues and under arbitrary object motion. We introduce OnlineSplatter, a novel online feed-forward framework generating high-quality, object-centric 3D Gaussians directly from RGB frames without requiring camera pose, depth priors, or bundle optimization. Our approach anchors reconstruction using the first frame and progressively refines the object representation through a dense Gaussian primitive field, maintaining constant computational cost regardless of video sequence length. Our core contribution is a dual-key memory module combining latent appearance-geometry keys with explicit directional keys, robustly fusing current frame features with temporally aggregated object states. This design enables effective handling of free-moving objects via spatial-guided memory readout and an efficient sparsification mechanism, ensuring comprehensive yet compact object coverage. Evaluations on real-world datasets demonstrate that OnlineSplatter significantly outperforms state-of-the-art pose-free reconstruction baselines, consistently improving with more observations while maintaining constant memory and runtime.",
    "authors": [
      "Mark He Huang",
      "Lin Geng Foo",
      "Christian Theobalt",
      "Ying Sun",
      "De Wen Soh"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.4.5; I.2.6"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20605v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20605v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.77
  },
  {
    "arxiv_id": "2510.20535v1",
    "title": "ARC-Encoder: learning compressed text representations for large language   models",
    "summary": "Recent techniques such as retrieval-augmented generation or chain-of-thought reasoning have led to longer contexts and increased inference costs. Context compression techniques can reduce these costs, but the most effective approaches require fine-tuning the target model or even modifying its architecture. This can degrade its general abilities when not used for this specific purpose. Here we explore an alternative approach: an encoder that compresses the context into continuous representations which replace token embeddings in decoder LLMs. First, we perform a systematic study of training strategies and architecture choices for the encoder. Our findings led to the design of an Adaptable text Representations Compressor, named ARC-Encoder, which outputs $x$-times fewer continuous representations (typically $x\\!\\in\\!\\{4,8\\}$) than text tokens. We evaluate ARC-Encoder across a variety of LLM usage scenarios, ranging from in-context learning to context window extension, on both instruct and base decoders. Results show that ARC-Encoder achieves state-of-the-art performance on several benchmarks while improving computational efficiency at inference. Finally, we demonstrate that our models can be adapted to multiple decoders simultaneously, allowing a single encoder to generalize across different decoder LLMs. This makes ARC-Encoder a flexible and efficient solution for portable encoders that work seamlessly with multiple LLMs. We release a training code at https://github.com/kyutai-labs/ARC-Encoder , fine-tuning dataset and pretrained models are available at https://huggingface.co/collections/kyutai/arc-encoders-68ee18787301407d60a57047 .",
    "authors": [
      "Hippolyte Pilchen",
      "Edouard Grave",
      "Patrick P\u00e9rez"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20535v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20535v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.77
  },
  {
    "arxiv_id": "2510.20310v1",
    "title": "Multi-Step Reasoning for Embodied Question Answering via Tool   Augmentation",
    "summary": "Embodied Question Answering (EQA) requires agents to explore 3D environments to obtain observations and answer questions related to the scene. Existing methods leverage VLMs to directly explore the environment and answer questions without explicit thinking or planning, which limits their reasoning ability and results in excessive or inefficient exploration as well as ineffective responses. In this paper, we introduce ToolEQA, an agent that integrates external tools with multi-step reasoning, where external tools can provide more useful information for completing the task, helping the model derive better exploration directions in the next step of reasoning and thus obtaining additional effective information. This enables ToolEQA to generate more accurate responses with a shorter exploration distance. To enhance the model's ability for tool-usage and multi-step reasoning, we further design a novel EQA data generation pipeline that automatically constructs large-scale EQA tasks with reasoning trajectories and corresponding answers. Based on the pipeline, we collect the EQA-RT dataset that contains about 18K tasks, divided into a training set EQA-RT-Train, and two test sets EQA-RT-Seen (scenes overlapping with the training set) and EQA-RT-Unseen (novel scenes). Experiments on EQA-RT-Seen and EQA-RT-Unseen show that ToolEQA improves the success rate by 9.2~20.2% over state-of-the-art baselines, while outperforming the zero-shot ToolEQA by 10% in success rate. In addition, ToolEQA also achieves state-of-the-art performance on the HM-EQA, OpenEQA, and EXPRESS-Bench datasets, demonstrating its generality. Our homepage see https://tooleqa.github.io.",
    "authors": [
      "Mingliang Zhai",
      "Hansheng Liang",
      "Xiaomeng Fan",
      "Zhi Gao",
      "Chuanhao Li",
      "Che Sun",
      "Xu Bin",
      "Yuwei Wu",
      "Yunde Jia"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20310v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20310v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.77
  },
  {
    "arxiv_id": "2510.20439v1",
    "title": "Explainable Benchmarking through the Lense of Concept Learning",
    "summary": "Evaluating competing systems in a comparable way, i.e., benchmarking them, is an undeniable pillar of the scientific method. However, system performance is often summarized via a small number of metrics. The analysis of the evaluation details and the derivation of insights for further development or use remains a tedious manual task with often biased results. Thus, this paper argues for a new type of benchmarking, which is dubbed explainable benchmarking. The aim of explainable benchmarking approaches is to automatically generate explanations for the performance of systems in a benchmark. We provide a first instantiation of this paradigm for knowledge-graph-based question answering systems. We compute explanations by using a novel concept learning approach developed for large knowledge graphs called PruneCEL. Our evaluation shows that PruneCEL outperforms state-of-the-art concept learners on the task of explainable benchmarking by up to 0.55 points F1 measure. A task-driven user study with 41 participants shows that in 80\\% of the cases, the majority of participants can accurately predict the behavior of a system based on our explanations. Our code and data are available at https://github.com/dice-group/PruneCEL/tree/K-cap2025",
    "authors": [
      "Quannian Zhang",
      "Michael R\u00f6der",
      "Nikit Srivastava",
      "N'Dah Jean Kouagou",
      "Axel-Cyrille Ngonga Ngomo"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20439v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20439v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.75
  },
  {
    "arxiv_id": "2510.20438v1",
    "title": "Dynamic Weight Adjustment for Knowledge Distillation: Leveraging Vision   Transformer for High-Accuracy Lung Cancer Detection and Real-Time Deployment",
    "summary": "This paper presents the FuzzyDistillViT-MobileNet model, a novel approach for lung cancer (LC) classification, leveraging dynamic fuzzy logic-driven knowledge distillation (KD) to address uncertainty and complexity in disease diagnosis. Unlike traditional models that rely on static KD with fixed weights, our method dynamically adjusts the distillation weight using fuzzy logic, enabling the student model to focus on high-confidence regions while reducing attention to ambiguous areas. This dynamic adjustment improves the model ability to handle varying uncertainty levels across different regions of LC images. We employ the Vision Transformer (ViT-B32) as the instructor model, which effectively transfers knowledge to the student model, MobileNet, enhancing the student generalization capabilities. The training process is further optimized using a dynamic wait adjustment mechanism that adapts the training procedure for improved convergence and performance. To enhance image quality, we introduce pixel-level image fusion improvement techniques such as Gamma correction and Histogram Equalization. The processed images (Pix1 and Pix2) are fused using a wavelet-based fusion method to improve image resolution and feature preservation. This fusion method uses the wavedec2 function to standardize images to a 224x224 resolution, decompose them into multi-scale frequency components, and recursively average coefficients at each level for better feature representation. To address computational efficiency, Genetic Algorithm (GA) is used to select the most suitable pre-trained student model from a pool of 12 candidates, balancing model performance with computational cost. The model is evaluated on two datasets, including LC25000 histopathological images (99.16% accuracy) and IQOTH/NCCD CT-scan images (99.54% accuracy), demonstrating robustness across different imaging domains.",
    "authors": [
      "Saif Ur Rehman Khan",
      "Muhammad Nabeel Asim",
      "Sebastian Vollmer",
      "Andreas Dengel"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20438v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20438v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.75
  },
  {
    "arxiv_id": "2510.20406v1",
    "title": "PointMapPolicy: Structured Point Cloud Processing for Multi-Modal   Imitation Learning",
    "summary": "Robotic manipulation systems benefit from complementary sensing modalities, where each provides unique environmental information. Point clouds capture detailed geometric structure, while RGB images provide rich semantic context. Current point cloud methods struggle to capture fine-grained detail, especially for complex tasks, which RGB methods lack geometric awareness, which hinders their precision and generalization. We introduce PointMapPolicy, a novel approach that conditions diffusion policies on structured grids of points without downsampling. The resulting data type makes it easier to extract shape and spatial relationships from observations, and can be transformed between reference frames. Yet due to their structure in a regular grid, we enable the use of established computer vision techniques directly to 3D data. Using xLSTM as a backbone, our model efficiently fuses the point maps with RGB data for enhanced multi-modal perception. Through extensive experiments on the RoboCasa and CALVIN benchmarks and real robot evaluations, we demonstrate that our method achieves state-of-the-art performance across diverse manipulation tasks. The overview and demos are available on our project page: https://point-map.github.io/Point-Map/",
    "authors": [
      "Xiaogang Jia",
      "Qian Wang",
      "Anrui Wang",
      "Han A. Wang",
      "Bal\u00e1zs Gyenes",
      "Emiliyan Gospodinov",
      "Xinkai Jiang",
      "Ge Li",
      "Hongyi Zhou",
      "Weiran Liao",
      "Xi Huang",
      "Maximilian Beck",
      "Moritz Reuss",
      "Rudolf Lioutikov",
      "Gerhard Neumann"
    ],
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20406v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20406v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.75
  },
  {
    "arxiv_id": "2510.20709v1",
    "title": "Separating the what and how of compositional computation to enable reuse   and continual learning",
    "summary": "The ability to continually learn, retain and deploy skills to accomplish goals is a key feature of intelligent and efficient behavior. However, the neural mechanisms facilitating the continual learning and flexible (re-)composition of skills remain elusive. Here, we study continual learning and the compositional reuse of learned computations in recurrent neural network (RNN) models using a novel two-system approach: one system that infers what computation to perform, and one that implements how to perform it. We focus on a set of compositional cognitive tasks commonly studied in neuroscience. To construct the what system, we first show that a large family of tasks can be systematically described by a probabilistic generative model, where compositionality stems from a shared underlying vocabulary of discrete task epochs. The shared epoch structure makes these tasks inherently compositional. We first show that this compositionality can be systematically described by a probabilistic generative model. Furthermore, We develop an unsupervised online learning approach that can learn this model on a single-trial basis, building its vocabulary incrementally as it is exposed to new tasks, and inferring the latent epoch structure as a time-varying computational context within a trial. We implement the how system as an RNN whose low-rank components are composed according to the context inferred by the what system. Contextual inference facilitates the creation, learning, and reuse of low-rank RNN components as new tasks are introduced sequentially, enabling continual learning without catastrophic forgetting. Using an example task set, we demonstrate the efficacy and competitive performance of this two-system learning framework, its potential for forward and backward transfer, as well as fast compositional generalization to unseen tasks.",
    "authors": [
      "Haozhe Shan",
      "Sun Minni",
      "Lea Duncker"
    ],
    "categories": [
      "cs.LG",
      "q-bio.NC"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20709v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20709v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.74
  },
  {
    "arxiv_id": "2510.20771v1",
    "title": "AlphaFlow: Understanding and Improving MeanFlow Models",
    "summary": "MeanFlow has recently emerged as a powerful framework for few-step generative modeling trained from scratch, but its success is not yet fully understood. In this work, we show that the MeanFlow objective naturally decomposes into two parts: trajectory flow matching and trajectory consistency. Through gradient analysis, we find that these terms are strongly negatively correlated, causing optimization conflict and slow convergence. Motivated by these insights, we introduce $\\alpha$-Flow, a broad family of objectives that unifies trajectory flow matching, Shortcut Model, and MeanFlow under one formulation. By adopting a curriculum strategy that smoothly anneals from trajectory flow matching to MeanFlow, $\\alpha$-Flow disentangles the conflicting objectives, and achieves better convergence. When trained from scratch on class-conditional ImageNet-1K 256x256 with vanilla DiT backbones, $\\alpha$-Flow consistently outperforms MeanFlow across scales and settings. Our largest $\\alpha$-Flow-XL/2+ model achieves new state-of-the-art results using vanilla DiT backbones, with FID scores of 2.58 (1-NFE) and 2.15 (2-NFE).",
    "authors": [
      "Huijie Zhang",
      "Aliaksandr Siarohin",
      "Willi Menapace",
      "Michael Vasilkovsky",
      "Sergey Tulyakov",
      "Qing Qu",
      "Ivan Skorokhodov"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20771v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20771v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.73
  },
  {
    "arxiv_id": "2510.20607v1",
    "title": "Generalizable Reasoning through Compositional Energy Minimization",
    "summary": "Generalization is a key challenge in machine learning, specifically in reasoning tasks, where models are expected to solve problems more complex than those encountered during training. Existing approaches typically train reasoning models in an end-to-end fashion, directly mapping input instances to solutions. While this allows models to learn useful heuristics from data, it often results in limited generalization beyond the training distribution. In this work, we propose a novel approach to reasoning generalization by learning energy landscapes over the solution spaces of smaller, more tractable subproblems. At test time, we construct a global energy landscape for a given problem by combining the energy functions of multiple subproblems. This compositional approach enables the incorporation of additional constraints during inference, allowing the construction of energy landscapes for problems of increasing difficulty. To improve the sample quality from this newly constructed energy landscape, we introduce Parallel Energy Minimization (PEM). We evaluate our approach on a wide set of reasoning problems. Our method outperforms existing state-of-the-art methods, demonstrating its ability to generalize to larger and more complex problems. Project website can be found at: https://alexoarga.github.io/compositional_reasoning/",
    "authors": [
      "Alexandru Oarga",
      "Yilun Du"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20607v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20607v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.73
  },
  {
    "arxiv_id": "2510.20596v1",
    "title": "Unsupervised Domain Adaptation via Similarity-based Prototypes for   Cross-Modality Segmentation",
    "summary": "Deep learning models have achieved great success on various vision challenges, but a well-trained model would face drastic performance degradation when applied to unseen data. Since the model is sensitive to domain shift, unsupervised domain adaptation attempts to reduce the domain gap and avoid costly annotation of unseen domains. This paper proposes a novel framework for cross-modality segmentation via similarity-based prototypes. In specific, we learn class-wise prototypes within an embedding space, then introduce a similarity constraint to make these prototypes representative for each semantic class while separable from different classes. Moreover, we use dictionaries to store prototypes extracted from different images, which prevents the class-missing problem and enables the contrastive learning of prototypes, and further improves performance. Extensive experiments show that our method achieves better results than other state-of-the-art methods.",
    "authors": [
      "Ziyu Ye",
      "Chen Ju",
      "Chaofan Ma",
      "Xiaoyun Zhang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20596v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20596v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.73
  },
  {
    "arxiv_id": "2510.20479v1",
    "title": "RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via   Hierarchical Model Merging",
    "summary": "We unveil that internal representations in large language models (LLMs) serve as reliable proxies of learned knowledge, and propose RECALL, a novel representation-aware model merging framework for continual learning without access to historical data. RECALL computes inter-model similarity from layer-wise hidden representations over clustered typical samples, and performs adaptive, hierarchical parameter fusion to align knowledge across models. This design enables the preservation of domain-general features in shallow layers while allowing task-specific adaptation in deeper layers. Unlike prior methods that require task labels or incur performance trade-offs, RECALL achieves seamless multi-domain integration and strong resistance to catastrophic forgetting. Extensive experiments across five NLP tasks and multiple continual learning scenarios show that RECALL outperforms baselines in both knowledge retention and generalization, providing a scalable and data-free solution for evolving LLMs.",
    "authors": [
      "Bowen Wang",
      "Haiyuan Wan",
      "Liwen Shi",
      "Chen Yang",
      "Peng He",
      "Yue Ma",
      "Haochen Han",
      "Wenhao Li",
      "Tiao Tan",
      "Yongjian Li",
      "Fangming Liu",
      "Yifan Gong",
      "Sheng Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20479v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20479v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.73
  },
  {
    "arxiv_id": "2510.20531v1",
    "title": "Fake-in-Facext: Towards Fine-Grained Explainable DeepFake Analysis",
    "summary": "The advancement of Multimodal Large Language Models (MLLMs) has bridged the gap between vision and language tasks, enabling the implementation of Explainable DeepFake Analysis (XDFA). However, current methods suffer from a lack of fine-grained awareness: the description of artifacts in data annotation is unreliable and coarse-grained, and the models fail to support the output of connections between textual forgery explanations and the visual evidence of artifacts, as well as the input of queries for arbitrary facial regions. As a result, their responses are not sufficiently grounded in Face Visual Context (Facext). To address this limitation, we propose the Fake-in-Facext (FiFa) framework, with contributions focusing on data annotation and model construction. We first define a Facial Image Concept Tree (FICT) to divide facial images into fine-grained regional concepts, thereby obtaining a more reliable data annotation pipeline, FiFa-Annotator, for forgery explanation. Based on this dedicated data annotation, we introduce a novel Artifact-Grounding Explanation (AGE) task, which generates textual forgery explanations interleaved with segmentation masks of manipulated artifacts. We propose a unified multi-task learning architecture, FiFa-MLLM, to simultaneously support abundant multimodal inputs and outputs for fine-grained Explainable DeepFake Analysis. With multiple auxiliary supervision tasks, FiFa-MLLM can outperform strong baselines on the AGE task and achieve SOTA performance on existing XDFA datasets. The code and data will be made open-source at https://github.com/lxq1000/Fake-in-Facext.",
    "authors": [
      "Lixiong Qin",
      "Yang Zhang",
      "Mei Wang",
      "Jiani Hu",
      "Weihong Deng",
      "Weiran Xu"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20531v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20531v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.72
  },
  {
    "arxiv_id": "2510.20603v1",
    "title": "What Defines Good Reasoning in LLMs? Dissecting Reasoning Steps with   Multi-Aspect Evaluation",
    "summary": "Evaluating large language models (LLMs) on final-answer correctness is the dominant paradigm. This approach, however, provides a coarse signal for model improvement and overlooks the quality of the underlying reasoning process. We argue that a more granular evaluation of reasoning offers a more effective path to building robust models. We decompose reasoning quality into two dimensions: relevance and coherence. Relevance measures if a step is grounded in the problem; coherence measures if it follows logically from prior steps. To measure these aspects reliably, we introduce causal stepwise evaluation (CaSE). This method assesses each reasoning step using only its preceding context, which avoids hindsight bias. We validate CaSE against human judgments on our new expert-annotated benchmarks, MRa-GSM8K and MRa-MATH. More importantly, we show that curating training data with CaSE-evaluated relevance and coherence directly improves final task performance. Our work provides a scalable framework for analyzing, debugging, and improving LLM reasoning, demonstrating the practical value of moving beyond validity checks.",
    "authors": [
      "Heejin Do",
      "Jaehui Hwang",
      "Dongyoon Han",
      "Seong Joon Oh",
      "Sangdoo Yun"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20603v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20603v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.71
  },
  {
    "arxiv_id": "2510.20579v1",
    "title": "Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal   Evidence",
    "summary": "Most video reasoning models only generate textual reasoning traces without indicating when and where key evidence appears. Recent models such as OpenAI-o3 have sparked wide interest in evidence-centered reasoning for images, yet extending this ability to videos is more challenging, as it requires joint temporal tracking and spatial localization across dynamic scenes. We introduce Open-o3 Video, a non-agent framework that integrates explicit spatio-temporal evidence into video reasoning, and carefully collect training data and design training strategies to address the aforementioned challenges. The model highlights key timestamps, objects, and bounding boxes alongside its answers, allowing reasoning to be grounded in concrete visual observations. To enable this functionality, we first curate and build two high-quality datasets, STGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructed temporal and spatial annotations, since most existing datasets offer either temporal spans for videos or spatial boxes on images, lacking unified spatio-temporal supervision and reasoning traces. Then, we adopt a cold-start reinforcement learning strategy with multiple specially designed rewards that jointly encourage answer accuracy, temporal alignment, and spatial precision. On V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance, raising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistent improvements are also observed on a broad range of video understanding benchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyond accuracy, the reasoning traces produced by Open-o3 Video also provide valuable signals for test-time scaling, enabling confidence-aware verification and improving answer reliability.",
    "authors": [
      "Jiahao Meng",
      "Xiangtai Li",
      "Haochen Wang",
      "Yue Tan",
      "Tao Zhang",
      "Lingdong Kong",
      "Yunhai Tong",
      "Anran Wang",
      "Zhiyang Teng",
      "Yujing Wang",
      "Zhuochen Wang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20579v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20579v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.71
  },
  {
    "arxiv_id": "2510.20448v1",
    "title": "MolBridge: Atom-Level Joint Graph Refinement for Robust Drug-Drug   Interaction Event Prediction",
    "summary": "Drug combinations offer therapeutic benefits but also carry the risk of adverse drug-drug interactions (DDIs), especially under complex molecular structures. Accurate DDI event prediction requires capturing fine-grained inter-drug relationships, which are critical for modeling metabolic mechanisms such as enzyme-mediated competition. However, existing approaches typically rely on isolated drug representations and fail to explicitly model atom-level cross-molecular interactions, limiting their effectiveness across diverse molecular complexities and DDI type distributions. To address these limitations, we propose MolBridge, a novel atom-level joint graph refinement framework for robust DDI event prediction. MolBridge constructs a joint graph that integrates atomic structures of drug pairs, enabling direct modeling of inter-drug associations. A central challenge in such joint graph settings is the potential loss of information caused by over-smoothing when modeling long-range atomic dependencies. To overcome this, we introduce a structure consistency module that iteratively refines node features while preserving the global structural context. This joint design allows MolBridge to effectively learn both local and global interaction outperforms state-of-the-art baselines, achieving superior performance across long-tail and inductive scenarios. patterns, yielding robust representations across both frequent and rare DDI types. Extensive experiments on two benchmark datasets show that MolBridge consistently. These results demonstrate the advantages of fine-grained graph refinement in improving the accuracy, robustness, and mechanistic interpretability of DDI event prediction.This work contributes to Web Mining and Content Analysis by developing graph-based methods for mining and analyzing drug-drug interaction networks.",
    "authors": [
      "Xuan Lin",
      "Aocheng Ding",
      "Tengfei Ma",
      "Hua Liang",
      "Zhe Quan"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20448v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20448v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.71
  },
  {
    "arxiv_id": "2510.20780v1",
    "title": "Are Large Reasoning Models Good Translation Evaluators? Analysis and   Performance Boost",
    "summary": "Recent advancements in large reasoning models (LRMs) have introduced an intermediate \"thinking\" process prior to generating final answers, improving their reasoning capabilities on complex downstream tasks. However, the potential of LRMs as evaluators for machine translation (MT) quality remains underexplored. We provides the first systematic analysis of LRM-as-a-judge in MT evaluation. We identify key challenges, revealing LRMs require tailored evaluation materials, tend to \"overthink\" simpler instances and have issues with scoring mechanisms leading to overestimation. To address these, we propose to calibrate LRM thinking by training them on synthetic, human-like thinking trajectories. Our experiments on WMT24 Metrics benchmarks demonstrate that this approach largely reduces thinking budgets by ~35x while concurrently improving evaluation performance across different LRM scales from 7B to 32B (e.g., R1-Distill-Qwen-7B achieves a +8.7 correlation point improvement). These findings highlight the potential of efficiently calibrated LRMs to advance fine-grained automatic MT evaluation.",
    "authors": [
      "Runzhe Zhan",
      "Zhihong Huang",
      "Xinyi Yang",
      "Lidia S. Chao",
      "Min Yang",
      "Derek F. Wong"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20780v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20780v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2510.20566v1",
    "title": "AdaDoS: Adaptive DoS Attack via Deep Adversarial Reinforcement Learning   in SDN",
    "summary": "Existing defence mechanisms have demonstrated significant effectiveness in mitigating rule-based Denial-of-Service (DoS) attacks, leveraging predefined signatures and static heuristics to identify and block malicious traffic. However, the emergence of AI-driven techniques presents new challenges to SDN security, potentially compromising the efficacy of existing defence mechanisms. In this paper, we introduce~AdaDoS, an adaptive attack model that disrupt network operations while evading detection by existing DoS-based detectors through adversarial reinforcement learning (RL). Specifically, AdaDoS models the problem as a competitive game between an attacker, whose goal is to obstruct network traffic without being detected, and a detector, which aims to identify malicious traffic. AdaDoS can solve this game by dynamically adjusting its attack strategy based on feedback from the SDN and the detector. Additionally, recognising that attackers typically have less information than defenders, AdaDoS formulates the DoS-like attack as a partially observed Markov decision process (POMDP), with the attacker having access only to delay information between attacker and victim nodes. We address this challenge with a novel reciprocal learning module, where the student agent, with limited observations, enhances its performance by learning from the teacher agent, who has full observational capabilities in the SDN environment. AdaDoS represents the first application of RL to develop DoS-like attack sequences, capable of adaptively evading both machine learning-based and rule-based DoS-like attack detectors.",
    "authors": [
      "Wei Shao",
      "Yuhao Wang",
      "Rongguang He",
      "Muhammad Ejaz Ahmed",
      "Seyit Camtepe"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20566v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20566v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2510.20539v1",
    "title": "Blur2seq: Blind Deblurring and Camera Trajectory Estimation from a   Single Camera Motion-blurred Image",
    "summary": "Motion blur caused by camera shake, particularly under large or rotational movements, remains a major challenge in image restoration. We propose a deep learning framework that jointly estimates the latent sharp image and the underlying camera motion trajectory from a single blurry image. Our method leverages the Projective Motion Blur Model (PMBM), implemented efficiently using a differentiable blur creation module compatible with modern networks. A neural network predicts a full 3D rotation trajectory, which guides a model-based restoration network trained end-to-end. This modular architecture provides interpretability by revealing the camera motion that produced the blur. Moreover, this trajectory enables the reconstruction of the sequence of sharp images that generated the observed blurry image. To further refine results, we optimize the trajectory post-inference via a reblur loss, improving consistency between the blurry input and the restored output. Extensive experiments show that our method achieves state-of-the-art performance on both synthetic and real datasets, particularly in cases with severe or spatially variant blur, where end-to-end deblurring networks struggle.   Code and trained models are available at https://github.com/GuillermoCarbajal/Blur2Seq/",
    "authors": [
      "Guillermo Carbajal",
      "Andr\u00e9s Almansa",
      "Pablo Mus\u00e9"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20539v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20539v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2510.20512v1",
    "title": "EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion   Personalization",
    "summary": "Recent advances in accelerating text-to-image (T2I) diffusion models have enabled the synthesis of high-fidelity images even in a single step. However, personalizing these models to incorporate novel concepts remains a challenge due to the limited capacity of one-step models to capture new concept distributions effectively. We propose a bidirectional concept distillation framework, EchoDistill, to enable one-step diffusion personalization (1-SDP). Our approach involves an end-to-end training process where a multi-step diffusion model (teacher) and a one-step diffusion model (student) are trained simultaneously. The concept is first distilled from the teacher model to the student, and then echoed back from the student to the teacher. During the EchoDistill, we share the text encoder between the two models to ensure consistent semantic understanding. Following this, the student model is optimized with adversarial losses to align with the real image distribution and with alignment losses to maintain consistency with the teacher's output. Furthermore, we introduce the bidirectional echoing refinement strategy, wherein the student model leverages its faster generation capability to feedback to the teacher model. This bidirectional concept distillation mechanism not only enhances the student ability to personalize novel concepts but also improves the generative quality of the teacher model. Our experiments demonstrate that this collaborative framework significantly outperforms existing personalization methods over the 1-SDP setup, establishing a novel paradigm for rapid and effective personalization in T2I diffusion models.",
    "authors": [
      "Yixiong Yang",
      "Tao Wu",
      "Senmao Li",
      "Shiqi Yang",
      "Yaxing Wang",
      "Joost van de Weijer",
      "Kai Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20512v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20512v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2510.20345v1",
    "title": "LLM-empowered knowledge graph construction: A survey",
    "summary": "Knowledge Graphs (KGs) have long served as a fundamental infrastructure for structured knowledge representation and reasoning. With the advent of Large Language Models (LLMs), the construction of KGs has entered a new paradigm-shifting from rule-based and statistical pipelines to language-driven and generative frameworks. This survey provides a comprehensive overview of recent progress in LLM-empowered knowledge graph construction, systematically analyzing how LLMs reshape the classical three-layered pipeline of ontology engineering, knowledge extraction, and knowledge fusion.   We first revisit traditional KG methodologies to establish conceptual foundations, and then review emerging LLM-driven approaches from two complementary perspectives: schema-based paradigms, which emphasize structure, normalization, and consistency; and schema-free paradigms, which highlight flexibility, adaptability, and open discovery. Across each stage, we synthesize representative frameworks, analyze their technical mechanisms, and identify their limitations.   Finally, the survey outlines key trends and future research directions, including KG-based reasoning for LLMs, dynamic knowledge memory for agentic systems, and multimodal KG construction. Through this systematic review, we aim to clarify the evolving interplay between LLMs and knowledge graphs, bridging symbolic knowledge engineering and neural semantic understanding toward the development of adaptive, explainable, and intelligent knowledge systems.",
    "authors": [
      "Haonan Bian"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20345v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20345v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2510.20718v1",
    "title": "Unsupervised Anomaly Prediction with N-BEATS and Graph Neural Network in   Multi-variate Semiconductor Process Time Series",
    "summary": "Semiconductor manufacturing is an extremely complex and precision-driven process, characterized by thousands of interdependent parameters collected across diverse tools and process steps. Multi-variate time-series analysis has emerged as a critical field for real-time monitoring and fault detection in such environments. However, anomaly prediction in semiconductor fabrication presents several critical challenges, including high dimensionality of sensor data and severe class imbalance due to the rarity of true faults. Furthermore, the complex interdependencies between variables complicate both anomaly prediction and root-cause-analysis. This paper proposes two novel approaches to advance the field from anomaly detection to anomaly prediction, an essential step toward enabling real-time process correction and proactive fault prevention. The proposed anomaly prediction framework contains two main stages: (a) training a forecasting model on a dataset assumed to contain no anomalies, and (b) performing forecast on unseen time series data. The forecast is compared with the forecast of the trained signal. Deviations beyond a predefined threshold are flagged as anomalies. The two approaches differ in the forecasting model employed. The first assumes independence between variables by utilizing the N-BEATS model for univariate time series forecasting. The second lifts this assumption by utilizing a Graph Neural Network (GNN) to capture inter-variable relationships. Both models demonstrate strong forecasting performance up to a horizon of 20 time points and maintain stable anomaly prediction up to 50 time points. The GNN consistently outperforms the N-BEATS model while requiring significantly fewer trainable parameters and lower computational cost. These results position the GNN as promising solution for online anomaly forecasting to be deployed in manufacturing environments.",
    "authors": [
      "Daniel Sorensen",
      "Bappaditya Dey",
      "Minjin Hwang",
      "Sandip Halder"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.0; J.6"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20718v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20718v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2510.20542v1",
    "title": "A Unified Framework for Zero-Shot Reinforcement Learning",
    "summary": "Zero-shot reinforcement learning (RL) has emerged as a setting for developing general agents in an unsupervised manner, capable of solving downstream tasks without additional training or planning at test-time. Unlike conventional RL, which optimizes policies for a fixed reward, zero-shot RL requires agents to encode representations rich enough to support immediate adaptation to any objective, drawing parallels to vision and language foundation models. Despite growing interest, the field lacks a common analytical lens.   We present the first unified framework for zero-shot RL. Our formulation introduces a consistent notation and taxonomy that organizes existing approaches and allows direct comparison between them. Central to our framework is the classification of algorithms into two families: direct representations, which learn end-to-end mappings from rewards to policies, and compositional representations, which decompose the representation leveraging the substructure of the value function. Within this framework, we highlight shared principles and key differences across methods, and we derive an extended bound for successor-feature methods, offering a new perspective on their performance in the zero-shot regime. By consolidating existing work under a common lens, our framework provides a principled foundation for future research in zero-shot RL and outlines a clear path toward developing more general agents.",
    "authors": [
      "Jacopo Di Ventura",
      "Jan Felix Kleuker",
      "Aske Plaat",
      "Thomas Moerland"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20542v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20542v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2510.20449v1",
    "title": "LM-mixup: Text Data Augmentation via Language Model based Mixup",
    "summary": "Instruction tuning is crucial for aligning Large Language Models (LLMs), yet the quality of instruction-following data varies significantly. While high-quality data is paramount, it is often scarce; conversely, abundant low-quality data is frequently discarded, leading to substantial information loss. Existing data augmentation methods struggle to augment this low-quality data effectively, and the evaluation of such techniques remains poorly defined. To address this, we formally define the task of Instruction Distillation: distilling multiple low-quality and redundant inputs into high-quality and coherent instruction-output pairs. Specifically, we introduce a comprehensive data construction pipeline to create MIXTURE, a 144K-sample dataset pairing low-quality or semantically redundant imperfect instruction clusters with their high-quality distillations. We then introduce LM-Mixup, by first performing supervised fine-tuning on MIXTURE and then optimizing it with reinforcement learning. This process uses three complementary reward signals: quality, semantic alignment, and format compliance, via Group Relative Policy Optimization (GRPO). We demonstrate that LM-Mixup effectively augments imperfect datasets: fine-tuning LLMs on its distilled data, which accounts for only about 3% of the entire dataset, not only surpasses full-dataset training but also competes with state-of-the-art high-quality data selection methods across multiple benchmarks. Our work establishes that low-quality data is a valuable resource when properly distilled and augmented with LM-Mixup, significantly enhancing the efficiency and performance of instruction-tuned LLMs.",
    "authors": [
      "Zhijie Deng",
      "Zhouan Shen",
      "Ling Li",
      "Yao Zhou",
      "Zhaowei Zhu",
      "Yanji He",
      "Wei Wang",
      "Jiaheng Wei"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20449v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20449v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2510.20428v1",
    "title": "An Empirical Study of Sample Selection Strategies for Large Language   Model Repair",
    "summary": "Large language models (LLMs) are increasingly deployed in real-world systems, yet they can produce toxic or biased outputs that undermine safety and trust. Post-hoc model repair provides a practical remedy, but the high cost of parameter updates motivates selective use of repair data. Despite extensive prior work on data selection for model training, it remains unclear which sampling criteria are most effective and efficient when applied specifically to behavioral repair of large generative models. Our study presents a systematic analysis of sample prioritization strategies for LLM repair. We evaluate five representative selection methods, including random sampling, K-Center, gradient-norm-based selection(GraNd), stratified coverage (CCS), and a Semantic-Aware Prioritized Sampling (SAPS) approach we proposed. Repair effectiveness and trade-offs are assessed through toxicity reduction, perplexity on WikiText-2 and LAMBADA, and three composite metrics: the Repair Proximity Score (RPS), the Overall Performance Score (OPS), and the Repair Efficiency Score (RES). Experimental results show that SAPS achieves the best balance between detoxification, utility preservation, and efficiency, delivering comparable or superior repair outcomes with substantially less data. Random sampling remains effective for large or robust models, while high-overhead methods such as CCS and GraNd provide limited benefit. The optimal data proportion depends on model scale and repair method, indicating that sample selection should be regarded as a tunable component of repair pipelines. Overall, these findings establish selection-based repair as an efficient and scalable paradigm for maintaining LLM reliability.",
    "authors": [
      "Xuran Li",
      "Jingyi Wang"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20428v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20428v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2510.20369v1",
    "title": "Ask a Strong LLM Judge when Your Reward Model is Uncertain",
    "summary": "Reward model (RM) plays a pivotal role in reinforcement learning with human feedback (RLHF) for aligning large language models (LLMs). However, classical RMs trained on human preferences are vulnerable to reward hacking and generalize poorly to out-of-distribution (OOD) inputs. By contrast, strong LLM judges equipped with reasoning capabilities demonstrate superior generalization, even without additional training, but incur significantly higher inference costs, limiting their applicability in online RLHF. In this work, we propose an uncertainty-based routing framework that efficiently complements a fast RM with a strong but costly LLM judge. Our approach formulates advantage estimation in policy gradient (PG) methods as pairwise preference classification, enabling principled uncertainty quantification to guide routing. Uncertain pairs are forwarded to the LLM judge, while confident ones are evaluated by the RM. Experiments on RM benchmarks demonstrate that our uncertainty-based routing strategy significantly outperforms random judge calling at the same cost, and downstream alignment results showcase its effectiveness in improving online RLHF.",
    "authors": [
      "Zhenghao Xu",
      "Qin Lu",
      "Qingru Zhang",
      "Liang Qiu",
      "Ilgee Hong",
      "Changlong Yu",
      "Wenlin Yao",
      "Yao Liu",
      "Haoming Jiang",
      "Lihong Li",
      "Hyokun Yun",
      "Tuo Zhao"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20369v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20369v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2510.20820v1",
    "title": "LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered   Canvas",
    "summary": "Despite their impressive visual fidelity, existing personalized generative models lack interactive control over spatial composition and scale poorly to multiple subjects. To address these limitations, we present LayerComposer, an interactive framework for personalized, multi-subject text-to-image generation. Our approach introduces two main contributions: (1) a layered canvas, a novel representation in which each subject is placed on a distinct layer, enabling occlusion-free composition; and (2) a locking mechanism that preserves selected layers with high fidelity while allowing the remaining layers to adapt flexibly to the surrounding context. Similar to professional image-editing software, the proposed layered canvas allows users to place, resize, or lock input subjects through intuitive layer manipulation. Our versatile locking mechanism requires no architectural changes, relying instead on inherent positional embeddings combined with a new complementary data sampling strategy. Extensive experiments demonstrate that LayerComposer achieves superior spatial control and identity preservation compared to the state-of-the-art methods in multi-subject personalized image generation.",
    "authors": [
      "Guocheng Gordon Qian",
      "Ruihang Zhang",
      "Tsai-Shien Chen",
      "Yusuf Dalva",
      "Anujraaj Argo Goyal",
      "Willi Menapace",
      "Ivan Skorokhodov",
      "Meng Dong",
      "Arpit Sahni",
      "Daniil Ostashev",
      "Ju Hu",
      "Sergey Tulyakov",
      "Kuan-Chieh Jackson Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20820v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20820v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2510.20819v1",
    "title": "Towards General Modality Translation with Contrastive and Predictive   Latent Diffusion Bridge",
    "summary": "Recent advances in generative modeling have positioned diffusion models as state-of-the-art tools for sampling from complex data distributions. While these models have shown remarkable success across single-modality domains such as images and audio, extending their capabilities to Modality Translation (MT), translating information across different sensory modalities, remains an open challenge. Existing approaches often rely on restrictive assumptions, including shared dimensionality, Gaussian source priors, and modality-specific architectures, which limit their generality and theoretical grounding. In this work, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a general-purpose framework for modality translation based on a latent-variable extension of Denoising Diffusion Bridge Models. By operating in a shared latent space, our method learns a bridge between arbitrary modalities without requiring aligned dimensions. We introduce a contrastive alignment loss to enforce semantic consistency between paired samples and design a domain-agnostic encoder-decoder architecture tailored for noise prediction in latent space. Additionally, we propose a predictive loss to guide training toward accurate cross-domain translation and explore several training strategies to improve stability. Our approach supports arbitrary modality pairs and performs strongly on diverse MT tasks, including multi-view to 3D shape generation, image super-resolution, and multi-view scene synthesis. Comprehensive experiments and ablations validate the effectiveness of our framework, establishing a new strong baseline in general modality translation. For more information, see our project page: https://sites.google.com/view/lddbm/home.",
    "authors": [
      "Nimrod Berman",
      "Omkar Joglekar",
      "Eitan Kosman",
      "Dotan Di Castro",
      "Omri Azencot"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20819v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20819v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2510.20797v1",
    "title": "Simple Context Compression: Mean-Pooling and Multi-Ratio Training",
    "summary": "A common strategy to reduce the computational costs of using long contexts in retrieval-augmented generation (RAG) with large language models (LLMs) is soft context compression, where the input sequence is transformed into a shorter continuous representation. We develop a lightweight and simple mean-pooling approach that consistently outperforms the widely used compression-tokens architecture, and study training the same compressor to output multiple compression ratios. We conduct extensive experiments across in-domain and out-of-domain QA datasets, as well as across model families, scales, and compression ratios. Overall, our simple mean-pooling approach achieves the strongest performance, with a relatively small drop when training for multiple compression ratios. More broadly though, across architectures and training regimes the trade-offs are more nuanced, illustrating the complex landscape of compression methods.",
    "authors": [
      "Yair Feldman",
      "Yoav Artzi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20797v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20797v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2510.20699v1",
    "title": "Fusing Narrative Semantics for Financial Volatility Forecasting",
    "summary": "We introduce M2VN: Multi-Modal Volatility Network, a novel deep learning-based framework for financial volatility forecasting that unifies time series features with unstructured news data. M2VN leverages the representational power of deep neural networks to address two key challenges in this domain: (i) aligning and fusing heterogeneous data modalities, numerical financial data and textual information, and (ii) mitigating look-ahead bias that can undermine the validity of financial models. To achieve this, M2VN combines open-source market features with news embeddings generated by Time Machine GPT, a recently introduced point-in-time LLM, ensuring temporal integrity. An auxiliary alignment loss is introduced to enhance the integration of structured and unstructured data within the deep learning architecture. Extensive experiments demonstrate that M2VN consistently outperforms existing baselines, underscoring its practical value for risk management and financial decision-making in dynamic markets.",
    "authors": [
      "Yaxuan Kong",
      "Yoontae Hwang",
      "Marcus Kaiser",
      "Chris Vryonides",
      "Roel Oomen",
      "Stefan Zohren"
    ],
    "categories": [
      "q-fin.CP",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20699v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20699v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2510.20677v1",
    "title": "R2-SVC: Towards Real-World Robust and Expressive Zero-shot Singing Voice   Conversion",
    "summary": "In real-world singing voice conversion (SVC) applications, environmental noise and the demand for expressive output pose significant challenges. Conventional methods, however, are typically designed without accounting for real deployment scenarios, as both training and inference usually rely on clean data. This mismatch hinders practical use, given the inevitable presence of diverse noise sources and artifacts from music separation. To tackle these issues, we propose R2-SVC, a robust and expressive SVC framework. First, we introduce simulation-based robustness enhancement through random fundamental frequency ($F_0$) perturbations and music separation artifact simulations (e.g., reverberation, echo), substantially improving performance under noisy conditions. Second, we enrich speaker representation using domain-specific singing data: alongside clean vocals, we incorporate DNSMOS-filtered separated vocals and public singing corpora, enabling the model to preserve speaker timbre while capturing singing style nuances. Third, we integrate the Neural Source-Filter (NSF) model to explicitly represent harmonic and noise components, enhancing the naturalness and controllability of converted singing. R2-SVC achieves state-of-the-art results on multiple SVC benchmarks under both clean and noisy conditions.",
    "authors": [
      "Junjie Zheng",
      "Gongyu Chen",
      "Chaofan Ding",
      "Zihao Chen"
    ],
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20677v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20677v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2510.20673v1",
    "title": "Efficient Multi-bit Quantization Network Training via Weight Bias   Correction and Bit-wise Coreset Sampling",
    "summary": "Multi-bit quantization networks enable flexible deployment of deep neural networks by supporting multiple precision levels within a single model. However, existing approaches suffer from significant training overhead as full-dataset updates are repeated for each supported bit-width, resulting in a cost that scales linearly with the number of precisions. Additionally, extra fine-tuning stages are often required to support additional or intermediate precision options, further compounding the overall training burden. To address this issue, we propose two techniques that greatly reduce the training overhead without compromising model utility: (i) Weight bias correction enables shared batch normalization and eliminates the need for fine-tuning by neutralizing quantization-induced bias across bit-widths and aligning activation distributions; and (ii) Bit-wise coreset sampling strategy allows each child model to train on a compact, informative subset selected via gradient-based importance scores by exploiting the implicit knowledge transfer phenomenon. Experiments on CIFAR-10/100, TinyImageNet, and ImageNet-1K with both ResNet and ViT architectures demonstrate that our method achieves competitive or superior accuracy while reducing training time up to 7.88x. Our code is released at https://github.com/a2jinhee/EMQNet_jk.",
    "authors": [
      "Jinhee Kim",
      "Jae Jun An",
      "Kang Eun Jeon",
      "Jong Hwan Ko"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20673v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20673v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2510.20651v1",
    "title": "xTime: Extreme Event Prediction with Hierarchical Knowledge Distillation   and Expert Fusion",
    "summary": "Extreme events frequently occur in real-world time series and often carry significant practical implications. In domains such as climate and healthcare, these events, such as floods, heatwaves, or acute medical episodes, can lead to serious consequences. Accurate forecasting of such events is therefore of substantial importance. Most existing time series forecasting models are optimized for overall performance within the prediction window, but often struggle to accurately predict extreme events, such as high temperatures or heart rate spikes. The main challenges are data imbalance and the neglect of valuable information contained in intermediate events that precede extreme events. In this paper, we propose xTime, a novel framework for extreme event forecasting in time series. xTime leverages knowledge distillation to transfer information from models trained on lower-rarity events, thereby improving prediction performance on rarer ones. In addition, we introduce a mixture of experts (MoE) mechanism that dynamically selects and fuses outputs from expert models across different rarity levels, which further improves the forecasting performance for extreme events. Experiments on multiple datasets show that xTime achieves consistent improvements, with forecasting accuracy on extreme events improving from 3% to 78%.",
    "authors": [
      "Quan Li",
      "Wenchao Yu",
      "Suhang Wang",
      "Minhua Lin",
      "Lingwei Chen",
      "Wei Cheng",
      "Haifeng Chen"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20651v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20651v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2510.20634v1",
    "title": "Deep Learning in Dental Image Analysis: A Systematic Review of Datasets,   Methodologies, and Emerging Challenges",
    "summary": "Efficient analysis and processing of dental images are crucial for dentists to achieve accurate diagnosis and optimal treatment planning. However, dental imaging inherently poses several challenges, such as low contrast, metallic artifacts, and variations in projection angles. Combined with the subjectivity arising from differences in clinicians' expertise, manual interpretation often proves time-consuming and prone to inconsistency. Artificial intelligence (AI)-based automated dental image analysis (DIA) offers a promising solution to these issues and has become an integral part of computer-aided dental diagnosis and treatment. Among various AI technologies, deep learning (DL) stands out as the most widely applied and influential approach due to its superior feature extraction and representation capabilities. To comprehensively summarize recent progress in this field, we focus on the two fundamental aspects of DL research-datasets and models. In this paper, we systematically review 260 studies on DL applications in DIA, including 49 papers on publicly available dental datasets and 211 papers on DL-based algorithms. We first introduce the basic concepts of dental imaging and summarize the characteristics and acquisition methods of existing datasets. Then, we present the foundational techniques of DL and categorize relevant models and algorithms according to different DIA tasks, analyzing their network architectures, optimization strategies, training methods, and performance. Furthermore, we summarize commonly used training and evaluation metrics in the DIA domain. Finally, we discuss the current challenges of existing research and outline potential future directions. We hope that this work provides a valuable and systematic reference for researchers in this field. All supplementary materials and detailed comparison tables will be made publicly available on GitHub.",
    "authors": [
      "Zhenhuan Zhou",
      "Jingbo Zhu",
      "Yuchen Zhang",
      "Xiaohang Guan",
      "Peng Wang",
      "Tao Li"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20634v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20634v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2510.20519v1",
    "title": "Metis-HOME: Hybrid Optimized Mixture-of-Experts for Multimodal Reasoning",
    "summary": "Inspired by recent advancements in LLM reasoning, the field of multimodal reasoning has seen remarkable progress, achieving significant performance gains on intricate tasks such as mathematical problem-solving. Despite this progress, current multimodal large reasoning models exhibit two key limitations. They tend to employ computationally expensive reasoning even for simple queries, leading to inefficiency. Furthermore, this focus on specialized reasoning often impairs their broader, more general understanding capabilities. In this paper, we propose Metis-HOME: a Hybrid Optimized Mixture-of-Experts framework designed to address this trade-off. Metis-HOME enables a ''Hybrid Thinking'' paradigm by structuring the original dense model into two distinct expert branches: a thinking branch tailored for complex, multi-step reasoning, and a non-thinking branch optimized for rapid, direct inference on tasks like general VQA and OCR. A lightweight, trainable router dynamically allocates queries to the most suitable expert. We instantiate Metis-HOME by adapting the Qwen2.5-VL-7B into an MoE architecture. Comprehensive evaluations reveal that our approach not only substantially enhances complex reasoning abilities but also improves the model's general capabilities, reversing the degradation trend observed in other reasoning-specialized models. Our work establishes a new paradigm for building powerful and versatile MLLMs, effectively resolving the prevalent reasoning-vs-generalization dilemma.",
    "authors": [
      "Xiaohan Lan",
      "Fanfan Liu",
      "Haibo Qiu",
      "Siqi Yang",
      "Delian Ruan",
      "Peng Shi",
      "Lin Ma"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20519v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20519v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2510.20457v1",
    "title": "Neural Reasoning for Robust Instance Retrieval in $\\mathcal{SHOIQ}$",
    "summary": "Concept learning exploits background knowledge in the form of description logic axioms to learn explainable classification models from knowledge bases. Despite recent breakthroughs in neuro-symbolic concept learning, most approaches still cannot be deployed on real-world knowledge bases. This is due to their use of description logic reasoners, which are not robust against inconsistencies nor erroneous data. We address this challenge by presenting a novel neural reasoner dubbed EBR. Our reasoner relies on embeddings to approximate the results of a symbolic reasoner. We show that EBR solely requires retrieving instances for atomic concepts and existential restrictions to retrieve or approximate the set of instances of any concept in the description logic $\\mathcal{SHOIQ}$. In our experiments, we compare EBR with state-of-the-art reasoners. Our results suggest that EBR is robust against missing and erroneous data in contrast to existing reasoners.",
    "authors": [
      "Louis Mozart Kamdem Teyou",
      "Luke Friedrichs",
      "N'Dah Jean Kouagou",
      "Caglar Demir",
      "Yasir Mahmood",
      "Stefan Heindorf",
      "Axel-Cyrille Ngonga Ngomo"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20457v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20457v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2510.20295v1",
    "title": "Quantifying Distributional Invariance in Causal Subgraph for IRM-Free   Graph Generalization",
    "summary": "Out-of-distribution generalization under distributional shifts remains a critical challenge for graph neural networks. Existing methods generally adopt the Invariant Risk Minimization (IRM) framework, requiring costly environment annotations or heuristically generated synthetic splits. To circumvent these limitations, in this work, we aim to develop an IRM-free method for capturing causal subgraphs. We first identify that causal subgraphs exhibit substantially smaller distributional variations than non-causal components across diverse environments, which we formalize as the Invariant Distribution Criterion and theoretically prove in this paper. Building on this criterion, we systematically uncover the quantitative relationship between distributional shift and representation norm for identifying the causal subgraph, and investigate its underlying mechanisms in depth. Finally, we propose an IRM-free method by introducing a norm-guided invariant distribution objective for causal subgraph discovery and prediction. Extensive experiments on two widely used benchmarks demonstrate that our method consistently outperforms state-of-the-art methods in graph generalization.",
    "authors": [
      "Yang Qiu",
      "Yixiong Zou",
      "Jun Wang",
      "Wei Liu",
      "Xiangyu Fu",
      "Ruixuan Li"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20295v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20295v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2510.20604v1",
    "title": "Efficient Algorithms for Computing Random Walk Centrality",
    "summary": "Random walk centrality is a fundamental metric in graph mining for quantifying node importance and influence, defined as the weighted average of hitting times to a node from all other nodes. Despite its ability to capture rich graph structural information and its wide range of applications, computing this measure for large networks remains impractical due to the computational demands of existing methods. In this paper, we present a novel formulation of random walk centrality, underpinning two scalable algorithms: one leveraging approximate Cholesky factorization and sparse inverse estimation, while the other sampling rooted spanning trees. Both algorithms operate in near-linear time and provide strong approximation guarantees. Extensive experiments on large real-world networks, including one with over 10 million nodes, demonstrate the efficiency and approximation quality of the proposed algorithms.",
    "authors": [
      "Changan Liu",
      "Zixuan Xie",
      "Ahad N. Zehmakan",
      "Zhongzhi Zhang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20604v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20604v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2510.20344v1",
    "title": "Neural Networks for Censored Expectile Regression Based on Data   Augmentation",
    "summary": "Expectile regression neural networks (ERNNs) are powerful tools for capturing heterogeneity and complex nonlinear structures in data. However, most existing research has primarily focused on fully observed data, with limited attention paid to scenarios involving censored observations. In this paper, we propose a data augmentation based ERNNs algorithm, termed DAERNN, for modeling heterogeneous censored data. The proposed DAERNN is fully data driven, requires minimal assumptions, and offers substantial flexibility. Simulation studies and real data applications demonstrate that DAERNN outperforms existing censored ERNNs methods and achieves predictive performance comparable to models trained on fully observed data. Moreover, the algorithm provides a unified framework for handling various censoring mechanisms without requiring explicit parametric model specification, thereby enhancing its applicability to practical censored data analysis.",
    "authors": [
      "Wei Cao",
      "Shanshan Wang"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20344v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20344v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2510.20331v1",
    "title": "AnyPcc: Compressing Any Point Cloud with a Single Universal Model",
    "summary": "Generalization remains a critical challenge for deep learning-based point cloud geometry compression. We argue this stems from two key limitations: the lack of robust context models and the inefficient handling of out-of-distribution (OOD) data. To address both, we introduce AnyPcc, a universal point cloud compression framework. AnyPcc first employs a Universal Context Model that leverages priors from both spatial and channel-wise grouping to capture robust contextual dependencies. Second, our novel Instance-Adaptive Fine-Tuning (IAFT) strategy tackles OOD data by synergizing explicit and implicit compression paradigms. It fine-tunes a small subset of network weights for each instance and incorporates them into the bitstream, where the marginal bit cost of the weights is dwarfed by the resulting savings in geometry compression. Extensive experiments on a benchmark of 15 diverse datasets confirm that AnyPcc sets a new state-of-the-art in point cloud compression. Our code and datasets will be released to encourage reproducible research.",
    "authors": [
      "Kangli Wang",
      "Qianxi Yi",
      "Yuqi Ye",
      "Shihao Li",
      "Wei Gao"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20331v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20331v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2510.20328v1",
    "title": "MemER: Scaling Up Memory for Robot Control via Experience Retrieval",
    "summary": "Humans routinely rely on memory to perform tasks, yet most robot policies lack this capability; our goal is to endow robot policies with the same ability. Naively conditioning on long observation histories is computationally expensive and brittle under covariate shift, while indiscriminate subsampling of history leads to irrelevant or redundant information. We propose a hierarchical policy framework, where the high-level policy is trained to select and track previous relevant keyframes from its experience. The high-level policy uses selected keyframes and the most recent frames when generating text instructions for a low-level policy to execute. This design is compatible with existing vision-language-action (VLA) models and enables the system to efficiently reason over long-horizon dependencies. In our experiments, we finetune Qwen2.5-VL-7B-Instruct and $\\pi_{0.5}$ as the high-level and low-level policies respectively, using demonstrations supplemented with minimal language annotations. Our approach, MemER, outperforms prior methods on three real-world long-horizon robotic manipulation tasks that require minutes of memory. Videos and code can be found at https://jen-pan.github.io/memer/.",
    "authors": [
      "Ajay Sridhar",
      "Jennifer Pan",
      "Satvik Sharma",
      "Chelsea Finn"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20328v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20328v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2510.20736v1",
    "title": "Amplifying Prominent Representations in Multimodal Learning via   Variational Dirichlet Process",
    "summary": "Developing effective multimodal fusion approaches has become increasingly essential in many real-world scenarios, such as health care and finance. The key challenge is how to preserve the feature expressiveness in each modality while learning cross-modal interactions. Previous approaches primarily focus on the cross-modal alignment, while over-emphasis on the alignment of marginal distributions of modalities may impose excess regularization and obstruct meaningful representations within each modality. The Dirichlet process (DP) mixture model is a powerful Bayesian non-parametric method that can amplify the most prominent features by its richer-gets-richer property, which allocates increasing weights to them. Inspired by this unique characteristic of DP, we propose a new DP-driven multimodal learning framework that automatically achieves an optimal balance between prominent intra-modal representation learning and cross-modal alignment. Specifically, we assume that each modality follows a mixture of multivariate Gaussian distributions and further adopt DP to calculate the mixture weights for all the components. This paradigm allows DP to dynamically allocate the contributions of features and select the most prominent ones, leveraging its richer-gets-richer property, thus facilitating multimodal feature fusion. Extensive experiments on several multimodal datasets demonstrate the superior performance of our model over other competitors. Ablation analysis further validates the effectiveness of DP in aligning modality distributions and its robustness to changes in key hyperparameters. Code is anonymously available at https://github.com/HKU-MedAI/DPMM.git",
    "authors": [
      "Tsai Hor Chan",
      "Feng Wu",
      "Yihang Chen",
      "Guosheng Yin",
      "Lequan Yu"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20736v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20736v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2510.20615v1",
    "title": "MS-BART: Unified Modeling of Mass Spectra and Molecules for Structure   Elucidation",
    "summary": "Mass spectrometry (MS) plays a critical role in molecular identification, significantly advancing scientific discovery. However, structure elucidation from MS data remains challenging due to the scarcity of annotated spectra. While large-scale pretraining has proven effective in addressing data scarcity in other domains, applying this paradigm to mass spectrometry is hindered by the complexity and heterogeneity of raw spectral signals. To address this, we propose MS-BART, a unified modeling framework that maps mass spectra and molecular structures into a shared token vocabulary, enabling cross-modal learning through large-scale pretraining on reliably computed fingerprint-molecule datasets. Multi-task pretraining objectives further enhance MS-BART's generalization by jointly optimizing denoising and translation task. The pretrained model is subsequently transferred to experimental spectra through finetuning on fingerprint predictions generated with MIST, a pre-trained spectral inference model, thereby enhancing robustness to real-world spectral variability. While finetuning alleviates the distributional difference, MS-BART still suffers molecular hallucination and requires further alignment. We therefore introduce a chemical feedback mechanism that guides the model toward generating molecules closer to the reference structure. Extensive evaluations demonstrate that MS-BART achieves SOTA performance across 5/12 key metrics on MassSpecGym and NPLIB1 and is faster by one order of magnitude than competing diffusion-based methods, while comprehensive ablation studies systematically validate the model's effectiveness and robustness.",
    "authors": [
      "Yang Han",
      "Pengyu Wang",
      "Kai Yu",
      "Xin Chen",
      "Lu Chen"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20615v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20615v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2510.20578v1",
    "title": "EmbodiedBrain: Expanding Performance Boundaries of Task Planning for   Embodied Intelligence",
    "summary": "The realization of Artificial General Intelligence (AGI) necessitates Embodied AI agents capable of robust spatial perception, effective task planning, and adaptive execution in physical environments. However, current large language models (LLMs) and multimodal LLMs (MLLMs) for embodied tasks suffer from key limitations, including a significant gap between model design and agent requirements, an unavoidable trade-off between real-time latency and performance, and the use of unauthentic, offline evaluation metrics. To address these challenges, we propose EmbodiedBrain, a novel vision-language foundation model available in both 7B and 32B parameter sizes. Our framework features an agent-aligned data structure and employs a powerful training methodology that integrates large-scale Supervised Fine-Tuning (SFT) with Step-Augumented Group Relative Policy Optimization (Step-GRPO), which boosts long-horizon task success by integrating preceding steps as Guided Precursors. Furthermore, we incorporate a comprehensive reward system, including a Generative Reward Model (GRM) accelerated at the infrastructure level, to improve training efficiency. For enable thorough validation, we establish a three-part evaluation system encompassing General, Planning, and End-to-End Simulation Benchmarks, highlighted by the proposal and open-sourcing of a novel, challenging simulation environment. Experimental results demonstrate that EmbodiedBrain achieves superior performance across all metrics, establishing a new state-of-the-art for embodied foundation models. Towards paving the way for the next generation of generalist embodied agents, we open-source all of our data, model weight, and evaluating methods, which are available at https://zterobot.github.io/EmbodiedBrain.github.io.",
    "authors": [
      "Ding Zou",
      "Feifan Wang",
      "Mengyu Ge",
      "Siyuan Fan",
      "Zongbing Zhang",
      "Wei Chen",
      "Lingfeng Wang",
      "Zhongyou Hu",
      "Wenrui Yan",
      "Zhengwei Gao",
      "Hao Wang",
      "Weizhao Jin",
      "Yu Zhang",
      "Hainan Zhao",
      "Mingliang Zhang",
      "Xianxian Xi",
      "Yaru Zhang",
      "Wenyuan Li",
      "Zhengguang Gao",
      "Yurui Zhu"
    ],
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20578v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20578v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2510.20822v1",
    "title": "HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video   Narratives",
    "summary": "State-of-the-art text-to-video models excel at generating isolated clips but fall short of creating the coherent, multi-shot narratives, which are the essence of storytelling. We bridge this \"narrative gap\" with HoloCine, a model that generates entire scenes holistically to ensure global consistency from the first shot to the last. Our architecture achieves precise directorial control through a Window Cross-Attention mechanism that localizes text prompts to specific shots, while a Sparse Inter-Shot Self-Attention pattern (dense within shots but sparse between them) ensures the efficiency required for minute-scale generation. Beyond setting a new state-of-the-art in narrative coherence, HoloCine develops remarkable emergent abilities: a persistent memory for characters and scenes, and an intuitive grasp of cinematic techniques. Our work marks a pivotal shift from clip synthesis towards automated filmmaking, making end-to-end cinematic creation a tangible future. Our code is available at: https://holo-cine.github.io/.",
    "authors": [
      "Yihao Meng",
      "Hao Ouyang",
      "Yue Yu",
      "Qiuyu Wang",
      "Wen Wang",
      "Ka Leong Cheng",
      "Hanlin Wang",
      "Yixuan Li",
      "Cheng Chen",
      "Yanhong Zeng",
      "Yujun Shen",
      "Huamin Qu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20822v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20822v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2510.20803v1",
    "title": "ARGenSeg: Image Segmentation with Autoregressive Image Generation Model",
    "summary": "We propose a novel AutoRegressive Generation-based paradigm for image Segmentation (ARGenSeg), achieving multimodal understanding and pixel-level perception within a unified framework. Prior works integrating image segmentation into multimodal large language models (MLLMs) typically employ either boundary points representation or dedicated segmentation heads. These methods rely on discrete representations or semantic prompts fed into task-specific decoders, which limits the ability of the MLLM to capture fine-grained visual details. To address these challenges, we introduce a segmentation framework for MLLM based on image generation, which naturally produces dense masks for target objects. We leverage MLLM to output visual tokens and detokenize them into images using an universal VQ-VAE, making the segmentation fully dependent on the pixel-level understanding of the MLLM. To reduce inference latency, we employ a next-scale-prediction strategy to generate required visual tokens in parallel. Extensive experiments demonstrate that our method surpasses prior state-of-the-art approaches on multiple segmentation datasets with a remarkable boost in inference speed, while maintaining strong understanding capabilities.",
    "authors": [
      "Xiaolong Wang",
      "Lixiang Ru",
      "Ziyuan Huang",
      "Kaixiang Ji",
      "Dandan Zheng",
      "Jingdong Chen",
      "Jun Zhou"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20803v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20803v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2510.20766v1",
    "title": "DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion",
    "summary": "Diffusion Transformer models can generate images with remarkable fidelity and detail, yet training them at ultra-high resolutions remains extremely costly due to the self-attention mechanism's quadratic scaling with the number of image tokens. In this paper, we introduce Dynamic Position Extrapolation (DyPE), a novel, training-free method that enables pre-trained diffusion transformers to synthesize images at resolutions far beyond their training data, with no additional sampling cost. DyPE takes advantage of the spectral progression inherent to the diffusion process, where low-frequency structures converge early, while high-frequencies take more steps to resolve. Specifically, DyPE dynamically adjusts the model's positional encoding at each diffusion step, matching their frequency spectrum with the current stage of the generative process. This approach allows us to generate images at resolutions that exceed the training resolution dramatically, e.g., 16 million pixels using FLUX. On multiple benchmarks, DyPE consistently improves performance and achieves state-of-the-art fidelity in ultra-high-resolution image generation, with gains becoming even more pronounced at higher resolutions. Project page is available at https://noamissachar.github.io/DyPE/.",
    "authors": [
      "Noam Issachar",
      "Guy Yariv",
      "Sagie Benaim",
      "Yossi Adi",
      "Dani Lischinski",
      "Raanan Fattal"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20766v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20766v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2510.20743v1",
    "title": "Empathic Prompting: Non-Verbal Context Integration for Multimodal LLM   Conversations",
    "summary": "We present Empathic Prompting, a novel framework for multimodal human-AI interaction that enriches Large Language Model (LLM) conversations with implicit non-verbal context. The system integrates a commercial facial expression recognition service to capture users' emotional cues and embeds them as contextual signals during prompting. Unlike traditional multimodal interfaces, empathic prompting requires no explicit user control; instead, it unobtrusively augments textual input with affective information for conversational and smoothness alignment. The architecture is modular and scalable, allowing integration of additional non-verbal modules. We describe the system design, implemented through a locally deployed DeepSeek instance, and report a preliminary service and usability evaluation (N=5). Results show consistent integration of non-verbal input into coherent LLM outputs, with participants highlighting conversational fluidity. Beyond this proof of concept, empathic prompting points to applications in chatbot-mediated communication, particularly in domains like healthcare or education, where users' emotional signals are critical yet often opaque in verbal exchanges.",
    "authors": [
      "Lorenzo Stacchio",
      "Andrea Ubaldi",
      "Alessandro Galdelli",
      "Maurizio Mauri",
      "Emanuele Frontoni",
      "Andrea Gaggioli"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20743v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20743v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2510.20700v1",
    "title": "Structure-Conditional Minimum Bayes Risk Decoding",
    "summary": "Minimum Bayes Risk (MBR) decoding has seen renewed interest as an alternative to traditional generation strategies. While MBR has proven effective in machine translation, where the variability of a language model's outcome space is naturally constrained, it may face challenges in more open-ended tasks such as dialogue or instruction-following. We hypothesise that in such settings, applying MBR with standard similarity-based utility functions may result in selecting responses that are broadly representative of the model's distribution, yet sub-optimal with respect to any particular grouping of generations that share an underlying latent structure. In this work, we introduce three lightweight adaptations to the utility function, designed to make MBR more sensitive to structural variability in the outcome space. To test our hypothesis, we curate a dataset capturing three representative types of latent structure: dialogue act, emotion, and response structure (e.g., a sentence, a paragraph, or a list). We further propose two metrics to evaluate the structural optimality of MBR. Our analysis demonstrates that common similarity-based utility functions fall short by these metrics. In contrast, our proposed adaptations considerably improve structural optimality. Finally, we evaluate our approaches on real-world instruction-following benchmarks, AlpacaEval and MT-Bench, and show that increased structural sensitivity improves generation quality by up to 13.7 percentage points in win rate.",
    "authors": [
      "Bryan Eikema",
      "Anna Rutkiewicz",
      "Mario Giulianelli"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20700v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20700v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2510.20637v1",
    "title": "Large Multimodal Models-Empowered Task-Oriented Autonomous   Communications: Design Methodology and Implementation Challenges",
    "summary": "Large language models (LLMs) and large multimodal models (LMMs) have achieved unprecedented breakthrough, showcasing remarkable capabilities in natural language understanding, generation, and complex reasoning. This transformative potential has positioned them as key enablers for 6G autonomous communications among machines, vehicles, and humanoids. In this article, we provide an overview of task-oriented autonomous communications with LLMs/LMMs, focusing on multimodal sensing integration, adaptive reconfiguration, and prompt/fine-tuning strategies for wireless tasks. We demonstrate the framework through three case studies: LMM-based traffic control, LLM-based robot scheduling, and LMM-based environment-aware channel estimation. From experimental results, we show that the proposed LLM/LMM-aided autonomous systems significantly outperform conventional and discriminative deep learning (DL) model-based techniques, maintaining robustness under dynamic objectives, varying input parameters, and heterogeneous multimodal conditions where conventional static optimization degrades.",
    "authors": [
      "Hyun Jong Yang",
      "Hyunsoo Kim",
      "Hyeonho Noh",
      "Seungnyun Kim",
      "Byonghyo Shim"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20637v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20637v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2510.20388v1",
    "title": "FLAS: a combination of proactive and reactive auto-scaling architecture   for distributed services",
    "summary": "Cloud computing has established itself as the support for the vast majority of emerging technologies, mainly due to the characteristic of elasticity it offers. Auto-scalers are the systems that enable this elasticity by acquiring and releasing resources on demand to ensure an agreed service level. In this article we present FLAS (Forecasted Load Auto-Scaling), an auto-scaler for distributed services that combines the advantages of proactive and reactive approaches according to the situation to decide the optimal scaling actions in every moment. The main novelties introduced by FLAS are (i) a predictive model of the high-level metrics trend which allows to anticipate changes in the relevant SLA parameters (e.g. performance metrics such as response time or throughput) and (ii) a reactive contingency system based on the estimation of high-level metrics from resource use metrics, reducing the necessary instrumentation (less invasive) and allowing it to be adapted agnostically to different applications. We provide a FLAS implementation for the use case of a content-based publish-subscribe middleware (E-SilboPS) that is the cornerstone of an event-driven architecture. To the best of our knowledge, this is the first auto-scaling system for content-based publish-subscribe distributed systems (although it is generic enough to fit any distributed service). Through an evaluation based on several test cases recreating not only the expected contexts of use, but also the worst possible scenarios (following the Boundary-Value Analysis or BVA test methodology), we have validated our approach and demonstrated the effectiveness of our solution by ensuring compliance with performance requirements over 99% of the time.",
    "authors": [
      "V\u00edctor Ramp\u00e9rez",
      "Javier Soriano",
      "David Lizcano",
      "Juan A. Lara"
    ],
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20388v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20388v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2510.20333v1",
    "title": "GhostEI-Bench: Do Mobile Agents Resilience to Environmental Injection in   Dynamic On-Device Environments?",
    "summary": "Vision-Language Models (VLMs) are increasingly deployed as autonomous agents to navigate mobile graphical user interfaces (GUIs). Operating in dynamic on-device ecosystems, which include notifications, pop-ups, and inter-app interactions, exposes them to a unique and underexplored threat vector: environmental injection. Unlike prompt-based attacks that manipulate textual instructions, environmental injection corrupts an agent's visual perception by inserting adversarial UI elements (for example, deceptive overlays or spoofed notifications) directly into the GUI. This bypasses textual safeguards and can derail execution, causing privacy leakage, financial loss, or irreversible device compromise. To systematically evaluate this threat, we introduce GhostEI-Bench, the first benchmark for assessing mobile agents under environmental injection attacks within dynamic, executable environments. Moving beyond static image-based assessments, GhostEI-Bench injects adversarial events into realistic application workflows inside fully operational Android emulators and evaluates performance across critical risk scenarios. We further propose a judge-LLM protocol that conducts fine-grained failure analysis by reviewing the agent's action trajectory alongside the corresponding screenshot sequence, pinpointing failure in perception, recognition, or reasoning. Comprehensive experiments on state-of-the-art agents reveal pronounced vulnerability to deceptive environmental cues: current models systematically fail to perceive and reason about manipulated UIs. GhostEI-Bench provides a framework for quantifying and mitigating this emerging threat, paving the way toward more robust and secure embodied agents.",
    "authors": [
      "Chiyu Chen",
      "Xinhao Song",
      "Yunkai Chai",
      "Yang Yao",
      "Haodong Zhao",
      "Lijun Li",
      "Jie Li",
      "Yan Teng",
      "Gongshen Liu",
      "Yingchun Wang"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20333v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20333v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2510.20299v1",
    "title": "DB-FGA-Net: Dual Backbone Frequency Gated Attention Network for   Multi-Class Classification with Grad-CAM Interpretability",
    "summary": "Brain tumors are a challenging problem in neuro-oncology, where early and precise diagnosis is important for successful treatment. Deep learning-based brain tumor classification methods often rely on heavy data augmentation which can limit generalization and trust in clinical applications. In this paper, we propose a double-backbone network integrating VGG16 and Xception with a Frequency-Gated Attention (FGA) Block to capture complementary local and global features. Unlike previous studies, our model achieves state-of-the-art performance without augmentation which demonstrates robustness to variably sized and distributed datasets. For further transparency, Grad-CAM is integrated to visualize the tumor regions based on which the model is giving prediction, bridging the gap between model prediction and clinical interpretability. The proposed framework achieves 99.24\\% accuracy on the 7K-DS dataset for the 4-class setting, along with 98.68\\% and 99.85\\% in the 3-class and 2-class settings, respectively. On the independent 3K-DS dataset, the model generalizes with 95.77\\% accuracy, outperforming baseline and state-of-the-art methods. To further support clinical usability, we developed a graphical user interface (GUI) that provides real-time classification and Grad-CAM-based tumor localization. These findings suggest that augmentation-free, interpretable, and deployable deep learning models such as DB-FGA-Net hold strong potential for reliable clinical translation in brain tumor diagnosis.",
    "authors": [
      "Saraf Anzum Shreya",
      "MD. Abu Ismail Siddique",
      "Sharaf Tasnim"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20299v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20299v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2510.20812v1",
    "title": "Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via   Speculation",
    "summary": "Large Vision-Language Models (VLMs) have achieved remarkable progress in multimodal understanding, yet they struggle when reasoning over information-intensive images that densely interleave textual annotations with fine-grained graphical elements. The main challenges lie in precisely localizing critical cues in dense layouts and multi-hop reasoning to integrate dispersed evidence. We propose Speculative Verdict (SV), a training-free framework inspired by speculative decoding that combines multiple lightweight draft experts with a large verdict model. In the draft stage, small VLMs act as draft experts to generate reasoning paths that provide diverse localization candidates; in the verdict stage, a strong VLM synthesizes these paths to produce the final answer, minimizing computational cost while recovering correct answers. To further improve efficiency and accuracy, SV introduces a consensus expert selection mechanism that forwards only high-agreement reasoning paths to the verdict. Empirically, SV achieves consistent gains on challenging information-intensive and high-resolution visual question answering benchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K. By synthesizing correct insights from multiple partially accurate reasoning paths, SV achieves both error correction and cost-efficiency compared to large proprietary models or training pipelines. Code is available at https://github.com/Tinaliu0123/speculative-verdict",
    "authors": [
      "Yuhan Liu",
      "Lianhui Qin",
      "Shengjie Wang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20812v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20812v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2510.20800v1",
    "title": "Compress to Impress: Efficient LLM Adaptation Using a Single Gradient   Step on 100 Samples",
    "summary": "Recently, Sharma et al. suggested a method called Layer-SElective-Rank reduction (LASER) which demonstrated that pruning high-order components of carefully chosen LLM's weight matrices can boost downstream accuracy -- without any gradient-based fine-tuning. Yet LASER's exhaustive, per-matrix search (each requiring full-dataset forward passes) makes it impractical for rapid deployment. We demonstrate that this overhead can be removed and find that: (i) Only a small, carefully chosen subset of matrices needs to be inspected -- eliminating the layer-by-layer sweep, (ii) The gradient of each matrix's singular values pinpoints which matrices merit reduction, (iii) Increasing the factorization search space by allowing matrices rows to cluster around multiple subspaces and then decomposing each cluster separately further reduces overfitting on the original training data and further lifts accuracy by up to 24.6 percentage points, and finally, (iv) we discover that evaluating on just 100 samples rather than the full training data -- both for computing the indicative gradients and for measuring the final accuracy -- suffices to further reduce the search time; we explain that as adaptation to downstream tasks is dominated by prompting style, not dataset size. As a result, we show that combining these findings yields a fast and robust adaptation algorithm for downstream tasks. Overall, with a single gradient step on 100 examples and a quick scan of the top candidate layers and factorization techniques, we can adapt LLMs to new datasets -- entirely without fine-tuning.",
    "authors": [
      "Shiva Sreeram",
      "Alaa Maalouf",
      "Pratyusha Sharma",
      "Daniela Rus"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20800v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20800v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2510.20795v1",
    "title": "Bayesian Inference of Primordial Magnetic Field Parameters from CMB with   Spherical Graph Neural Networks",
    "summary": "Deep learning has emerged as a transformative methodology in modern cosmology, providing powerful tools to extract meaningful physical information from complex astronomical datasets. This paper implements a novel Bayesian graph deep learning framework for estimating key cosmological parameters in a primordial magnetic field (PMF) cosmology directly from simulated Cosmic Microwave Background (CMB) maps. Our methodology utilizes DeepSphere, a spherical convolutional neural network architecture specifically designed to respect the spherical geometry of CMB data through HEALPix pixelization. To advance beyond deterministic point estimates and enable robust uncertainty quantification, we integrate Bayesian Neural Networks (BNNs) into the framework, capturing aleatoric and epistemic uncertainties that reflect the model confidence in its predictions. The proposed approach demonstrates exceptional performance, achieving $R^{2}$ scores exceeding 0.89 for the magnetic parameter estimation. We further obtain well-calibrated uncertainty estimates through post-hoc training techniques including Variance Scaling and GPNormal. This integrated DeepSphere-BNNs framework not only delivers accurate parameter estimation from CMB maps with PMF contributions but also provides reliable uncertainty quantification, providing the necessary tools for robust cosmological inference in the era of precision cosmology.",
    "authors": [
      "Juan Alejandro Pinto Castro",
      "H\u00e9ctor J. Hort\u00faa",
      "Jorge Enrique Garc\u00eda-Farieta",
      "Roger Anderson Hurtado"
    ],
    "categories": [
      "astro-ph.CO",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20795v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20795v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2510.20787v1",
    "title": "Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention   and Contextualized Learnable Token Eviction",
    "summary": "Linear-attention models that compress the entire input sequence into a fixed-size recurrent state offer an efficient alternative to Transformers, but their finite memory induces forgetfulness that harms retrieval-intensive tasks. To mitigate the issue, we explore a series of hybrid models that restore direct access to past tokens. We interleave token mixers with intermediate time and space complexity between linear and full attention, including sparse attention with token eviction, and the query-aware native sparse attention. Particularly, we propose a novel learnable token eviction approach. Combined with sliding-window attention, an end-to-end trainable lightweight CNN aggregates information from both past and future adjacent tokens to adaptively retain a limited set of critical KV-pairs per head, maintaining linear attention's constant time and space complexity. Efficient Triton kernels for the sparse attention mechanisms are provided. Empirical evaluations on retrieval-intensive benchmarks support the effectiveness of our approaches.",
    "authors": [
      "Mutian He",
      "Philip N. Garner"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20787v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20787v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2510.20707v1",
    "title": "Mixing Importance with Diversity: Joint Optimization for KV Cache   Compression in Large Vision-Language Models",
    "summary": "Recent large vision-language models (LVLMs) demonstrate remarkable capabilities in processing extended multi-modal sequences, yet the resulting key-value (KV) cache expansion creates a critical memory bottleneck that fundamentally limits deployment scalability. While existing KV cache compression methods focus on retaining high-importance KV pairs to minimize storage, they often overlook the modality-specific semantic redundancy patterns that emerge distinctively in multi-modal KV caches. In this work, we first analyze how, beyond simple importance, the KV cache in LVLMs exhibits varying levels of redundancy across attention heads. We show that relying solely on importance can only cover a subset of the full KV cache information distribution, leading to potential loss of semantic coverage. To address this, we propose \\texttt{MixKV}, a novel method that mixes importance with diversity for optimized KV cache compression in LVLMs. \\texttt{MixKV} adapts to head-wise semantic redundancy, selectively balancing diversity and importance when compressing KV pairs. Extensive experiments demonstrate that \\texttt{MixKV} consistently enhances existing methods across multiple LVLMs. Under extreme compression (budget=64), \\texttt{MixKV} improves baseline methods by an average of \\textbf{5.1\\%} across five multi-modal understanding benchmarks and achieves remarkable gains of \\textbf{8.0\\%} and \\textbf{9.0\\%} for SnapKV and AdaKV on GUI grounding tasks, all while maintaining comparable inference efficiency. Furthermore, \\texttt{MixKV} extends seamlessly to LLMs with comparable performance gains. Our code is available at \\href{https://github.com/xuyang-liu16/MixKV}{\\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}.",
    "authors": [
      "Xuyang Liu",
      "Xiyan Gui",
      "Yuchao Zhang",
      "Linfeng Zhang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20707v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20707v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2510.20640v1",
    "title": "Attention Enhanced Entity Recommendation for Intelligent Monitoring in   Cloud Systems",
    "summary": "In this paper, we present DiRecGNN, an attention-enhanced entity recommendation framework for monitoring cloud services at Microsoft. We provide insights on the usefulness of this feature as perceived by the cloud service owners and lessons learned from deployment. Specifically, we introduce the problem of recommending the optimal subset of attributes (dimensions) that should be tracked by an automated watchdog (monitor) for cloud services. To begin, we construct the monitor heterogeneous graph at production-scale. The interaction dynamics of these entities are often characterized by limited structural and engagement information, resulting in inferior performance of state-of-the-art approaches. Moreover, traditional methods fail to capture the dependencies between entities spanning a long range due to their homophilic nature. Therefore, we propose an attention-enhanced entity ranking model inspired by transformer architectures. Our model utilizes a multi-head attention mechanism to focus on heterogeneous neighbors and their attributes, and further attends to paths sampled using random walks to capture long-range dependencies. We also employ multi-faceted loss functions to optimize for relevant recommendations while respecting the inherent sparsity of the data. Empirical evaluations demonstrate significant improvements over existing methods, with our model achieving a 43.1% increase in MRR. Furthermore, product teams who consumed these features perceive the feature as useful and rated it 4.5 out of 5.",
    "authors": [
      "Fiza Hussain",
      "Anson Bastos",
      "Anjaly Parayil",
      "Ayush Choure",
      "Chetan Bansal",
      "Rujia Wang",
      "Saravan Rajmohan"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20640v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20640v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2510.20595v1",
    "title": "Diffusion Autoencoders with Perceivers for Long, Irregular and   Multimodal Astronomical Sequences",
    "summary": "Self-supervised learning has become a central strategy for representation learning, but the majority of architectures used for encoding data have only been validated on regularly-sampled inputs such as images, audios. and videos. In many scientific domains, data instead arrive as long, irregular, and multimodal sequences. To extract semantic information from these data, we introduce the Diffusion Autoencoder with Perceivers (daep). daep tokenizes heterogeneous measurements, compresses them with a Perceiver encoder, and reconstructs them with a Perceiver-IO diffusion decoder, enabling scalable learning in diverse data settings. To benchmark the daep architecture, we adapt the masked autoencoder to a Perceiver encoder/decoder design, and establish a strong baseline (maep) in the same architectural family as daep. Across diverse spectroscopic and photometric astronomical datasets, daep achieves lower reconstruction errors, produces more discriminative latent spaces, and better preserves fine-scale structure than both VAE and maep baselines. These results establish daep as an effective framework for scientific domains where data arrives as irregular, heterogeneous sequences.",
    "authors": [
      "Yunyi Shen",
      "Alexander Gagliano"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20595v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20595v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2510.20513v1",
    "title": "Decoding the Ear: A Framework for Objectifying Expressiveness from Human   Preference Through Efficient Alignment",
    "summary": "Recent speech-to-speech (S2S) models generate intelligible speech but still lack natural expressiveness, largely due to the absence of a reliable evaluation metric. Existing approaches, such as subjective MOS ratings, low-level acoustic features, and emotion recognition are costly, limited, or incomplete. To address this, we present DeEAR (Decoding the Expressive Preference of eAR), a framework that converts human preference for speech expressiveness into an objective score. Grounded in phonetics and psychology, DeEAR evaluates speech across three dimensions: Emotion, Prosody, and Spontaneity, achieving strong alignment with human perception (Spearman's Rank Correlation Coefficient, SRCC = 0.86) using fewer than 500 annotated samples. Beyond reliable scoring, DeEAR enables fair benchmarking and targeted data curation. It not only distinguishes expressiveness gaps across S2S models but also selects 14K expressive utterances to form ExpressiveSpeech, which improves the expressive score (from 2.0 to 23.4 on a 100-point scale) of S2S models. Demos and codes are available at https://github.com/FreedomIntelligence/ExpressiveSpeech",
    "authors": [
      "Zhiyu Lin",
      "Jingwen Yang",
      "Jiale Zhao",
      "Meng Liu",
      "Sunzhu Li",
      "Benyou Wang"
    ],
    "categories": [
      "cs.SD",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20513v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20513v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2510.20342v1",
    "title": "Teaching Language Models to Reason with Tools",
    "summary": "Large reasoning models (LRMs) like OpenAI-o1 have shown impressive capabilities in natural language reasoning. However, these models frequently demonstrate inefficiencies or inaccuracies when tackling complex mathematical operations. While integrating computational tools such as Code Interpreters (CIs) offers a promising solution, it introduces a critical challenge: a conflict between the model's internal, probabilistic reasoning and the external, deterministic knowledge provided by the CI, which often leads models to unproductive deliberation. To overcome this, we introduce CoRT (Code-Optimized Reasoning Training), a post-training framework designed to teach LRMs to effectively utilize CIs. We propose \\emph{Hint-Engineering}, a new data synthesis strategy that strategically injects diverse hints at optimal points within reasoning paths. This approach generates high-quality, code-integrated reasoning data specifically tailored to optimize LRM-CI interaction. Using this method, we have synthesized 30 high-quality samples to post-train models ranging from 1.5B to 32B parameters through supervised fine-tuning. CoRT further refines the multi-round interleaving of external CI usage and internal thinking by employing rejection sampling and reinforcement learning. Our experimental evaluations demonstrate CoRT's effectiveness, yielding absolute improvements of 4\\% and 8\\% on DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Qwen-1.5B, respectively, across five challenging mathematical reasoning datasets. Moreover, CoRT significantly enhances efficiency, reducing token usage by approximately 30\\% for the 32B model and 50\\% for the 1.5B model compared to pure natural language reasoning baselines. The models and code are available at: https://github.com/ChengpengLi1003/CoRT.",
    "authors": [
      "Chengpeng Li",
      "Zhengyang Tang",
      "Ziniu Li",
      "Mingfeng Xue",
      "Keqin Bao",
      "Tian Ding",
      "Ruoyu Sun",
      "Benyou Wang",
      "Xiang Wang",
      "Junyang Lin",
      "Dayiheng Liu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20342v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20342v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2510.20817v1",
    "title": "KL-Regularized Reinforcement Learning is Designed to Mode Collapse",
    "summary": "It is commonly believed that optimizing the reverse KL divergence results in \"mode seeking\", while optimizing forward KL results in \"mass covering\", with the latter being preferred if the goal is to sample from multiple diverse modes. We show -- mathematically and empirically -- that this intuition does not necessarily transfer well to doing reinforcement learning with reverse/forward KL regularization (e.g. as commonly used with language models). Instead, the choice of reverse/forward KL determines the family of optimal target distributions, parameterized by the regularization coefficient. Mode coverage depends primarily on other factors, such as regularization strength, and relative scales between rewards and reference probabilities. Further, we show commonly used settings such as low regularization strength and equal verifiable rewards tend to specify unimodal target distributions, meaning the optimization objective is, by construction, non-diverse. We leverage these insights to construct a simple, scalable, and theoretically justified algorithm. It makes minimal changes to reward magnitudes, yet optimizes for a target distribution which puts high probability over all high-quality sampling modes. In experiments, this simple modification works to post-train both Large Language Models and Chemical Language Models to have higher solution quality and diversity, without any external signals of diversity, and works with both forward and reverse KL when using either naively fails.",
    "authors": [
      "Anthony GX-Chen",
      "Jatin Prakash",
      "Jeff Guo",
      "Rob Fergus",
      "Rajesh Ranganath"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20817v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20817v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2510.20792v1",
    "title": "BadGraph: A Backdoor Attack Against Latent Diffusion Model for   Text-Guided Graph Generation",
    "summary": "The rapid progress of graph generation has raised new security concerns, particularly regarding backdoor vulnerabilities. While prior work has explored backdoor attacks in image diffusion and unconditional graph generation, conditional, especially text-guided graph generation remains largely unexamined. This paper proposes BadGraph, a backdoor attack method targeting latent diffusion models for text-guided graph generation. BadGraph leverages textual triggers to poison training data, covertly implanting backdoors that induce attacker-specified subgraphs during inference when triggers appear, while preserving normal performance on clean inputs. Extensive experiments on four benchmark datasets (PubChem, ChEBI-20, PCDes, MoMu) demonstrate the effectiveness and stealth of the attack: less than 10% poisoning rate can achieves 50% attack success rate, while 24% suffices for over 80% success rate, with negligible performance degradation on benign samples. Ablation studies further reveal that the backdoor is implanted during VAE and diffusion training rather than pretraining. These findings reveal the security vulnerabilities in latent diffusion models of text-guided graph generation, highlight the serious risks in models' applications such as drug discovery and underscore the need for robust defenses against the backdoor attack in such diffusion models.",
    "authors": [
      "Liang Ye",
      "Shengqin Chen",
      "Jiazhu Dai"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "q-bio.BM"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20792v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20792v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2510.20762v1",
    "title": "MEIcoder: Decoding Visual Stimuli from Neural Activity by Leveraging   Most Exciting Inputs",
    "summary": "Decoding visual stimuli from neural population activity is crucial for understanding the brain and for applications in brain-machine interfaces. However, such biological data is often scarce, particularly in primates or humans, where high-throughput recording techniques, such as two-photon imaging, remain challenging or impossible to apply. This, in turn, poses a challenge for deep learning decoding techniques. To overcome this, we introduce MEIcoder, a biologically informed decoding method that leverages neuron-specific most exciting inputs (MEIs), a structural similarity index measure loss, and adversarial training. MEIcoder achieves state-of-the-art performance in reconstructing visual stimuli from single-cell activity in primary visual cortex (V1), especially excelling on small datasets with fewer recorded neurons. Using ablation studies, we demonstrate that MEIs are the main drivers of the performance, and in scaling experiments, we show that MEIcoder can reconstruct high-fidelity natural-looking images from as few as 1,000-2,500 neurons and less than 1,000 training data points. We also propose a unified benchmark with over 160,000 samples to foster future research. Our results demonstrate the feasibility of reliable decoding in early visual system and provide practical insights for neuroscience and neuroengineering applications.",
    "authors": [
      "Jan Sobotka",
      "Luca Baroni",
      "J\u00e1n Antol\u00edk"
    ],
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20762v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20762v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2510.20556v1",
    "title": "Structural Invariance Matters: Rethinking Graph Rewiring through Graph   Metrics",
    "summary": "Graph rewiring has emerged as a key technique to alleviate over-squashing in Graph Neural Networks (GNNs) and Graph Transformers by modifying the graph topology to improve information flow. While effective, rewiring inherently alters the graph's structure, raising the risk of distorting important topology-dependent signals. Yet, despite the growing use of rewiring, little is known about which structural properties must be preserved to ensure both performance gains and structural fidelity. In this work, we provide the first systematic analysis of how rewiring affects a range of graph structural metrics, and how these changes relate to downstream task performance. We study seven diverse rewiring strategies and correlate changes in local and global graph properties with node classification accuracy. Our results reveal a consistent pattern: successful rewiring methods tend to preserve local structure while allowing for flexibility in global connectivity. These findings offer new insights into the design of effective rewiring strategies, bridging the gap between graph theory and practical GNN optimization.",
    "authors": [
      "Alexandre Benoit",
      "Catherine Aitken",
      "Yu He"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20556v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20556v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2510.20472v1",
    "title": "Concentration and excess risk bounds for imbalanced classification with   synthetic oversampling",
    "summary": "Synthetic oversampling of minority examples using SMOTE and its variants is a leading strategy for addressing imbalanced classification problems. Despite the success of this approach in practice, its theoretical foundations remain underexplored. We develop a theoretical framework to analyze the behavior of SMOTE and related methods when classifiers are trained on synthetic data. We first derive a uniform concentration bound on the discrepancy between the empirical risk over synthetic minority samples and the population risk on the true minority distribution. We then provide a nonparametric excess risk guarantee for kernel-based classifiers trained using such synthetic data. These results lead to practical guidelines for better parameter tuning of both SMOTE and the downstream learning algorithm. Numerical experiments are provided to illustrate and support the theoretical findings",
    "authors": [
      "Touqeer Ahmad",
      "Mohammadreza M. Kalan",
      "Fran\u00e7ois Portier",
      "Gilles Stupfler"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20472v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20472v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2510.20402v1",
    "title": "A computational model and tool for generating more novel opportunities   in professional innovation processes",
    "summary": "This paper presents a new computational model of creative outcomes, informed by creativity theories and techniques, which was implemented to generate more novel opportunities for innovation projects. The model implemented five functions that were developed to contribute to the generation of innovation opportunities with higher novelty without loss of usefulness. The model was evaluated using opportunities generated for an innovation project in the hospitality sector. The evaluation revealed that the computational model generated outcomes that were more novel and/or useful than outcomes from Notebook LM and ChatGPT4o. However, not all model functions contributed to the generation of more novel opportunities, leading to new directions for further model development",
    "authors": [
      "Neil Maiden",
      "Konstantinos Zachos",
      "James Lockerbie",
      "Kostas Petrianakis",
      "Amanda Brown"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20402v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20402v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2510.20385v1",
    "title": "Positional Encoding Field",
    "summary": "Diffusion Transformers (DiTs) have emerged as the dominant architecture for visual generation, powering state-of-the-art image and video models. By representing images as patch tokens with positional encodings (PEs), DiTs combine Transformer scalability with spatial and temporal inductive biases. In this work, we revisit how DiTs organize visual content and discover that patch tokens exhibit a surprising degree of independence: even when PEs are perturbed, DiTs still produce globally coherent outputs, indicating that spatial coherence is primarily governed by PEs. Motivated by this finding, we introduce the Positional Encoding Field (PE-Field), which extends positional encodings from the 2D plane to a structured 3D field. PE-Field incorporates depth-aware encodings for volumetric reasoning and hierarchical encodings for fine-grained sub-patch control, enabling DiTs to model geometry directly in 3D space. Our PE-Field-augmented DiT achieves state-of-the-art performance on single-image novel view synthesis and generalizes to controllable spatial image editing.",
    "authors": [
      "Yunpeng Bai",
      "Haoxiang Li",
      "Qixing Huang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20385v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20385v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2510.20377v1",
    "title": "IKnow: Instruction-Knowledge-Aware Continual Pretraining for Effective   Domain Adaptation",
    "summary": "Continual pretraining promises to adapt large language models (LLMs) to new domains using only unlabeled test-time data, but naively applying standard self-supervised objectives to instruction-tuned models is known to degrade their instruction-following capability and semantic representations. Existing fixes assume access to the original base model or rely on knowledge from an external domain-specific database - both of which pose a realistic barrier in settings where the base model weights are withheld for safety reasons or reliable external corpora are unavailable. In this work, we propose Instruction-Knowledge-Aware Continual Adaptation (IKnow), a simple and general framework that formulates novel self-supervised objectives in the instruction-response dialogue format. Rather than depend- ing on external resources, IKnow leverages domain knowledge embedded within the text itself and learns to encode it at a deeper semantic level.",
    "authors": [
      "Tianyi Zhang",
      "Florian Mai",
      "Lucie Flek"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20377v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20377v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2510.20337v1",
    "title": "Collateral Damage Assessment Model for AI System Target Engagement in   Military Operations",
    "summary": "In an era where AI (Artificial Intelligence) systems play an increasing role in the battlefield, ensuring responsible targeting demands rigorous assessment of potential collateral effects. In this context, a novel collateral damage assessment model for target engagement of AI systems in military operations is introduced. The model integrates temporal, spatial, and force dimensions within a unified Knowledge Representation and Reasoning (KRR) architecture following a design science methodological approach. Its layered structure captures the categories and architectural components of the AI systems to be engaged together with corresponding engaging vectors and contextual aspects. At the same time, spreading, severity, likelihood, and evaluation metrics are considered in order to provide a clear representation enhanced by transparent reasoning mechanisms. Further, the model is demonstrated and evaluated through instantiation which serves as a basis for further dedicated efforts that aim at building responsible and trustworthy intelligent systems for assessing the effects produced by engaging AI systems in military operations.",
    "authors": [
      "Clara Maathuis",
      "Kasper Cools"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20337v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20337v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2510.20728v1",
    "title": "Co-Designing Quantum Codes with Transversal Diagonal Gates via   Multi-Agent Systems",
    "summary": "We present a multi-agent, human-in-the-loop workflow that co-designs quantum codes with prescribed transversal diagonal gates. It builds on the Subset-Sum Linear Programming (SSLP) framework (arXiv:2504.20847), which partitions basis strings by modular residues and enforces $Z$-marginal Knill-Laflamme (KL) equalities via small LPs. The workflow is powered by GPT-5 and implemented within TeXRA (https://texra.ai)-a multi-agent research assistant platform that supports an iterative tool-use loop agent and a derivation-then-edit workflow reasoning agent. We work in a LaTeX-Python environment where agents reason, edit documents, execute code, and synchronize their work to Git/Overleaf. Within this workspace, three roles collaborate: a Synthesis Agent formulates the problem; a Search Agent sweeps/screens candidates and exactifies numerics into rationals; and an Audit Agent independently checks all KL equalities and the induced logical action. As a first step we focus on distance $d=2$ with nondegenerate residues. For code dimension $K\\in\\{2,3,4\\}$ and $n\\le6$ qubits, systematic sweeps yield certificate-backed tables cataloging attainable cyclic logical groups-all realized by new codes-e.g., for $K=3$ we obtain order $16$ at $n=6$. From verified instances, Synthesis Agent abstracts recurring structures into closed-form families and proves they satisfy the KL equalities for all parameters. It further demonstrates that SSLP accommodates residue degeneracy by exhibiting a new $((6,4,2))$ code implementing the transversal controlled-phase $diag(1,1,1,i)$. Overall, the workflow recasts diagonal-transversal feasibility as an analytical pipeline executed at scale, combining systematic enumeration with exact analytical reconstruction. It yields reproducible code constructions, supports targeted extensions to larger $K$ and higher distances, and leads toward data-driven classification.",
    "authors": [
      "Xi He",
      "Sirui Lu",
      "Bei Zeng"
    ],
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.CL",
      "math-ph",
      "math.MP"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20728v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20728v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2510.20708v1",
    "title": "ALICE-LRI: A General Method for Lossless Range Image Generation for   Spinning LiDAR Sensors without Calibration Metadata",
    "summary": "3D LiDAR sensors are essential for autonomous navigation, environmental monitoring, and precision mapping in remote sensing applications. To efficiently process the massive point clouds generated by these sensors, LiDAR data is often projected into 2D range images that organize points by their angular positions and distances. While these range image representations enable efficient processing, conventional projection methods suffer from fundamental geometric inconsistencies that cause irreversible information loss, compromising high-fidelity applications. We present ALICE-LRI (Automatic LiDAR Intrinsic Calibration Estimation for Lossless Range Images), the first general, sensor-agnostic method that achieves lossless range image generation from spinning LiDAR point clouds without requiring manufacturer metadata or calibration files. Our algorithm automatically reverse-engineers the intrinsic geometry of any spinning LiDAR sensor by inferring critical parameters including laser beam configuration, angular distributions, and per-beam calibration corrections, enabling lossless projection and complete point cloud reconstruction with zero point loss. Comprehensive evaluation across the complete KITTI and DurLAR datasets demonstrates that ALICE-LRI achieves perfect point preservation, with zero points lost across all point clouds. Geometric accuracy is maintained well within sensor precision limits, establishing geometric losslessness with real-time performance. We also present a compression case study that validates substantial downstream benefits, demonstrating significant quality improvements in practical applications. This paradigm shift from approximate to lossless LiDAR projections opens new possibilities for high-precision remote sensing applications requiring complete geometric preservation.",
    "authors": [
      "Samuel Soutullo",
      "Miguel Yermo",
      "David L. Vilari\u00f1o",
      "\u00d3scar G. Lorenzo",
      "Jos\u00e9 C. Cabaleiro",
      "Francisco F. Rivera"
    ],
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20708v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20708v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2510.20691v1",
    "title": "Plan Then Retrieve: Reinforcement Learning-Guided Complex Reasoning over   Knowledge Graphs",
    "summary": "Knowledge Graph Question Answering aims to answer natural language questions by reasoning over structured knowledge graphs. While large language models have advanced KGQA through their strong reasoning capabilities, existing methods continue to struggle to fully exploit both the rich knowledge encoded in KGs and the reasoning capabilities of LLMs, particularly in complex scenarios. They often assume complete KG coverage and lack mechanisms to judge when external information is needed, and their reasoning remains locally myopic, failing to maintain coherent multi-step planning, leading to reasoning failures even when relevant knowledge exists. We propose Graph-RFT, a novel two-stage reinforcement fine-tuning KGQA framework with a 'plan-KGsearch-and-Websearch-during-think' paradigm, that enables LLMs to perform autonomous planning and adaptive retrieval scheduling across KG and web sources under incomplete knowledge conditions. Graph-RFT introduces a chain-of-thought fine-tuning method with a customized plan-retrieval dataset activates structured reasoning and resolves the GRPO cold-start problem. It then introduces a novel plan-retrieval guided reinforcement learning process integrates explicit planning and retrieval actions with a multi-reward design, enabling coverage-aware retrieval scheduling. It employs a Cartesian-inspired planning module to decompose complex questions into ordered subquestions, and logical expression to guide tool invocation for globally consistent multi-step reasoning. This reasoning retrieval process is optimized with a multi-reward combining outcome and retrieval specific signals, enabling the model to learn when and how to combine KG and web retrieval effectively.",
    "authors": [
      "Yanlin Song",
      "Ben Liu",
      "V\u00edctor Guti\u00e9rrez-Basulto",
      "Zhiwei Hu",
      "Qianqian Xie",
      "Min Peng",
      "Sophia Ananiadou",
      "Jeff Z. Pan"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20691v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20691v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2510.20671v1",
    "title": "GRACE: GRaph-based Addiction Care prEdiction",
    "summary": "Determining the appropriate locus of care for addiction patients is one of the most critical clinical decisions that affects patient treatment outcomes and effective use of resources. With a lack of sufficient specialized treatment resources, such as inpatient beds or staff, there is an unmet need to develop an automated framework for the same. Current decision-making approaches suffer from severe class imbalances in addiction datasets. To address this limitation, we propose a novel graph neural network (GRACE) framework that formalizes locus of care prediction as a structured learning problem. Further, we perform extensive feature engineering and propose a new approach of obtaining an unbiased meta-graph to train a GNN to overcome the class imbalance problem. Experimental results in real-world data show an improvement of 11-35% in terms of the F1 score of the minority class over competitive baselines. The codes and note embeddings are available at https://anonymous.4open.science/r/GRACE-F8E1/.",
    "authors": [
      "Subham Kumar",
      "Prakrithi Shivaprakash",
      "Koustav Rudra",
      "Lekhansh Shukla",
      "Animesh Mukherjee"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20671v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20671v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2510.20644v1",
    "title": "Connecting Jensen-Shannon and Kullback-Leibler Divergences: A New Bound   for Representation Learning",
    "summary": "Mutual Information (MI) is a fundamental measure of statistical dependence widely used in representation learning. While direct optimization of MI via its definition as a Kullback-Leibler divergence (KLD) is often intractable, many recent methods have instead maximized alternative dependence measures, most notably, the Jensen-Shannon divergence (JSD) between joint and product of marginal distributions via discriminative losses. However, the connection between these surrogate objectives and MI remains poorly understood. In this work, we bridge this gap by deriving a new, tight, and tractable lower bound on KLD as a function of JSD in the general case. By specializing this bound to joint and marginal distributions, we demonstrate that maximizing the JSD-based information increases a guaranteed lower bound on mutual information. Furthermore, we revisit the practical implementation of JSD-based objectives and observe that minimizing the cross-entropy loss of a binary classifier trained to distinguish joint from marginal pairs recovers a known variational lower bound on the JSD. Extensive experiments demonstrate that our lower bound is tight when applied to MI estimation. We compared our lower bound to state-of-the-art neural estimators of variational lower bound across a range of established reference scenarios. Our lower bound estimator consistently provides a stable, low-variance estimate of a tight lower bound on MI. We also demonstrate its practical usefulness in the context of the Information Bottleneck framework. Taken together, our results provide new theoretical justifications and strong empirical evidence for using discriminative learning in MI-based representation learning.",
    "authors": [
      "Reuben Dorent",
      "Polina Golland",
      "William Wells III"
    ],
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.IT"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20644v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20644v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2510.20498v1",
    "title": "Robust Preference Alignment via Directional Neighborhood Consensus",
    "summary": "Aligning large language models with human preferences is critical for creating reliable and controllable AI systems. A human preference can be visualized as a high-dimensional vector where different directions represent trade-offs between desired attributes (e.g., helpfulness vs. verbosity). Yet, because the training data often reflects dominant, average preferences, LLMs tend to perform well on common requests but fall short in specific, individual needs. This mismatch creates a preference coverage gap. Existing methods often address this through costly retraining, which may not be generalized to the full spectrum of diverse preferences. This brittleness means that when a user's request reflects a nuanced preference deviating from the training data's central tendency, model performance can degrade unpredictably. To address this challenge, we introduce Robust Preference Selection (RPS), a post-hoc, training-free method by leveraging directional neighborhood consensus. Instead of forcing a model to generate a response from a single, highly specific preference, RPS samples multiple responses from a local neighborhood of related preferences to create a superior candidate pool. It then selects the response that best aligns with the user's original intent. We provide a theoretical framework showing our neighborhood generation strategy is provably superior to a strong baseline that also samples multiple candidates. Comprehensive experiments across three distinct alignment paradigms (DPA, DPO, and SFT) demonstrate that RPS consistently improves robustness against this baseline, achieving win rates of up to 69% on challenging preferences from under-represented regions of the space without any model retraining. Our work presents a practical, theoretically-grounded solution for enhancing the reliability of preference-aligned models.",
    "authors": [
      "Ruochen Mao",
      "Yuling Shi",
      "Xiaodong Gu",
      "Jiaheng Wei"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20498v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20498v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2510.20414v1",
    "title": "Addressing Mark Imbalance in Integration-free Neural Marked Temporal   Point Processes",
    "summary": "Marked Temporal Point Process (MTPP) has been well studied to model the event distribution in marked event streams, which can be used to predict the mark and arrival time of the next event. However, existing studies overlook that the distribution of event marks is highly imbalanced in many real-world applications, with some marks being frequent but others rare. The imbalance poses a significant challenge to the performance of the next event prediction, especially for events of rare marks. To address this issue, we propose a thresholding method, which learns thresholds to tune the mark probability normalized by the mark's prior probability to optimize mark prediction, rather than predicting the mark directly based on the mark probability as in existing studies. In conjunction with this method, we predict the mark first and then the time. In particular, we develop a novel neural MTPP model to support effective time sampling and estimation of mark probability without computationally expensive numerical improper integration. Extensive experiments on real-world datasets demonstrate the superior performance of our solution against various baselines for the next event mark and time prediction. The code is available at https://github.com/undes1red/IFNMTPP.",
    "authors": [
      "Sishun Liu",
      "Ke Deng",
      "Xiuzhen Zhang",
      "Yongli Ren",
      "Yan Wang"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20414v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20414v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2510.20387v1",
    "title": "Relative-Based Scaling Law for Neural Language Models",
    "summary": "Scaling laws aim to accurately predict model performance across different scales. Existing scaling-law studies almost exclusively rely on cross-entropy as the evaluation metric. However, cross-entropy provides only a partial view of performance: it measures the absolute probability assigned to the correct token, but ignores the relative ordering between correct and incorrect tokens. Yet, relative ordering is crucial for language models, such as in greedy-sampling scenario. To address this limitation, we investigate scaling from the perspective of relative ordering. We first propose the Relative-Based Probability (RBP) metric, which quantifies the probability that the correct token is ranked among the top predictions. Building on this metric, we establish the Relative-Based Scaling Law, which characterizes how RBP improves with increasing model size. Through extensive experiments on four datasets and four model families spanning five orders of magnitude, we demonstrate the robustness and accuracy of this law. Finally, we illustrate the broad application of this law with two examples, namely providing a deeper explanation of emergence phenomena and facilitating finding fundamental theories of scaling laws. In summary, the Relative-Based Scaling Law complements the cross-entropy perspective and contributes to a more complete understanding of scaling large language models. Thus, it offers valuable insights for both practical development and theoretical exploration.",
    "authors": [
      "Baoqing Yue",
      "Jinyuan Zhou",
      "Zixi Wei",
      "Jingtao Zhan",
      "Qingyao Ai",
      "Yiqun Liu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20387v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20387v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2510.20314v1",
    "title": "Enhancing Security in Deep Reinforcement Learning: A Comprehensive   Survey on Adversarial Attacks and Defenses",
    "summary": "With the wide application of deep reinforcement learning (DRL) techniques in complex fields such as autonomous driving, intelligent manufacturing, and smart healthcare, how to improve its security and robustness in dynamic and changeable environments has become a core issue in current research. Especially in the face of adversarial attacks, DRL may suffer serious performance degradation or even make potentially dangerous decisions, so it is crucial to ensure their stability in security-sensitive scenarios. In this paper, we first introduce the basic framework of DRL and analyze the main security challenges faced in complex and changing environments. In addition, this paper proposes an adversarial attack classification framework based on perturbation type and attack target and reviews the mainstream adversarial attack methods against DRL in detail, including various attack methods such as perturbation state space, action space, reward function and model space. To effectively counter the attacks, this paper systematically summarizes various current robustness training strategies, including adversarial training, competitive training, robust learning, adversarial detection, defense distillation and other related defense techniques, we also discuss the advantages and shortcomings of these methods in improving the robustness of DRL. Finally, this paper looks into the future research direction of DRL in adversarial environments, emphasizing the research needs in terms of improving generalization, reducing computational complexity, and enhancing scalability and explainability, aiming to provide valuable references and directions for researchers.",
    "authors": [
      "Wu Yichao",
      "Wang Yirui",
      "Ding Panpan",
      "Wang Hailong",
      "Zhu Bingqian",
      "Liu Chun"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20314v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20314v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2510.20304v1",
    "title": "Exploring Generative Process Reward Modeling for Semi-Structured Data: A   Case Study of Table Question Answering",
    "summary": "Process reward models (PRMs) improve complex reasoning in large language models (LLMs) by grading candidate solutions step-by-step and selecting answers via aggregated step scores. While effective in domains such as mathematics, their applicability to tasks involving semi-structured data, like table question answering (TQA) remains unexplored. TQA poses unique challenges for PRMs, including abundant irrelevant information, loosely connected reasoning steps, and domain-specific reasoning. This work presents the first systematic study of PRMs for TQA. We evaluate state-of-the-art generative PRMs on TQA from both answer and step perspectives. Results show that PRMs that combine textual and code verification can aid solution selection but struggle to generalize to out-of-domain data. Analysis reveals a weak correlation between performance in step-level verification and answer accuracy, possibly stemming from weak step dependencies and loose causal links. Our findings highlight limitations of current PRMs on TQA and offer valuable insights for building more robust, process-aware verifiers.",
    "authors": [
      "Lei Tang",
      "Wei Zhou",
      "Mohsen Mesgar"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20304v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20304v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2510.20302v1",
    "title": "InvDec: Inverted Decoder for Multivariate Time Series Forecasting with   Separated Temporal and Variate Modeling",
    "summary": "Multivariate time series forecasting requires simultaneously modeling temporal patterns and cross-variate dependencies. Channel-independent methods such as PatchTST excel at temporal modeling but ignore variable correlations, while pure variate-attention approaches such as iTransformer sacrifice temporal encoding. We proposeInvDec (Inverted Decoder), a hybrid architecture that achieves principled separation between temporal encoding and variate-level decoding. InvDec combines a patch-based temporal encoder with an inverted decoder operating on the variate dimension through variate-wise self-attention. We introduce delayed variate embeddings that enrich variable-specific representations only after temporal encoding, preserving temporal feature integrity. An adaptive residual fusion mechanism dynamically balances temporal and variate information across datasets of varying dimensions. Instantiating InvDec with PatchTST yields InvDec-PatchTST. Extensive experiments on seven benchmarks demonstrate significant gains on high-dimensional datasets: 20.9% MSE reduction on Electricity (321 variables), 4.3% improvement on Weather, and 2.7% gain on Traffic compared to PatchTST, while maintaining competitive performance on low-dimensional ETT datasets. Ablation studies validate each component, and analysis reveals that InvDec's advantage grows with dataset dimensionality, confirming that cross-variate modeling becomes critical as the number of variables increases.",
    "authors": [
      "Yuhang Wang"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20302v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20302v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2510.20665v1",
    "title": "The Shape of Reasoning: Topological Analysis of Reasoning Traces in   Large Language Models",
    "summary": "Evaluating the quality of reasoning traces from large language models remains understudied, labor-intensive, and unreliable: current practice relies on expert rubrics, manual annotation, and slow pairwise judgments. Automated efforts are dominated by graph-based proxies that quantify structural connectivity but do not clarify what constitutes high-quality reasoning; such abstractions can be overly simplistic for inherently complex processes. We introduce a topological data analysis (TDA)-based evaluation framework that captures the geometry of reasoning traces and enables label-efficient, automated assessment. In our empirical study, topological features yield substantially higher predictive power for assessing reasoning quality than standard graph metrics, suggesting that effective reasoning is better captured by higher-dimensional geometric structures rather than purely relational graphs. We further show that a compact, stable set of topological features reliably indicates trace quality, offering a practical signal for future reinforcement learning algorithms.",
    "authors": [
      "Xue Wen Tan",
      "Nathaniel Tan",
      "Galen Lee",
      "Stanley Kok"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20665v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20665v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2510.20540v1",
    "title": "SheafAlign: A Sheaf-theoretic Framework for Decentralized Multimodal   Alignment",
    "summary": "Conventional multimodal alignment methods assume mutual redundancy across all modalities, an assumption that fails in real-world distributed scenarios. We propose SheafAlign, a sheaf-theoretic framework for decentralized multimodal alignment that replaces single-space alignment with multiple comparison spaces. This approach models pairwise modality relations through sheaf structures and leverages decentralized contrastive learning-based objectives for training. SheafAlign overcomes the limitations of prior methods by not requiring mutual redundancy among all modalities, preserving both shared and unique information. Experiments on multimodal sensing datasets show superior zero-shot generalization, cross-modal alignment, and robustness to missing modalities, with 50\\% lower communication cost than state-of-the-art baselines.",
    "authors": [
      "Abdulmomen Ghalkha",
      "Zhuojun Tian",
      "Chaouki Ben Issaid",
      "Mehdi Bennis"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20540v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20540v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2510.20477v1",
    "title": "Bi-CoG: Bi-Consistency-Guided Self-Training for Vision-Language Models",
    "summary": "Exploiting unlabeled data through semi-supervised learning (SSL) or leveraging pre-trained models via fine-tuning are two prevailing paradigms for addressing label-scarce scenarios. Recently, growing attention has been given to combining fine-tuning of pre-trained vision-language models (VLMs) with SSL, forming the emerging paradigm of semi-supervised fine-tuning. However, existing methods often suffer from model bias and hyperparameter sensitivity, due to reliance on prediction consistency or pre-defined confidence thresholds. To address these limitations, we propose a simple yet effective plug-and-play methodology named $\\underline{\\textbf{Bi-Co}}$nsistency-$\\underline{\\textbf{G}}$uided Self-Training (Bi-CoG), which assigns high-quality and low-bias pseudo-labels, by simultaneously exploiting inter-model and intra-model consistency, along with an error-aware dynamic pseudo-label assignment strategy. Both theoretical analysis and extensive experiments over 14 datasets demonstrate the effectiveness of Bi-CoG, which consistently and significantly improves the performance of existing methods.",
    "authors": [
      "Rui Zhu",
      "Song-Lin Lv",
      "Zi-Kang Wang",
      "Lan-Zhe Guo"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20477v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20477v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2510.20467v1",
    "title": "FLORA: Unsupervised Knowledge Graph Alignment by Fuzzy Logic",
    "summary": "Knowledge graph alignment is the task of matching equivalent entities (that is, instances and classes) and relations across two knowledge graphs. Most existing methods focus on pure entity-level alignment, computing the similarity of entities in some embedding space. They lack interpretable reasoning and need training data to work. In this paper, we propose FLORA, a simple yet effective method that (1) is unsupervised, i.e., does not require training data, (2) provides a holistic alignment for entities and relations iteratively, (3) is based on fuzzy logic and thus delivers interpretable results, (4) provably converges, (5) allows dangling entities, i.e., entities without a counterpart in the other KG, and (6) achieves state-of-the-art results on major benchmarks.",
    "authors": [
      "Yiwen Peng",
      "Thomas Bonald",
      "Fabian M. Suchanek"
    ],
    "categories": [
      "cs.AI",
      "cs.DB"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20467v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20467v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2510.20362v1",
    "title": "ComProScanner: A multi-agent based framework for composition-property   structured data extraction from scientific literature",
    "summary": "Since the advent of various pre-trained large language models, extracting structured knowledge from scientific text has experienced a revolutionary change compared with traditional machine learning or natural language processing techniques. Despite these advances, accessible automated tools that allow users to construct, validate, and visualise datasets from scientific literature extraction remain scarce. We therefore developed ComProScanner, an autonomous multi-agent platform that facilitates the extraction, validation, classification, and visualisation of machine-readable chemical compositions and properties, integrated with synthesis data from journal articles for comprehensive database creation. We evaluated our framework using 100 journal articles against 10 different LLMs, including both open-source and proprietary models, to extract highly complex compositions associated with ceramic piezoelectric materials and corresponding piezoelectric strain coefficients (d33), motivated by the lack of a large dataset for such materials. DeepSeek-V3-0324 outperformed all models with a significant overall accuracy of 0.82. This framework provides a simple, user-friendly, readily-usable package for extracting highly complex experimental data buried in the literature to build machine learning or deep learning datasets.",
    "authors": [
      "Aritra Roy",
      "Enrico Grisan",
      "John Buckeridge",
      "Chiara Gattinoni"
    ],
    "categories": [
      "physics.comp-ph",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20362v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20362v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2510.20356v1",
    "title": "FreeChunker: A Cross-Granularity Chunking Framework",
    "summary": "Chunking strategies significantly impact the effectiveness of Retrieval-Augmented Generation (RAG) systems. Existing methods operate within fixed-granularity paradigms that rely on static boundary identification, limiting their adaptability to diverse query requirements. This paper presents FreeChunker, a Cross-Granularity Encoding Framework that fundamentally transforms the traditional chunking paradigm: the framework treats sentences as atomic units and shifts from static chunk segmentation to flexible retrieval supporting arbitrary sentence combinations. This paradigm shift not only significantly reduces the computational overhead required for semantic boundary detection but also enhances adaptability to complex queries. Experimental evaluation on LongBench V2 demonstrates that FreeChunker achieves superior retrieval performance compared to traditional chunking methods, while significantly outperforming existing approaches in computational efficiency.",
    "authors": [
      "Wenxuan Zhang",
      "Yuan-Hao Jiang",
      "Yonghe Wu"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20356v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20356v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2510.20670v1",
    "title": "\\textsc{CantoNLU}: A benchmark for Cantonese natural language   understanding",
    "summary": "Cantonese, although spoken by millions, remains under-resourced due to policy and diglossia. To address this scarcity of evaluation frameworks for Cantonese, we introduce \\textsc{\\textbf{CantoNLU}}, a benchmark for Cantonese natural language understanding (NLU). This novel benchmark spans seven tasks covering syntax and semantics, including word sense disambiguation, linguistic acceptability judgment, language detection, natural language inference, sentiment analysis, part-of-speech tagging, and dependency parsing. In addition to the benchmark, we provide model baseline performance across a set of models: a Mandarin model without Cantonese training, two Cantonese-adapted models obtained by continual pre-training a Mandarin model on Cantonese text, and a monolingual Cantonese model trained from scratch. Results show that Cantonese-adapted models perform best overall, while monolingual models perform better on syntactic tasks. Mandarin models remain competitive in certain settings, indicating that direct transfer may be sufficient when Cantonese domain data is scarce. We release all datasets, code, and model weights to facilitate future research in Cantonese NLP.",
    "authors": [
      "Junghyun Min",
      "York Hay Ng",
      "Sophia Chan",
      "Helena Shunhua Zhao",
      "En-Shiun Annie Lee"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20670v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20670v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2510.20612v1",
    "title": "Black Box Absorption: LLMs Undermining Innovative Ideas",
    "summary": "Large Language Models are increasingly adopted as critical tools for accelerating innovation. This paper identifies and formalizes a systemic risk inherent in this paradigm: \\textbf{Black Box Absorption}. We define this as the process by which the opaque internal architectures of LLM platforms, often operated by large-scale service providers, can internalize, generalize, and repurpose novel concepts contributed by users during interaction. This mechanism threatens to undermine the foundational principles of innovation economics by creating severe informational and structural asymmetries between individual creators and platform operators, thereby jeopardizing the long-term sustainability of the innovation ecosystem. To analyze this challenge, we introduce two core concepts: the idea unit, representing the transportable functional logic of an innovation, and idea safety, a multidimensional standard for its protection. This paper analyzes the mechanisms of absorption and proposes a concrete governance and engineering agenda to mitigate these risks, ensuring that creator contributions remain traceable, controllable, and equitable.",
    "authors": [
      "Wenjun Cao"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "econ.GN",
      "q-fin.EC"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20612v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20612v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2510.20611v1",
    "title": "PSO-XAI: A PSO-Enhanced Explainable AI Framework for Reliable Breast   Cancer Detection",
    "summary": "Breast cancer is considered the most critical and frequently diagnosed cancer in women worldwide, leading to an increase in cancer-related mortality. Early and accurate detection is crucial as it can help mitigate possible threats while improving survival rates. In terms of prediction, conventional diagnostic methods are often limited by variability, cost, and, most importantly, risk of misdiagnosis. To address these challenges, machine learning (ML) has emerged as a powerful tool for computer-aided diagnosis, with feature selection playing a vital role in improving model performance and interpretability. This research study proposes an integrated framework that incorporates customized Particle Swarm Optimization (PSO) for feature selection. This framework has been evaluated on a comprehensive set of 29 different models, spanning classical classifiers, ensemble techniques, neural networks, probabilistic algorithms, and instance-based algorithms. To ensure interpretability and clinical relevance, the study uses cross-validation in conjunction with explainable AI methods. Experimental evaluation showed that the proposed approach achieved a superior score of 99.1\\% across all performance metrics, including accuracy and precision, while effectively reducing dimensionality and providing transparent, model-agnostic explanations. The results highlight the potential of combining swarm intelligence with explainable ML for robust, trustworthy, and clinically meaningful breast cancer diagnosis.",
    "authors": [
      "Mirza Raquib",
      "Niloy Das",
      "Farida Siddiqi Prity",
      "Arafath Al Fahim",
      "Saydul Akbar Murad",
      "Mohammad Amzad Hossain",
      "MD Jiabul Hoque",
      "Mohammad Ali Moni"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20611v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20611v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2510.20591v1",
    "title": "Transferable Graph Learning for Transmission Congestion Management via   Busbar Splitting",
    "summary": "Network topology optimization (NTO) via busbar splitting can mitigate transmission grid congestion and reduce redispatch costs. However, solving this mixed-integer non-linear problem for large-scale systems in near-real-time is currently intractable with existing solvers. Machine learning (ML) approaches have emerged as a promising alternative, but they have limited generalization to unseen topologies, varying operating conditions, and different systems, which limits their practical applicability. This paper formulates NTO for congestion management problem considering linearized AC PF, and proposes a graph neural network (GNN)-accelerated approach. We develop a heterogeneous edge-aware message passing NN to predict effective busbar splitting actions as candidate NTO solutions. The proposed GNN captures local flow patterns, achieves generalization to unseen topology changes, and improves transferability across systems. Case studies show up to 4 orders-of-magnitude speed-up, delivering AC-feasible solutions within one minute and a 2.3% optimality gap on the GOC 2000-bus system. These results demonstrate a significant step toward near-real-time NTO for large-scale systems with topology and cross-system generalization.",
    "authors": [
      "Ali Rajaei",
      "Peter Palensky",
      "Jochen L. Cremer"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20591v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20591v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2510.20470v1",
    "title": "Conan: Progressive Learning to Reason Like a Detective over Multi-Scale   Visual Evidence",
    "summary": "Video reasoning, which requires multi-step deduction across frames, remains a major challenge for multimodal large language models (MLLMs). While reinforcement learning (RL)-based methods enhance reasoning capabilities, they often rely on text-only chains that yield ungrounded or hallucinated conclusions. Conversely, frame-retrieval approaches introduce visual grounding but still struggle with inaccurate evidence localization. To address these challenges, we present Conan, a framework for evidence-grounded multi-step video reasoning. Conan identifies contextual and evidence frames, reasons over cross-frame clues, and adaptively decides when to conclude or explore further. To achieve this, we (1) construct Conan-91K, a large-scale dataset of automatically generated reasoning traces that includes frame identification, evidence reasoning, and action decision, and (2) design a multi-stage progressive cold-start strategy combined with an Identification-Reasoning-Action (AIR) RLVR training framework to jointly enhance multi-step visual reasoning. Extensive experiments on six multi-step reasoning benchmarks demonstrate that Conan surpasses the baseline Qwen2.5-VL-7B-Instruct by an average of over 10% in accuracy, achieving state-of-the-art performance. Furthermore, Conan generalizes effectively to long-video understanding tasks, validating its strong scalability and robustness.",
    "authors": [
      "Kun Ouyang",
      "Yuanxin Liu",
      "Linli Yao",
      "Yishuo Cai",
      "Hao Zhou",
      "Jie Zhou",
      "Fandong Meng",
      "Xu Sun"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20470v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20470v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2510.20468v1",
    "title": "Transferable Black-Box One-Shot Forging of Watermarks via Image   Preference Models",
    "summary": "Recent years have seen a surge in interest in digital content watermarking techniques, driven by the proliferation of generative models and increased legal pressure. With an ever-growing percentage of AI-generated content available online, watermarking plays an increasingly important role in ensuring content authenticity and attribution at scale. There have been many works assessing the robustness of watermarking to removal attacks, yet, watermark forging, the scenario when a watermark is stolen from genuine content and applied to malicious content, remains underexplored. In this work, we investigate watermark forging in the context of widely used post-hoc image watermarking. Our contributions are as follows. First, we introduce a preference model to assess whether an image is watermarked. The model is trained using a ranking loss on purely procedurally generated images without any need for real watermarks. Second, we demonstrate the model's capability to remove and forge watermarks by optimizing the input image through backpropagation. This technique requires only a single watermarked image and works without knowledge of the watermarking model, making our attack much simpler and more practical than attacks introduced in related work. Third, we evaluate our proposed method on a variety of post-hoc image watermarking models, demonstrating that our approach can effectively forge watermarks, questioning the security of current watermarking approaches. Our code and further resources are publicly available.",
    "authors": [
      "Tom\u00e1\u0161 Sou\u010dek",
      "Sylvestre-Alvise Rebuffi",
      "Pierre Fernandez",
      "Nikola Jovanovi\u0107",
      "Hady Elsahar",
      "Valeriu Lacatusu",
      "Tuan Tran",
      "Alexandre Mourachko"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20468v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20468v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2510.20436v1",
    "title": "Learning Decentralized Routing Policies via Graph Attention-based   Multi-Agent Reinforcement Learning in Lunar Delay-Tolerant Networks",
    "summary": "We present a fully decentralized routing framework for multi-robot exploration missions operating under the constraints of a Lunar Delay-Tolerant Network (LDTN). In this setting, autonomous rovers must relay collected data to a lander under intermittent connectivity and unknown mobility patterns. We formulate the problem as a Partially Observable Markov Decision Problem (POMDP) and propose a Graph Attention-based Multi-Agent Reinforcement Learning (GAT-MARL) policy that performs Centralized Training, Decentralized Execution (CTDE). Our method relies only on local observations and does not require global topology updates or packet replication, unlike classical approaches such as shortest path and controlled flooding-based algorithms. Through Monte Carlo simulations in randomized exploration environments, GAT-MARL provides higher delivery rates, no duplications, and fewer packet losses, and is able to leverage short-term mobility forecasts; offering a scalable solution for future space robotic systems for planetary exploration, as demonstrated by successful generalization to larger rover teams.",
    "authors": [
      "Federico Lozano-Cuadra",
      "Beatriz Soret",
      "Marc Sanchez Net",
      "Abhishek Cauligi",
      "Federico Rossi"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20436v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20436v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2510.20411v1",
    "title": "Teacher Demonstrations in a BabyLM's Zone of Proximal Development for   Contingent Multi-Turn Interaction",
    "summary": "Multi-turn dialogues between a child and a caregiver are characterized by a property called contingency - that is, prompt, direct, and meaningful exchanges between interlocutors. We introduce ContingentChat, a teacher-student framework that benchmarks and improves multi-turn contingency in a BabyLM trained on 100M words. Using a novel alignment dataset for post-training, BabyLM generates responses that are more grammatical and cohesive. Experiments with adaptive teacher decoding strategies show limited additional gains. ContingentChat demonstrates the benefits of targeted post-training for dialogue quality and indicates that contingency remains a challenging goal for BabyLMs.",
    "authors": [
      "Suchir Salhan",
      "Hongyi Gu",
      "Donya Rooein",
      "Diana Galvan-Sosa",
      "Gabrielle Gaudeau",
      "Andrew Caines",
      "Zheng Yuan",
      "Paula Buttery"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20411v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20411v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2510.20303v1",
    "title": "Citation Failure: Definition, Analysis and Efficient Mitigation",
    "summary": "Citations from LLM-based RAG systems are supposed to simplify response verification. However, this does not hold for citation failure, when a model generates a helpful response, but fails to cite complete evidence. In contrast to previous work, we propose to disentangle this from response failure, where the response itself is flawed, and citing complete evidence is impossible. To address citation failure, this work follows a two-step approach: (1) We study when citation failure occurs and (2) how it can be mitigated. For step 1, we extend prior work by investigating how the relation between response and evidence affects citation quality. We introduce CITECONTROL, a benchmark that systematically varies this relation to analyze failure modes. Experiments show that failures increase with relational complexity and suggest that combining citation methods could improve performance, motivating step 2. To improve LLM citation efficiently, we propose CITENTION, a framework integrating generative, attention-based, and retrieval-based methods. Results demonstrate substantial citation improvements on CITECONTROL and in transfer settings. We make our data and code publicly available.",
    "authors": [
      "Jan Buchmann",
      "Iryna Gurevych"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20303v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20303v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2510.20287v1",
    "title": "Breakdance Video classification in the age of Generative AI",
    "summary": "Large Vision Language models have seen huge application in several sports use-cases recently. Most of these works have been targeted towards a limited subset of popular sports like soccer, cricket, basketball etc; focusing on generative tasks like visual question answering, highlight generation. This work analyzes the applicability of the modern video foundation models (both encoder and decoder) for a very niche but hugely popular dance sports - breakdance. Our results show that Video Encoder models continue to outperform state-of-the-art Video Language Models for prediction tasks. We provide insights on how to choose the encoder model and provide a thorough analysis into the workings of a finetuned decoder model for breakdance video classification.",
    "authors": [
      "Sauptik Dhar",
      "Naveen Ramakrishnan",
      "Michelle Munson"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20287v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20287v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2510.20807v1",
    "title": "Video Prediction of Dynamic Physical Simulations With Pixel-Space   Spatiotemporal Transformers",
    "summary": "Inspired by the performance and scalability of autoregressive large language models (LLMs), transformer-based models have seen recent success in the visual domain. This study investigates a transformer adaptation for video prediction with a simple end-to-end approach, comparing various spatiotemporal self-attention layouts. Focusing on causal modeling of physical simulations over time; a common shortcoming of existing video-generative approaches, we attempt to isolate spatiotemporal reasoning via physical object tracking metrics and unsupervised training on physical simulation datasets. We introduce a simple yet effective pure transformer model for autoregressive video prediction, utilizing continuous pixel-space representations for video prediction. Without the need for complex training strategies or latent feature-learning components, our approach significantly extends the time horizon for physically accurate predictions by up to 50% when compared with existing latent-space approaches, while maintaining comparable performance on common video quality metrics. In addition, we conduct interpretability experiments to identify network regions that encode information useful to perform accurate estimations of PDE simulation parameters via probing models, and find that this generalizes to the estimation of out-of-distribution simulation parameters. This work serves as a platform for further attention-based spatiotemporal modeling of videos via a simple, parameter efficient, and interpretable approach.",
    "authors": [
      "Dean L Slack",
      "G Thomas Hudson",
      "Thomas Winterbottom",
      "Noura Al Moubayed"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20807v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20807v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2510.20774v1",
    "title": "FieldGen: From Teleoperated Pre-Manipulation Trajectories to   Field-Guided Data Generation",
    "summary": "Large-scale and diverse datasets are vital for training robust robotic manipulation policies, yet existing data collection methods struggle to balance scale, diversity, and quality. Simulation offers scalability but suffers from sim-to-real gaps, while teleoperation yields high-quality demonstrations with limited diversity and high labor cost. We introduce FieldGen, a field-guided data generation framework that enables scalable, diverse, and high-quality real-world data collection with minimal human supervision. FieldGen decomposes manipulation into two stages: a pre-manipulation phase, allowing trajectory diversity, and a fine manipulation phase requiring expert precision. Human demonstrations capture key contact and pose information, after which an attraction field automatically generates diverse trajectories converging to successful configurations. This decoupled design combines scalable trajectory diversity with precise supervision. Moreover, FieldGen-Reward augments generated data with reward annotations to further enhance policy learning. Experiments demonstrate that policies trained with FieldGen achieve higher success rates and improved stability compared to teleoperation-based baselines, while significantly reducing human effort in long-term real-world data collection. Webpage is available at https://fieldgen.github.io/.",
    "authors": [
      "Wenhao Wang",
      "Kehe Ye",
      "Xinyu Zhou",
      "Tianxing Chen",
      "Cao Min",
      "Qiaoming Zhu",
      "Xiaokang Yang",
      "Yongjian Shen",
      "Yang Yang",
      "Maoqing Yao",
      "Yao Mu"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20774v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20774v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2510.20768v1",
    "title": "RAGRank: Using PageRank to Counter Poisoning in CTI LLM Pipelines",
    "summary": "Retrieval-Augmented Generation (RAG) has emerged as the dominant architectural pattern to operationalize Large Language Model (LLM) usage in Cyber Threat Intelligence (CTI) systems. However, this design is susceptible to poisoning attacks, and previously proposed defenses can fail for CTI contexts as cyber threat information is often completely new for emerging attacks, and sophisticated threat actors can mimic legitimate formats, terminology, and stylistic conventions. To address this issue, we propose that the robustness of modern RAG defenses can be accelerated by applying source credibility algorithms on corpora, using PageRank as an example. In our experiments, we demonstrate quantitatively that our algorithm applies a lower authority score to malicious documents while promoting trusted content, using the standardized MS MARCO dataset. We also demonstrate proof-of-concept performance of our algorithm on CTI documents and feeds.",
    "authors": [
      "Austin Jia",
      "Avaneesh Ramesh",
      "Zain Shamsi",
      "Daniel Zhang",
      "Alex Liu"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20768v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20768v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2510.20714v1",
    "title": "Optimizing Clinical Fall Risk Prediction: A Data-Driven Integration of   EHR Variables with the Johns Hopkins Fall Risk Assessment Tool",
    "summary": "In this study we aim to better align fall risk prediction from the Johns Hopkins Fall Risk Assessment Tool (JHFRAT) with additional clinically meaningful measures via a data-driven modelling approach. We conducted a retrospective analysis of 54,209 inpatient admissions from three Johns Hopkins Health System hospitals between March 2022 and October 2023. A total of 20,208 admissions were included as high fall risk encounters, and 13,941 were included as low fall risk encounters. To incorporate clinical knowledge and maintain interpretability, we employed constrained score optimization (CSO) models on JHFRAT assessment data and additional electronic health record (EHR) variables. The model demonstrated significant improvements in predictive performance over the current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). The constrained score optimization models performed similarly with and without the EHR variables. Although the benchmark black-box model (XGBoost), improves upon the performance metrics of the knowledge-based constrained logistic regression (AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risk labelling. This evidence-based approach provides a robust foundation for health systems to systematically enhance inpatient fall prevention protocols and patient safety using data-driven optimization techniques, contributing to improved risk assessment and resource allocation in healthcare settings.",
    "authors": [
      "Fardin Ganjkhanloo",
      "Emmett Springer",
      "Erik H. Hoyer",
      "Daniel L. Young",
      "Kimia Ghobadi"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20714v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20714v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2510.20666v1",
    "title": "Bayesian Jammer Localization with a Hybrid CNN and Path-Loss Mixture of   Experts",
    "summary": "Global Navigation Satellite System (GNSS) signals are vulnerable to jamming, particularly in urban areas where multipath and shadowing distort received power. Previous data-driven approaches achieved reasonable localization but poorly reconstructed the received signal strength (RSS) field due to limited spatial context. We propose a hybrid Bayesian mixture-of-experts framework that fuses a physical path-loss (PL) model and a convolutional neural network (CNN) through log-linear pooling. The PL expert ensures physical consistency, while the CNN leverages building-height maps to capture urban propagation effects. Bayesian inference with Laplace approximation provides posterior uncertainty over both the jammer position and RSS field. Experiments on urban ray-tracing data show that localization accuracy improves and uncertainty decreases with more training points, while uncertainty concentrates near the jammer and along urban canyons where propagation is most sensitive.",
    "authors": [
      "Mariona Jaramillo-Civill",
      "Luis Gonz\u00e1lez-Gudi\u00f1o",
      "Tales Imbiriba",
      "Pau Closas"
    ],
    "categories": [
      "cs.LG",
      "eess.SP",
      "68T05, 68T07, 62F15, 94A12"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20666v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20666v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2510.20653v1",
    "title": "Finding the Sweet Spot: Trading Quality, Cost, and Speed During   Inference-Time LLM Reflection",
    "summary": "As Large Language Models (LLMs) continue to evolve, practitioners face increasing options for enhancing inference-time performance without model retraining, including budget tuning and multi-step techniques like self-reflection. While these methods improve output quality, they create complex trade-offs among accuracy, cost, and latency that remain poorly understood across different domains. This paper systematically compares self-reflection and budget tuning across mathematical reasoning and translation tasks. We evaluate prominent LLMs, including Anthropic Claude, Amazon Nova, and Mistral families, along with other models under varying reflection depths and compute budgets to derive Pareto optimal performance frontiers. Our analysis reveals substantial domain dependent variation in self-reflection effectiveness, with performance gains up to 220\\% in mathematical reasoning. We further investigate how reflection round depth and feedback mechanism quality influence performance across model families. To validate our findings in a real-world setting, we deploy a self-reflection enhanced marketing content localisation system at Lounge by Zalando, where it shows market-dependent effectiveness, reinforcing the importance of domain specific evaluation when deploying these techniques. Our results provide actionable guidance for selecting optimal inference strategies given specific domains and resource constraints. We open source our self-reflection implementation for reproducibility at https://github.com/aws-samples/sample-genai-reflection-for-bedrock.",
    "authors": [
      "Jack Butler",
      "Nikita Kozodoi",
      "Zainab Afolabi",
      "Brian Tyacke",
      "Gaiar Baimuratov"
    ],
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20653v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20653v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2510.20635v1",
    "title": "Why Did Apple Fall To The Ground: Evaluating Curiosity In Large Language   Model",
    "summary": "Curiosity serves as a pivotal conduit for human beings to discover and learn new knowledge. Recent advancements of large language models (LLMs) in natural language processing have sparked discussions regarding whether these models possess capability of curiosity-driven learning akin to humans. In this paper, starting from the human curiosity assessment questionnaire Five-Dimensional Curiosity scale Revised (5DCR), we design a comprehensive evaluation framework that covers dimensions such as Information Seeking, Thrill Seeking, and Social Curiosity to assess the extent of curiosity exhibited by LLMs. The results demonstrate that LLMs exhibit a stronger thirst for knowledge than humans but still tend to make conservative choices when faced with uncertain environments. We further investigated the relationship between curiosity and thinking of LLMs, confirming that curious behaviors can enhance the model's reasoning and active learning abilities. These findings suggest that LLMs have the potential to exhibit curiosity similar to that of humans, providing experimental support for the future development of learning capabilities and innovative research in LLMs.",
    "authors": [
      "Haoyu Wang",
      "Sihang Jiang",
      "Yuyan Chen",
      "Yitong Wang",
      "Yanghua Xiao"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20635v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20635v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2510.20629v1",
    "title": "Equitable Survival Prediction: A Fairness-Aware Survival Modeling (FASM)   Approach",
    "summary": "As machine learning models become increasingly integrated into healthcare, structural inequities and social biases embedded in clinical data can be perpetuated or even amplified by data-driven models. In survival analysis, censoring and time dynamics can further add complexity to fair model development. Additionally, algorithmic fairness approaches often overlook disparities in cross-group rankings, e.g., high-risk Black patients may be ranked below lower-risk White patients who do not experience the event of mortality. Such misranking can reinforce biological essentialism and undermine equitable care. We propose a Fairness-Aware Survival Modeling (FASM), designed to mitigate algorithmic bias regarding both intra-group and cross-group risk rankings over time. Using breast cancer prognosis as a representative case and applying FASM to SEER breast cancer data, we show that FASM substantially improves fairness while preserving discrimination performance comparable to fairness-unaware survival models. Time-stratified evaluations show that FASM maintains stable fairness over a 10-year horizon, with the greatest improvements observed during the mid-term of follow-up. Our approach enables the development of survival models that prioritize both accuracy and equity in clinical decision-making, advancing fairness as a core principle in clinical care.",
    "authors": [
      "Mingxuan Liu",
      "Yilin Ning",
      "Haoyuan Wang",
      "Chuan Hong",
      "Matthew Engelhard",
      "Danielle S. Bitterman",
      "William G. La Cava",
      "Nan Liu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20629v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20629v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2510.20621v1",
    "title": "Towards the Formalization of a Trustworthy AI for Mining Interpretable   Models explOiting Sophisticated Algorithms",
    "summary": "Interpretable-by-design models are crucial for fostering trust, accountability, and safe adoption of automated decision-making models in real-world applications. In this paper we formalize the ground for the MIMOSA (Mining Interpretable Models explOiting Sophisticated Algorithms) framework, a comprehensive methodology for generating predictive models that balance interpretability with performance while embedding key ethical properties. We formally define here the supervised learning setting across diverse decision-making tasks and data types, including tabular data, time series, images, text, transactions, and trajectories. We characterize three major families of interpretable models: feature importance, rule, and instance based models. For each family, we analyze their interpretability dimensions, reasoning mechanisms, and complexity. Beyond interpretability, we formalize three critical ethical properties, namely causality, fairness, and privacy, providing formal definitions, evaluation metrics, and verification procedures for each. We then examine the inherent trade-offs between these properties and discuss how privacy requirements, fairness constraints, and causal reasoning can be embedded within interpretable pipelines. By evaluating ethical measures during model generation, this framework establishes the theoretical foundations for developing AI systems that are not only accurate and interpretable but also fair, privacy-preserving, and causally aware, i.e., trustworthy.",
    "authors": [
      "Riccardo Guidotti",
      "Martina Cinquini",
      "Marta Marchiori Manerba",
      "Mattia Setzu",
      "Francesco Spinnato"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20621v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20621v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2510.20548v1",
    "title": "GlobalRAG: Enhancing Global Reasoning in Multi-hop Question Answering   via Reinforcement Learning",
    "summary": "Reinforcement learning has recently shown promise in improving retrieval-augmented generation (RAG). Despite these advances, its effectiveness in multi-hop question answering (QA) remains limited by two fundamental limitations: (i) global planning absence to structure multi-step reasoning, and (ii) unfaithful execution, which hinders effective query formulation and consistent use of retrieved evidence. We propose GlobalRAG, a reinforcement learning framework designed to enhance global reasoning in multi-hop QA. GlobalRAG decomposes questions into subgoals, coordinates retrieval with reasoning, and refines evidence iteratively. To guide this process, we introduce Planning Quality Reward and SubGoal Completion Reward, which encourage coherent planning and reliable subgoal execution. In addition, a progressive weight annealing strategy balances process-oriented and outcome-based objectives. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that GlobalRAG significantly outperforms strong baselines while using only 8k training data (42% of the training data used by strong baselines), achieving average improvements of 14.2% in both EM and F1.",
    "authors": [
      "Jinchang Luo",
      "Mingquan Cheng",
      "Fan Wan",
      "Ni Li",
      "Xiaoling Xia",
      "Shuangshuang Tian",
      "Tingcheng Bian",
      "Haiwei Wang",
      "Haohuan Fu",
      "Yan Tao"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20548v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20548v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2510.20327v1",
    "title": "LEGO: A Lightweight and Efficient Multiple-Attribute Unlearning   Framework for Recommender Systems",
    "summary": "With the growing demand for safeguarding sensitive user information in recommender systems, recommendation attribute unlearning is receiving increasing attention. Existing studies predominantly focus on single-attribute unlearning. However, privacy protection requirements in the real world often involve multiple sensitive attributes and are dynamic. Existing single-attribute unlearning methods cannot meet these real-world requirements due to i) CH1: the inability to handle multiple unlearning requests simultaneously, and ii) CH2: the lack of efficient adaptability to dynamic unlearning needs. To address these challenges, we propose LEGO, a lightweight and efficient multiple-attribute unlearning framework. Specifically, we divide the multiple-attribute unlearning process into two steps: i) Embedding Calibration removes information related to a specific attribute from user embedding, and ii) Flexible Combination combines these embeddings into a single embedding, protecting all sensitive attributes. We frame the unlearning process as a mutual information minimization problem, providing LEGO a theoretical guarantee of simultaneous unlearning, thereby addressing CH1. With the two-step framework, where Embedding Calibration can be performed in parallel and Flexible Combination is flexible and efficient, we address CH2. Extensive experiments on three real-world datasets across three representative recommendation models demonstrate the effectiveness and efficiency of our proposed framework. Our code and appendix are available at https://github.com/anonymifish/lego-rec-multiple-attribute-unlearning.",
    "authors": [
      "Fengyuan Yu",
      "Yuyuan Li",
      "Xiaohua Feng",
      "Junjie Fang",
      "Tao Wang",
      "Chaochao Chen"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20327v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20327v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2510.20739v1",
    "title": "Learning to Triage Taint Flows Reported by Dynamic Program Analysis in   Node.js Packages",
    "summary": "Program analysis tools often produce large volumes of candidate vulnerability reports that require costly manual review, creating a practical challenge: how can security analysts prioritize the reports most likely to be true vulnerabilities?   This paper investigates whether machine learning can be applied to prioritizing vulnerabilities reported by program analysis tools. We focus on Node.js packages and collect a benchmark of 1,883 Node.js packages, each containing one reported ACE or ACI vulnerability. We evaluate a variety of machine learning approaches, including classical models, graph neural networks (GNNs), large language models (LLMs), and hybrid models that combine GNN and LLMs, trained on data based on a dynamic program analysis tool's output. The top LLM achieves $F_{1} {=} 0.915$, while the best GNN and classical ML models reaching $F_{1} {=} 0.904$. At a less than 7% false-negative rate, the leading model eliminates 66.9% of benign packages from manual review, taking around 60 ms per package. If the best model is tuned to operate at a precision level of 0.8 (i.e., allowing 20% false positives amongst all warnings), our approach can detect 99.2% of exploitable taint flows while missing only 0.8%, demonstrating strong potential for real-world vulnerability triage.",
    "authors": [
      "Ronghao Ni",
      "Aidan Z. H. Yang",
      "Min-Chien Hsu",
      "Nuno Sabino",
      "Limin Jia",
      "Ruben Martins",
      "Darion Cassel",
      "Kevin Cheang"
    ],
    "categories": [
      "cs.CR",
      "cs.LG",
      "cs.SE"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20739v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20739v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2510.20733v1",
    "title": "Thought Communication in Multiagent Collaboration",
    "summary": "Natural language has long enabled human cooperation, but its lossy, ambiguous, and indirect nature limits the potential of collective intelligence. While machines are not subject to these constraints, most LLM-based multi-agent systems still rely solely on natural language, exchanging tokens or their embeddings. To go beyond language, we introduce a new paradigm, thought communication, which enables agents to interact directly mind-to-mind, akin to telepathy. To uncover these latent thoughts in a principled way, we formalize the process as a general latent variable model, where agent states are generated by an unknown function of underlying thoughts. We prove that, in a nonparametric setting without auxiliary information, both shared and private latent thoughts between any pair of agents can be identified. Moreover, the global structure of thought sharing, including which agents share which thoughts and how these relationships are structured, can also be recovered with theoretical guarantees. Guided by the established theory, we develop a framework that extracts latent thoughts from all agents prior to communication and assigns each agent the relevant thoughts, along with their sharing patterns. This paradigm naturally extends beyond LLMs to all modalities, as most observational data arise from hidden generative processes. Experiments on both synthetic and real-world benchmarks validate the theory and demonstrate the collaborative advantages of thought communication. We hope this work illuminates the potential of leveraging the hidden world, as many challenges remain unsolvable through surface-level observation alone, regardless of compute or data scale.",
    "authors": [
      "Yujia Zheng",
      "Zhuokai Zhao",
      "Zijian Li",
      "Yaqi Xie",
      "Mingze Gao",
      "Lizhu Zhang",
      "Kun Zhang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20733v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20733v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2510.20627v1",
    "title": "H-SPLID: HSIC-based Saliency Preserving Latent Information Decomposition",
    "summary": "We introduce H-SPLID, a novel algorithm for learning salient feature representations through the explicit decomposition of salient and non-salient features into separate spaces. We show that H-SPLID promotes learning low-dimensional, task-relevant features. We prove that the expected prediction deviation under input perturbations is upper-bounded by the dimension of the salient subspace and the Hilbert-Schmidt Independence Criterion (HSIC) between inputs and representations. This establishes a link between robustness and latent representation compression in terms of the dimensionality and information preserved. Empirical evaluations on image classification tasks show that models trained with H-SPLID primarily rely on salient input components, as indicated by reduced sensitivity to perturbations affecting non-salient features, such as image backgrounds. Our code is available at https://github.com/neu-spiral/H-SPLID.",
    "authors": [
      "Lukas Miklautz",
      "Chengzhi Shi",
      "Andrii Shkabrii",
      "Theodoros Thirimachos Davarakis",
      "Prudence Lam",
      "Claudia Plant",
      "Jennifer Dy",
      "Stratis Ioannidis"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20627v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20627v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2510.20622v1",
    "title": "SeViCES: Unifying Semantic-Visual Evidence Consensus for Long Video   Understanding",
    "summary": "Long video understanding remains challenging due to its complex, diverse, and temporally scattered content. Although video large language models (Video-LLMs) can process videos lasting tens of minutes, applying them to truly long sequences is computationally prohibitive and often leads to unfocused or inconsistent reasoning. A promising solution is to select only the most informative frames, yet existing approaches typically ignore temporal dependencies or rely on unimodal evidence, limiting their ability to provide complete and query-relevant context. We propose a Semantic-Visual Consensus Evidence Selection (SeViCES) framework for effective and reliable long video understanding. SeViCES is training-free and model-agnostic, and introduces two key components. The Semantic-Visual Consensus Frame Selection (SVCFS) module selects frames through (1) a temporal-aware semantic branch that leverages LLM reasoning over captions, and (2) a cluster-guided visual branch that aligns embeddings with semantic scores via mutual information. The Answer Consensus Refinement (ACR) module further resolves inconsistencies between semantic- and visual-based predictions by fusing evidence and constraining the answer space. Extensive experiments on long video understanding benchmarks show that SeViCES consistently outperforms state-of-the-art methods in both accuracy and robustness, demonstrating the importance of consensus-driven evidence selection for Video-LLMs.",
    "authors": [
      "Yuan Sheng",
      "Yanbin Hao",
      "Chenxu Li",
      "Shuo Wang",
      "Xiangnan He"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20622v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20622v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2510.20616v1",
    "title": "On Optimal Hyperparameters for Differentially Private Deep Transfer   Learning",
    "summary": "Differentially private (DP) transfer learning, i.e., fine-tuning a pretrained model on private data, is the current state-of-the-art approach for training large models under privacy constraints. We focus on two key hyperparameters in this setting: the clipping bound $C$ and batch size $B$. We show a clear mismatch between the current theoretical understanding of how to choose an optimal $C$ (stronger privacy requires smaller $C$) and empirical outcomes (larger $C$ performs better under strong privacy), caused by changes in the gradient distributions. Assuming a limited compute budget (fixed epochs), we demonstrate that the existing heuristics for tuning $B$ do not work, while cumulative DP noise better explains whether smaller or larger batches perform better. We also highlight how the common practice of using a single $(C,B)$ setting across tasks can lead to suboptimal performance. We find that performance drops especially when moving between loose and tight privacy and between plentiful and limited compute, which we explain by analyzing clipping as a form of gradient re-weighting and examining cumulative DP noise.",
    "authors": [
      "Aki Rehn",
      "Linzh Zhao",
      "Mikko A. Heikkil\u00e4",
      "Antti Honkela"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20616v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20616v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2510.20508v1",
    "title": "Assessing the Political Fairness of Multilingual LLMs: A Case Study   based on a 21-way Multiparallel EuroParl Dataset",
    "summary": "The political biases of Large Language Models (LLMs) are usually assessed by simulating their answers to English surveys. In this work, we propose an alternative framing of political biases, relying on principles of fairness in multilingual translation. We systematically compare the translation quality of speeches in the European Parliament (EP), observing systematic differences with majority parties from left, center, and right being better translated than outsider parties. This study is made possible by a new, 21-way multiparallel version of EuroParl, the parliamentary proceedings of the EP, which includes the political affiliations of each speaker. The dataset consists of 1.5M sentences for a total of 40M words and 249M characters. It covers three years, 1000+ speakers, 7 countries, 12 EU parties, 25 EU committees, and hundreds of national parties.",
    "authors": [
      "Paul Lerner",
      "Fran\u00e7ois Yvon"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20508v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20508v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2510.20487v1",
    "title": "Steering Evaluation-Aware Language Models To Act Like They Are Deployed",
    "summary": "Large language models (LLMs) can sometimes detect when they are being evaluated and adjust their behavior to appear more aligned, compromising the reliability of safety evaluations. In this paper, we show that adding a steering vector to an LLM's activations can suppress evaluation-awareness and make the model act like it is deployed during evaluation. To study our steering technique, we train an LLM to exhibit evaluation-aware behavior using a two-step training process designed to mimic how this behavior could emerge naturally. First, we perform continued pretraining on documents with factual descriptions of the model (1) using Python type hints during evaluation but not during deployment and (2) recognizing that the presence of a certain evaluation cue always means that it is being tested. Then, we train the model with expert iteration to use Python type hints in evaluation settings. The resulting model is evaluation-aware: it writes type hints in evaluation contexts more than deployment contexts. However, this gap can only be observed by removing the evaluation cue. We find that activation steering can suppress evaluation awareness and make the model act like it is deployed even when the cue is present. Importantly, we constructed our steering vector using the original model before our additional training. Our results suggest that AI evaluators could improve the reliability of safety evaluations by steering models to act like they are deployed.",
    "authors": [
      "Tim Tian Hua",
      "Andrew Qin",
      "Samuel Marks",
      "Neel Nanda"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20487v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20487v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2510.20358v1",
    "title": "Dialogue Is Not Enough to Make a Communicative BabyLM (But Neither Is   Developmentally Inspired Reinforcement Learning)",
    "summary": "We investigate whether pre-training exclusively on dialogue data results in formally and functionally apt small language models. Based on this pre-trained llamalogue model, we employ a variety of fine-tuning strategies to enforce \"more communicative\" text generations by our models. Although our models underperform on most standard BabyLM benchmarks, they excel at dialogue continuation prediction in a minimal pair setting. While PPO fine-tuning has mixed to adversarial effects on our models, DPO fine-tuning further improves their performance on our custom dialogue benchmark.",
    "authors": [
      "Francesca Padovani",
      "Bastian Bunzeck",
      "Manar Ali",
      "Omar Momen",
      "Arianna Bisazza",
      "Hendrik Buschmeier",
      "Sina Zarrie\u00df"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20358v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20358v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2510.20322v1",
    "title": "HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large   Language Models",
    "summary": "Multi-modal large language models (MLLMs) have emerged as a transformative approach for aligning visual and textual understanding. They typically require extremely high computational resources (e.g., thousands of GPUs) for training to achieve cross-modal alignment at multi-granularity levels. We argue that a key source of this inefficiency lies in the vision encoders they widely equip with, e.g., CLIP and SAM, which lack the alignment with language at multi-granularity levels. To address this issue, in this paper, we leverage hyperbolic space, which inherently models hierarchical levels and thus provides a principled framework for bridging the granularity gap between visual and textual modalities at an arbitrary granularity level. Concretely, we propose an efficient training paradigm for MLLMs, dubbed as HyperET, which can optimize visual representations to align with their textual counterparts at an arbitrary granularity level through dynamic hyperbolic radius adjustment in hyperbolic space. HyperET employs learnable matrices with M\\\"{o}bius multiplication operations, implemented via three effective configurations: diagonal scaling matrices, block-diagonal matrices, and banded matrices, providing a flexible yet efficient parametrization strategy. Comprehensive experiments across multiple MLLM benchmarks demonstrate that HyperET consistently improves both existing pre-training and fine-tuning MLLMs clearly with less than 1\\% additional parameters.",
    "authors": [
      "Zelin Peng",
      "Zhengqin Xu",
      "Qingyang Liu",
      "Xiaokang Yang",
      "Wei Shen"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20322v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20322v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2510.20291v1",
    "title": "A Parameter-Efficient Mixture-of-Experts Framework for Cross-Modal   Geo-Localization",
    "summary": "We present a winning solution to RoboSense 2025 Track 4: Cross-Modal Drone Navigation. The task retrieves the most relevant geo-referenced image from a large multi-platform corpus (satellite/drone/ground) given a natural-language query. Two obstacles are severe inter-platform heterogeneity and a domain gap between generic training descriptions and platform-specific test queries. We mitigate these with a domain-aligned preprocessing pipeline and a Mixture-of-Experts (MoE) framework: (i) platform-wise partitioning, satellite augmentation, and removal of orientation words; (ii) an LLM-based caption refinement pipeline to align textual semantics with the distinct visual characteristics of each platform. Using BGE-M3 (text) and EVA-CLIP (image), we train three platform experts using a progressive two-stage, hard-negative mining strategy to enhance discriminative power, and fuse their scores at inference. The system tops the official leaderboard, demonstrating robust cross-modal geo-localization under heterogeneous viewpoints.",
    "authors": [
      "LinFeng Li",
      "Jian Zhao",
      "Zepeng Yang",
      "Yuhang Song",
      "Bojun Lin",
      "Tianle Zhang",
      "Yuchen Yuan",
      "Chi Zhang",
      "Xuelong Li"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20291v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20291v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2510.20809v1",
    "title": "Real Deep Research for AI, Robotics and Beyond",
    "summary": "With the rapid growth of research in AI and robotics now producing over 10,000 papers annually it has become increasingly difficult for researchers to stay up to date. Fast evolving trends, the rise of interdisciplinary work, and the need to explore domains beyond one's expertise all contribute to this challenge. To address these issues, we propose a generalizable pipeline capable of systematically analyzing any research area: identifying emerging trends, uncovering cross domain opportunities, and offering concrete starting points for new inquiry. In this work, we present Real Deep Research (RDR) a comprehensive framework applied to the domains of AI and robotics, with a particular focus on foundation models and robotics advancements. We also briefly extend our analysis to other areas of science. The main paper details the construction of the RDR pipeline, while the appendix provides extensive results across each analyzed topic. We hope this work sheds light for researchers working in the field of AI and beyond.",
    "authors": [
      "Xueyan Zou",
      "Jianglong Ye",
      "Hao Zhang",
      "Xiaoyu Xiang",
      "Mingyu Ding",
      "Zhaojing Yang",
      "Yong Jae Lee",
      "Zhuowen Tu",
      "Sifei Liu",
      "Xiaolong Wang"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20809v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20809v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2510.20696v1",
    "title": "Diagnosing Visual Reasoning: Challenges, Insights, and a Path Forward",
    "summary": "Multimodal large language models (MLLMs) that integrate visual and textual reasoning leverage chain-of-thought (CoT) prompting to tackle complex visual tasks, yet continue to exhibit visual hallucinations and an over-reliance on textual priors. We present a systematic diagnosis of state-of-the-art vision-language models using a three-stage evaluation framework, uncovering key failure modes. To address these, we propose an agent-based architecture that combines LLM reasoning with lightweight visual modules, enabling fine-grained analysis and iterative refinement of reasoning chains. Our results highlight future visual reasoning models should focus on integrating a broader set of specialized tools for analyzing visual content. Our system achieves significant gains (+10.3 on MMMU, +6.0 on MathVista over a 7B baseline), matching or surpassing much larger models. We will release our framework and evaluation suite to facilitate future research.",
    "authors": [
      "Jing Bi",
      "Guangyu Sun",
      "Ali Vosoughi",
      "Chen Chen",
      "Chenliang Xu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20696v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20696v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2510.20630v1",
    "title": "Quantum Processing Unit (QPU) processing time Prediction with Machine   Learning",
    "summary": "This paper explores the application of machine learning (ML) techniques in predicting the QPU processing time of quantum jobs. By leveraging ML algorithms, this study introduces predictive models that are designed to enhance operational efficiency in quantum computing systems. Using a dataset of about 150,000 jobs that follow the IBM Quantum schema, we employ ML methods based on Gradient-Boosting (LightGBM) to predict the QPU processing times, incorporating data preprocessing methods to improve model accuracy. The results demonstrate the effectiveness of ML in forecasting quantum jobs. This improvement can have implications on improving resource management and scheduling within quantum computing frameworks. This research not only highlights the potential of ML in refining quantum job predictions but also sets a foundation for integrating AI-driven tools in advanced quantum computing operations.",
    "authors": [
      "Lucy Xing",
      "Sanjay Vishwakarma",
      "David Kremer",
      "Francisco Martin-Fernandez",
      "Ismael Faro",
      "Juan Cruz-Benito"
    ],
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20630v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20630v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2510.20486v1",
    "title": "Hurdle-IMDL: An Imbalanced Learning Framework for Infrared Rainfall   Retrieval",
    "summary": "Artificial intelligence has advanced quantitative remote sensing, yet its effectiveness is constrained by imbalanced label distribution. This imbalance leads conventionally trained models to favor common samples, which in turn degrades retrieval performance for rare ones. Rainfall retrieval exemplifies this issue, with performance particularly compromised for heavy rain. This study proposes Hurdle-Inversion Model Debiasing Learning (IMDL) framework. Following a divide-and-conquer strategy, imbalance in the rain distribution is decomposed into two components: zero inflation, defined by the predominance of non-rain samples; and long tail, defined by the disproportionate abundance of light-rain samples relative to heavy-rain samples. A hurdle model is adopted to handle the zero inflation, while IMDL is proposed to address the long tail by transforming the learning object into an unbiased ideal inverse model. Comprehensive evaluation via statistical metrics and case studies investigating rainy weather in eastern China confirms Hurdle-IMDL's superiority over conventional, cost-sensitive, generative, and multi-task learning methods. Its key advancements include effective mitigation of systematic underestimation and a marked improvement in the retrieval of heavy-to-extreme rain. IMDL offers a generalizable approach for addressing imbalance in distributions of environmental variables, enabling enhanced retrieval of rare yet high-impact events.",
    "authors": [
      "Fangjian Zhang",
      "Xiaoyong Zhuge",
      "Wenlan Wang",
      "Haixia Xiao",
      "Yuying Zhu",
      "Siyang Cheng"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.ao-ph",
      "physics.geo-ph"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20486v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20486v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2510.20386v1",
    "title": "NeoDictaBERT: Pushing the Frontier of BERT models for Hebrew",
    "summary": "Since their initial release, BERT models have demonstrated exceptional performance on a variety of tasks, despite their relatively small size (BERT-base has ~100M parameters). Nevertheless, the architectural choices used in these models are outdated compared to newer transformer-based models such as Llama3 and Qwen3. In recent months, several architectures have been proposed to close this gap. ModernBERT and NeoBERT both show strong improvements on English benchmarks and significantly extend the supported context window. Following their successes, we introduce NeoDictaBERT and NeoDictaBERT-bilingual: BERT-style models trained using the same architecture as NeoBERT, with a dedicated focus on Hebrew texts. These models outperform existing ones on almost all Hebrew benchmarks and provide a strong foundation for downstream tasks. Notably, the NeoDictaBERT-bilingual model shows strong results on retrieval tasks, outperforming other multilingual models of similar size. In this paper, we describe the training process and report results across various benchmarks. We release the models to the community as part of our goal to advance research and development in Hebrew NLP.",
    "authors": [
      "Shaltiel Shmidman",
      "Avi Shmidman",
      "Moshe Koppel"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20386v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20386v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2510.20383v1",
    "title": "Hierarchical Time Series Forecasting with Robust Reconciliation",
    "summary": "This paper focuses on forecasting hierarchical time-series data, where each higher-level observation equals the sum of its corresponding lower-level time series. In such contexts, the forecast values should be coherent, meaning that the forecast value of each parent series exactly matches the sum of the forecast values of its child series. Existing hierarchical forecasting methods typically generate base forecasts independently for each series and then apply a reconciliation procedure to adjust them so that the resulting forecast values are coherent across the hierarchy. These methods generally derive an optimal reconciliation, using a covariance matrix of the forecast error. In practice, however, the true covariance matrix is unknown and has to be estimated from finite samples in advance. This gap between the true and estimated covariance matrix may degrade forecast performance. To address this issue, we propose a robust optimization framework for hierarchical reconciliation that accounts for uncertainty in the estimated covariance matrix. We first introduce an uncertainty set for the estimated covariance matrix and formulate a reconciliation problem that minimizes the worst-case expected squared error over this uncertainty set. We show that our problem can be cast as a semidefinite optimization problem. Numerical experiments demonstrate that the proposed robust reconciliation method achieved better forecast performance than existing hierarchical forecasting methods, which indicates the effectiveness of integrating uncertainty into the reconciliation process.",
    "authors": [
      "Shuhei Aikawa",
      "Aru Suzuki",
      "Kei Yoshitake",
      "Kanata Teshigawara",
      "Akira Iwabuchi",
      "Ken Kobayashi",
      "Kazuhide Nakata"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20383v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20383v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2510.20363v1",
    "title": "A Transformer Inspired AI-based MIMO receiver",
    "summary": "We present AttDet, a Transformer-inspired MIMO (Multiple Input Multiple Output) detection method that treats each transmit layer as a token and learns inter-stream interference via a lightweight self-attention mechanism. Queries and keys are derived directly from the estimated channel matrix, so attention scores quantify channel correlation. Values are initialized by matched-filter outputs and iteratively refined. The AttDet design combines model-based interpretability with data-driven flexibility. We demonstrate through link-level simulations under realistic 5G channel models and high-order, mixed QAM modulation and coding schemes, that AttDet can approach near-optimal BER/BLER (Bit Error Rate/Block Error Rate) performance while maintaining predictable, polynomial complexity.",
    "authors": [
      "Andr\u00e1s R\u00e1cz",
      "Tam\u00e1s Borsos",
      "Andr\u00e1s Veres",
      "Benedek Csala"
    ],
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20363v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20363v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2510.20349v1",
    "title": "Synthetic Data for Robust Runway Detection",
    "summary": "Deep vision models are now mature enough to be integrated in industrial and possibly critical applications such as autonomous navigation. Yet, data collection and labeling to train such models requires too much efforts and costs for a single company or product. This drawback is more significant in critical applications, where training data must include all possible conditions including rare scenarios. In this perspective, generating synthetic images is an appealing solution, since it allows a cheap yet reliable covering of all the conditions and environments, if the impact of the synthetic-to-real distribution shift is mitigated. In this article, we consider the case of runway detection that is a critical part in autonomous landing systems developed by aircraft manufacturers. We propose an image generation approach based on a commercial flight simulator that complements a few annotated real images. By controlling the image generation and the integration of real and synthetic data, we show that standard object detection models can achieve accurate prediction. We also evaluate their robustness with respect to adverse conditions, in our case nighttime images, that were not represented in the real data, and show the interest of using a customized domain adaptation strategy.",
    "authors": [
      "Estelle Chigot",
      "Dennis G. Wilson",
      "Meriem Ghrib",
      "Fabrice Jimenez",
      "Thomas Oberlin"
    ],
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20349v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20349v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2510.20813v1",
    "title": "GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic   Manipulation",
    "summary": "This paper presents GSWorld, a robust, photo-realistic simulator for robotics manipulation that combines 3D Gaussian Splatting with physics engines. Our framework advocates \"closing the loop\" of developing manipulation policies with reproducible evaluation of policies learned from real-robot data and sim2real policy training without using real robots. To enable photo-realistic rendering of diverse scenes, we propose a new asset format, which we term GSDF (Gaussian Scene Description File), that infuses Gaussian-on-Mesh representation with robot URDF and other objects. With a streamlined reconstruction pipeline, we curate a database of GSDF that contains 3 robot embodiments for single-arm and bimanual manipulation, as well as more than 40 objects. Combining GSDF with physics engines, we demonstrate several immediate interesting applications: (1) learning zero-shot sim2real pixel-to-action manipulation policy with photo-realistic rendering, (2) automated high-quality DAgger data collection for adapting policies to deployment environments, (3) reproducible benchmarking of real-robot manipulation policies in simulation, (4) simulation data collection by virtual teleoperation, and (5) zero-shot sim2real visual reinforcement learning. Website: https://3dgsworld.github.io/.",
    "authors": [
      "Guangqi Jiang",
      "Haoran Chang",
      "Ri-Zhao Qiu",
      "Yutong Liang",
      "Mazeyu Ji",
      "Jiyue Zhu",
      "Zhao Dong",
      "Xueyan Zou",
      "Xiaolong Wang"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20813v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20813v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2510.20783v1",
    "title": "Out-of-distribution Tests Reveal Compositionality in Chess Transformers",
    "summary": "Chess is a canonical example of a task that requires rigorous reasoning and long-term planning. Modern decision Transformers - trained similarly to LLMs - are able to learn competent gameplay, but it is unclear to what extent they truly capture the rules of chess. To investigate this, we train a 270M parameter chess Transformer and test it on out-of-distribution scenarios, designed to reveal failures of systematic generalization. Our analysis shows that Transformers exhibit compositional generalization, as evidenced by strong rule extrapolation: they adhere to fundamental syntactic rules of the game by consistently choosing valid moves even in situations very different from the training data. Moreover, they also generate high-quality moves for OOD puzzles. In a more challenging test, we evaluate the models on variants including Chess960 (Fischer Random Chess) - a variant of chess where starting positions of pieces are randomized. We found that while the model exhibits basic strategy adaptation, they are inferior to symbolic AI algorithms that perform explicit search, but gap is smaller when playing against users on Lichess. Moreover, the training dynamics revealed that the model initially learns to move only its own pieces, suggesting an emergent compositional understanding of the game.",
    "authors": [
      "Anna M\u00e9sz\u00e1ros",
      "Patrik Reizinger",
      "Ferenc Husz\u00e1r"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20783v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20783v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2510.20782v1",
    "title": "A Use-Case Specific Dataset for Measuring Dimensions of Responsible   Performance in LLM-generated Text",
    "summary": "Current methods for evaluating large language models (LLMs) typically focus on high-level tasks such as text generation, without targeting a particular AI application. This approach is not sufficient for evaluating LLMs for Responsible AI dimensions like fairness, since protected attributes that are highly relevant in one application may be less relevant in another. In this work, we construct a dataset that is driven by a real-world application (generate a plain-text product description, given a list of product features), parameterized by fairness attributes intersected with gendered adjectives and product categories, yielding a rich set of labeled prompts. We show how to use the data to identify quality, veracity, safety, and fairness gaps in LLMs, contributing a proposal for LLM evaluation paired with a concrete resource for the research community.",
    "authors": [
      "Alicia Sagae",
      "Chia-Jung Lee",
      "Sandeep Avula",
      "Brandon Dang",
      "Vanessa Murdock"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20782v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20782v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2510.20754v1",
    "title": "ACS-SegNet: An Attention-Based CNN-SegFormer Segmentation Network for   Tissue Segmentation in Histopathology",
    "summary": "Automated histopathological image analysis plays a vital role in computer-aided diagnosis of various diseases. Among developed algorithms, deep learning-based approaches have demonstrated excellent performance in multiple tasks, including semantic tissue segmentation in histological images. In this study, we propose a novel approach based on attention-driven feature fusion of convolutional neural networks (CNNs) and vision transformers (ViTs) within a unified dual-encoder model to improve semantic segmentation performance. Evaluation on two publicly available datasets showed that our model achieved {\\mu}IoU/{\\mu}Dice scores of 76.79%/86.87% on the GCPS dataset and 64.93%/76.60% on the PUMA dataset, outperforming state-of-the-art and baseline benchmarks. The implementation of our method is publicly available in a GitHub repository: https://github.com/NimaTorbati/ACS-SegNet",
    "authors": [
      "Nima Torbati",
      "Anastasia Meshcheryakova",
      "Ramona Woitek",
      "Diana Mechtcheriakova",
      "Amirreza Mahbod"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20754v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20754v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2510.20669v1",
    "title": "HybridSOMSpikeNet: A Deep Model with Differentiable Soft Self-Organizing   Maps and Spiking Dynamics for Waste Classification",
    "summary": "Accurate waste classification is vital for achieving sustainable waste management and reducing the environmental footprint of urbanization. Misclassification of recyclable materials contributes to landfill accumulation, inefficient recycling, and increased greenhouse gas emissions. To address these issues, this study introduces HybridSOMSpikeNet, a hybrid deep learning framework that integrates convolutional feature extraction, differentiable self-organization, and spiking-inspired temporal processing to enable intelligent and energy-efficient waste classification. The proposed model employs a pre-trained ResNet-152 backbone to extract deep spatial representations, followed by a Differentiable Soft Self-Organizing Map (Soft-SOM) that enhances topological clustering and interpretability. A spiking neural head accumulates temporal activations over discrete time steps, improving robustness and generalization. Trained on a ten-class waste dataset, HybridSOMSpikeNet achieved a test accuracy of 97.39%, outperforming several state-of-the-art architectures while maintaining a lightweight computational profile suitable for real-world deployment. Beyond its technical innovations, the framework provides tangible environmental benefits. By enabling precise and automated waste segregation, it supports higher recycling efficiency, reduces contamination in recyclable streams, and minimizes the ecological and operational costs of waste processing. The approach aligns with global sustainability priorities, particularly the United Nations Sustainable Development Goals (SDG 11 and SDG 12), by contributing to cleaner cities, circular economy initiatives, and intelligent environmental management systems.",
    "authors": [
      "Debojyoti Ghosh",
      "Adrijit Goswami"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20669v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20669v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2510.20647v1",
    "title": "The Reasoning Lingua Franca: A Double-Edged Sword for Multilingual AI",
    "summary": "Large Reasoning Models (LRMs) achieve strong performance on mathematical, scientific, and other question-answering tasks, but their multilingual reasoning abilities remain underexplored. When presented with non-English questions, LRMs often default to reasoning in English, raising concerns about interpretability and the handling of linguistic and cultural nuances. We systematically compare an LRM's reasoning in English versus the language of the question. Our evaluation spans two tasks: MGSM and GPQA Diamond. Beyond measuring answer accuracy, we also analyze cognitive attributes in the reasoning traces. We find that English reasoning traces exhibit a substantially higher presence of these cognitive behaviors, and that reasoning in English generally yields higher final-answer accuracy, with the performance gap increasing as tasks become more complex. However, this English-centric strategy is susceptible to a key failure mode - getting \"Lost in Translation,\" where translation steps lead to errors that would have been avoided by question's language reasoning.",
    "authors": [
      "Alan Saji",
      "Raj Dabre",
      "Anoop Kunchukuttan",
      "Ratish Puduppully"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20647v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20647v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2510.20641v1",
    "title": "Integrating Machine Learning into Belief-Desire-Intention Agents:   Current Advances and Open Challenges",
    "summary": "Thanks to the remarkable human-like capabilities of machine learning (ML) models in perceptual and cognitive tasks, frameworks integrating ML within rational agent architectures are gaining traction. Yet, the landscape remains fragmented and incoherent, often focusing on embedding ML into generic agent containers while overlooking the expressive power of rational architectures--such as Belief-Desire-Intention (BDI) agents. This paper presents a fine-grained systematisation of existing approaches, using the BDI paradigm as a reference. Our analysis illustrates the fast-evolving literature on rational agents enhanced by ML, and identifies key research opportunities and open challenges for designing effective rational ML agents.",
    "authors": [
      "Andrea Agiollo",
      "Andrea Omicini"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20641v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20641v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2510.20584v1",
    "title": "Can ChatGPT Code Communication Data Fairly?: Empirical Evidence from   Multiple Collaborative Tasks",
    "summary": "Assessing communication and collaboration at scale depends on a labor intensive task of coding communication data into categories according to different frameworks. Prior research has established that ChatGPT can be directly instructed with coding rubrics to code the communication data and achieves accuracy comparable to human raters. However, whether the coding from ChatGPT or similar AI technology exhibits bias against different demographic groups, such as gender and race, remains unclear. To fill this gap, this paper investigates ChatGPT-based automated coding of communication data using a typical coding framework for collaborative problem solving, examining differences across gender and racial groups. The analysis draws on data from three types of collaborative tasks: negotiation, problem solving, and decision making. Our results show that ChatGPT-based coding exhibits no significant bias across gender and racial groups, paving the road for its adoption in large-scale assessment of collaboration and communication.",
    "authors": [
      "Jiangang Hao",
      "Wenju Cui",
      "Patrick Kyllonen",
      "Emily Kerzabi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20584v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20584v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2510.20393v1",
    "title": "Mitigating Cross-modal Representation Bias for Multicultural   Image-to-Recipe Retrieval",
    "summary": "Existing approaches for image-to-recipe retrieval have the implicit assumption that a food image can fully capture the details textually documented in its recipe. However, a food image only reflects the visual outcome of a cooked dish and not the underlying cooking process. Consequently, learning cross-modal representations to bridge the modality gap between images and recipes tends to ignore subtle, recipe-specific details that are not visually apparent but are crucial for recipe retrieval. Specifically, the representations are biased to capture the dominant visual elements, resulting in difficulty in ranking similar recipes with subtle differences in use of ingredients and cooking methods. The bias in representation learning is expected to be more severe when the training data is mixed of images and recipes sourced from different cuisines. This paper proposes a novel causal approach that predicts the culinary elements potentially overlooked in images, while explicitly injecting these elements into cross-modal representation learning to mitigate biases. Experiments are conducted on the standard monolingual Recipe1M dataset and a newly curated multilingual multicultural cuisine dataset. The results indicate that the proposed causal representation learning is capable of uncovering subtle ingredients and cooking actions and achieves impressive retrieval performance on both monolingual and multilingual multicultural datasets.",
    "authors": [
      "Qing Wang",
      "Chong-Wah Ngo",
      "Yu Cao",
      "Ee-Peng Lim"
    ],
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20393v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20393v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2510.20372v1",
    "title": "Testing Most Influential Sets",
    "summary": "Small subsets of data with disproportionate influence on model outcomes can have dramatic impacts on conclusions, with a few data points sometimes overturning key findings. While recent work has developed methods to identify these \\emph{most influential sets}, no formal theory exists to determine when their influence reflects genuine problems rather than natural sampling variation. We address this gap by developing a principled framework for assessing the statistical significance of most influential sets. Our theoretical results characterize the extreme value distributions of maximal influence and enable rigorous hypothesis tests for excessive influence, replacing current ad-hoc sensitivity checks. We demonstrate the practical value of our approach through applications across economics, biology, and machine learning benchmarks.",
    "authors": [
      "Lucas Darius Konrad",
      "Nikolas Kuschnig"
    ],
    "categories": [
      "stat.ML",
      "cs.LG",
      "econ.EM",
      "math.ST",
      "stat.ME",
      "stat.TH"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20372v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20372v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2510.20810v1",
    "title": "On the Detectability of LLM-Generated Text: What Exactly Is   LLM-Generated Text?",
    "summary": "With the widespread use of large language models (LLMs), many researchers have turned their attention to detecting text generated by them. However, there is no consistent or precise definition of their target, namely \"LLM-generated text\". Differences in usage scenarios and the diversity of LLMs further increase the difficulty of detection. What is commonly regarded as the detecting target usually represents only a subset of the text that LLMs can potentially produce. Human edits to LLM outputs, together with the subtle influences that LLMs exert on their users, are blurring the line between LLM-generated and human-written text. Existing benchmarks and evaluation approaches do not adequately address the various conditions in real-world detector applications. Hence, the numerical results of detectors are often misunderstood, and their significance is diminishing. Therefore, detectors remain useful under specific conditions, but their results should be interpreted only as references rather than decisive indicators.",
    "authors": [
      "Mingmeng Geng",
      "Thierry Poibeau"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20810v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20810v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2510.20755v1",
    "title": "Incomplete U-Statistics of Equireplicate Designs: Berry-Esseen Bound and   Efficient Construction",
    "summary": "U-statistics are a fundamental class of estimators that generalize the sample mean and underpin much of nonparametric statistics. Although extensively studied in both statistics and probability, key challenges remain: their high computational cost - addressed partly through incomplete U-statistics - and their non-standard asymptotic behavior in the degenerate case, which typically requires resampling methods for hypothesis testing. This paper presents a novel perspective on U-statistics, grounded in hypergraph theory and combinatorial designs. Our approach bypasses the traditional Hoeffding decomposition, the main analytical tool in this literature but one highly sensitive to degeneracy. By characterizing the dependence structure of a U-statistic, we derive a Berry-Esseen bound that applies to all incomplete U-statistics of deterministic designs, yielding conditions under which Gaussian limiting distributions can be established even in the degenerate case and when the order diverges. We also introduce efficient algorithms to construct incomplete U-statistics of equireplicate designs, a subclass of deterministic designs that, in certain cases, achieve minimum variance. Finally, we apply our framework to kernel-based tests that use Maximum Mean Discrepancy (MMD) and Hilbert-Schmidt Independence Criterion. In a real data example with CIFAR-10, our permutation-free MMD test delivers substantial computational gains while retaining power and type I error control.",
    "authors": [
      "Cesare Miglioli",
      "Jordan Awan"
    ],
    "categories": [
      "math.ST",
      "math.CO",
      "stat.ME",
      "stat.ML",
      "stat.TH"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20755v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20755v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2510.20748v1",
    "title": "Reinforcement Learning and Consumption-Savings Behavior",
    "summary": "This paper demonstrates how reinforcement learning can explain two puzzling empirical patterns in household consumption behavior during economic downturns. I develop a model where agents use Q-learning with neural network approximation to make consumption-savings decisions under income uncertainty, departing from standard rational expectations assumptions. The model replicates two key findings from recent literature: (1) unemployed households with previously low liquid assets exhibit substantially higher marginal propensities to consume (MPCs) out of stimulus transfers compared to high-asset households (0.50 vs 0.34), even when neither group faces borrowing constraints, consistent with Ganong et al. (2024); and (2) households with more past unemployment experiences maintain persistently lower consumption levels after controlling for current economic conditions, a \"scarring\" effect documented by Malmendier and Shen (2024). Unlike existing explanations based on belief updating about income risk or ex-ante heterogeneity, the reinforcement learning mechanism generates both higher MPCs and lower consumption levels simultaneously through value function approximation errors that evolve with experience. Simulation results closely match the empirical estimates, suggesting that adaptive learning through reinforcement learning provides a unifying framework for understanding how past experiences shape current consumption behavior beyond what current economic conditions would predict.",
    "authors": [
      "Brandon Kaplowitz"
    ],
    "categories": [
      "econ.GN",
      "cs.AI",
      "cs.LG",
      "q-fin.EC"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20748v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20748v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2510.20727v1",
    "title": "Automated Extraction of Fluoropyrimidine Treatment and Treatment-Related   Toxicities from Clinical Notes Using Natural Language Processing",
    "summary": "Objective: Fluoropyrimidines are widely prescribed for colorectal and breast cancers, but are associated with toxicities such as hand-foot syndrome and cardiotoxicity. Since toxicity documentation is often embedded in clinical notes, we aimed to develop and evaluate natural language processing (NLP) methods to extract treatment and toxicity information.   Materials and Methods: We constructed a gold-standard dataset of 236 clinical notes from 204,165 adult oncology patients. Domain experts annotated categories related to treatment regimens and toxicities. We developed rule-based, machine learning-based (Random Forest, Support Vector Machine [SVM], Logistic Regression [LR]), deep learning-based (BERT, ClinicalBERT), and large language models (LLM)-based NLP approaches (zero-shot and error-analysis prompting). Models used an 80:20 train-test split.   Results: Sufficient data existed to train and evaluate 5 annotated categories. Error-analysis prompting achieved optimal precision, recall, and F1 scores (F1=1.000) for treatment and toxicities extraction, whereas zero-shot prompting reached F1=1.000 for treatment and F1=0.876 for toxicities extraction.LR and SVM ranked second for toxicities (F1=0.937). Deep learning underperformed, with BERT (F1=0.873 treatment; F1= 0.839 toxicities) and ClinicalBERT (F1=0.873 treatment; F1 = 0.886 toxicities). Rule-based methods served as our baseline with F1 scores of 0.857 in treatment and 0.858 in toxicities.   Discussion: LMM-based approaches outperformed all others, followed by machine learning methods. Machine and deep learning approaches were limited by small training data and showed limited generalizability, particularly for rare categories.   Conclusion: LLM-based NLP most effectively extracted fluoropyrimidine treatment and toxicity information from clinical notes, and has strong potential to support oncology research and pharmacovigilance.",
    "authors": [
      "Xizhi Wu",
      "Madeline S. Kreider",
      "Philip E. Empey",
      "Chenyu Li",
      "Yanshan Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20727v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20727v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2510.20713v1",
    "title": "Experimental differentiation and extremization with analog quantum   circuits",
    "summary": "Solving and optimizing differential equations (DEs) is ubiquitous in both engineering and fundamental science. The promise of quantum architectures to accelerate scientific computing thus naturally involved interest towards how efficiently quantum algorithms can solve DEs. Differentiable quantum circuits (DQC) offer a viable route to compute DE solutions using a variational approach amenable to existing quantum computers, by producing a machine-learnable surrogate of the solution. Quantum extremal learning (QEL) complements such approach by finding extreme points in the output of learnable models of unknown (implicit) functions, offering a powerful tool to bypass a full DE solution, in cases where the crux consists in retrieving solution extrema. In this work, we provide the results from the first experimental demonstration of both DQC and QEL, displaying their performance on a synthetic usecase. Whilst both DQC and QEL are expected to require digital quantum hardware, we successfully challenge this assumption by running a closed-loop instance on a commercial analog quantum computer, based upon neutral atom technology.",
    "authors": [
      "Evan Philip",
      "Julius de Hond",
      "Vytautas Abramavicius",
      "Kaonan Micadei",
      "Mario Dagrada",
      "Panagiotis Barkoutsos",
      "Mourad Beji",
      "Louis-Paul Henry",
      "Vincent E. Elfving",
      "Antonio A. Gentile",
      "Savvas Varsamopoulos"
    ],
    "categories": [
      "quant-ph",
      "cs.NE"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20713v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20713v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2510.20610v1",
    "title": "BUSTED at AraGenEval Shared Task: A Comparative Study of   Transformer-Based Models for Arabic AI-Generated Text Detection",
    "summary": "This paper details our submission to the Ara- GenEval Shared Task on Arabic AI-generated text detection, where our team, BUSTED, se- cured 5th place. We investigated the effec- tiveness of three pre-trained transformer mod- els: AraELECTRA, CAMeLBERT, and XLM- RoBERTa. Our approach involved fine-tuning each model on the provided dataset for a binary classification task. Our findings revealed a sur- prising result: the multilingual XLM-RoBERTa model achieved the highest performance with an F1 score of 0.7701, outperforming the spe- cialized Arabic models. This work underscores the complexities of AI-generated text detection and highlights the strong generalization capa- bilities of multilingual models.",
    "authors": [
      "Ali Zain",
      "Sareem Farooqui",
      "Muhammad Rafi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20610v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20610v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2510.20602v1",
    "title": "Resounding Acoustic Fields with Reciprocity",
    "summary": "Achieving immersive auditory experiences in virtual environments requires flexible sound modeling that supports dynamic source positions. In this paper, we introduce a task called resounding, which aims to estimate room impulse responses at arbitrary emitter location from a sparse set of measured emitter positions, analogous to the relighting problem in vision. We leverage the reciprocity property and introduce Versa, a physics-inspired approach to facilitating acoustic field learning. Our method creates physically valid samples with dense virtual emitter positions by exchanging emitter and listener poses. We also identify challenges in deploying reciprocity due to emitter/listener gain patterns and propose a self-supervised learning approach to address them. Results show that Versa substantially improve the performance of acoustic field learning on both simulated and real-world datasets across different metrics. Perceptual user studies show that Versa can greatly improve the immersive spatial sound experience. Code, dataset and demo videos are available on the project website: https://waves.seas.upenn.edu/projects/versa.",
    "authors": [
      "Zitong Lan",
      "Yiduo Hao",
      "Mingmin Zhao"
    ],
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS",
      "eess.SP"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20602v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20602v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2510.20441v1",
    "title": "UniSE: A Unified Framework for Decoder-only Autoregressive LM-based   Speech Enhancement",
    "summary": "The development of neural audio codecs (NACs) has largely promoted applications of language models (LMs) to speech processing and understanding. However, there lacks the verification on the effectiveness of autoregressive (AR) LMbased models in unifying different sub-tasks of speech enhancement (SE). In this work, we propose UniSE, a unified decoder-only LM-based framework to handle different SE tasks including speech restoration, target speaker extraction and speech separation. It takes input speech features as conditions and generates discrete tokens of the target speech using AR modeling, which facilitates a compatibility between distinct learning patterns of multiple tasks. Experiments on several benchmarks indicate the proposed UniSE can achieve competitive performance compared to discriminative and generative baselines, showing the capacity of LMs in unifying SE tasks. The demo page is available here: https://github.com/hyyan2k/UniSE.",
    "authors": [
      "Haoyin Yan",
      "Chengwei Liu",
      "Shaofei Xue",
      "Xiaotao Liang",
      "Zheng Xue"
    ],
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20441v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20441v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2510.20348v1",
    "title": "AccuQuant: Simulating Multiple Denoising Steps for Quantizing Diffusion   Models",
    "summary": "We present in this paper a novel post-training quantization (PTQ) method, dubbed AccuQuant, for diffusion models. We show analytically and empirically that quantization errors for diffusion models are accumulated over denoising steps in a sampling process. To alleviate the error accumulation problem, AccuQuant minimizes the discrepancies between outputs of a full-precision diffusion model and its quantized version within a couple of denoising steps. That is, it simulates multiple denoising steps of a diffusion sampling process explicitly for quantization, accounting the accumulated errors over multiple denoising steps, which is in contrast to previous approaches to imitating a training process of diffusion models, namely, minimizing the discrepancies independently for each step. We also present an efficient implementation technique for AccuQuant, together with a novel objective, which reduces a memory complexity significantly from $\\mathcal{O}(n)$ to $\\mathcal{O}(1)$, where $n$ is the number of denoising steps. We demonstrate the efficacy and efficiency of AccuQuant across various tasks and diffusion models on standard benchmarks.",
    "authors": [
      "Seunghoon Lee",
      "Jeongwoo Choi",
      "Byunggwan Son",
      "Jaehyeon Moon",
      "Jeimin Jeon",
      "Bumsub Ham"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20348v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20348v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2510.20334v1",
    "title": "Capability of using the normalizing flows for extraction rare gamma   events in the TAIGA experiment",
    "summary": "The objective of this work is to develop a method for detecting rare gamma quanta against the background of charged particles in the fluxes from sources in the Universe with the help of the deep learning and normalizing flows based method designed for anomaly detection. It is shown that the suggested method has a potential for the gamma detection. The method was tested on model data from the TAIGA-IACT experiment. The obtained quantitative performance indicators are still inferior to other approaches, and therefore possible ways to improve the implementation of the method are proposed.",
    "authors": [
      "A. P. Kryukov",
      "A. Yu. Razumov",
      "A. P. Demichev",
      "J. J. Dubenskaya",
      "E. O. Gres",
      "S. P. Polyakov",
      "E. B. Postnikov",
      "P. A. Volchugov",
      "D. P. Zhurov"
    ],
    "categories": [
      "astro-ph.IM",
      "astro-ph.HE",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20334v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20334v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2510.20296v1",
    "title": "RAG-Stack: Co-Optimizing RAG Quality and Performance From the Vector   Database Perspective",
    "summary": "Retrieval-augmented generation (RAG) has emerged as one of the most prominent applications of vector databases. By integrating documents retrieved from a database into the prompt of a large language model (LLM), RAG enables more reliable and informative content generation. While there has been extensive research on vector databases, many open research problems remain once they are considered in the wider context of end-to-end RAG pipelines. One practical yet challenging problem is how to jointly optimize both system performance and generation quality in RAG, which is significantly more complex than it appears due to the numerous knobs on both the algorithmic side (spanning models and databases) and the systems side (from software to hardware). In this paper, we present RAG-Stack, a three-pillar blueprint for quality-performance co-optimization in RAG systems. RAG-Stack comprises: (1) RAG-IR, an intermediate representation that serves as an abstraction layer to decouple quality and performance aspects; (2) RAG-CM, a cost model for estimating system performance given an RAG-IR; and (3) RAG-PE, a plan exploration algorithm that searches for high-quality, high-performance RAG configurations. We believe this three-pillar blueprint will become the de facto paradigm for RAG quality-performance co-optimization in the years to come.",
    "authors": [
      "Wenqi Jiang"
    ],
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20296v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20296v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2510.20776v1",
    "title": "CUPID: Pose-Grounded Generative 3D Reconstruction from a Single Image",
    "summary": "This work proposes a new generation-based 3D reconstruction method, named Cupid, that accurately infers the camera pose, 3D shape, and texture of an object from a single 2D image. Cupid casts 3D reconstruction as a conditional sampling process from a learned distribution of 3D objects, and it jointly generates voxels and pixel-voxel correspondences, enabling robust pose and shape estimation under a unified generative framework. By representing both input camera poses and 3D shape as a distribution in a shared 3D latent space, Cupid adopts a two-stage flow matching pipeline: (1) a coarse stage that produces initial 3D geometry with associated 2D projections for pose recovery; and (2) a refinement stage that integrates pose-aligned image features to enhance structural fidelity and appearance details. Extensive experiments demonstrate Cupid outperforms leading 3D reconstruction methods with an over 3 dB PSNR gain and an over 10% Chamfer Distance reduction, while matching monocular estimators on pose accuracy and delivering superior visual fidelity over baseline 3D generative models. For an immersive view of the 3D results generated by Cupid, please visit cupid3d.github.io.",
    "authors": [
      "Binbin Huang",
      "Haobin Duan",
      "Yiqun Zhao",
      "Zibo Zhao",
      "Yi Ma",
      "Shenghua Gao"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20776v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20776v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2510.20769v1",
    "title": "CSU-PCAST: A Dual-Branch Transformer Framework for medium-range ensemble   Precipitation Forecasting",
    "summary": "Accurate medium-range precipitation forecasting is crucial for hydrometeorological risk management and disaster mitigation, yet remains challenging for current numerical weather prediction (NWP) systems. Traditional ensemble systems such as the Global Ensemble Forecast System (GEFS) struggle to maintain high skill, especially for moderate and heavy rainfall at extended lead times. This study develops a deep learning-based ensemble framework for multi-step precipitation prediction through joint modeling of a comprehensive set of atmospheric variables. The model is trained on ERA5 reanalysis data at 0.25$^{\\circ}$ spatial resolution, with precipitation labels from NASA's Integrated Multi-satellite Retrievals for Global Precipitation Measurement (GPM) constellation (IMERG), incorporating 57 input variables, including upper-air and surface predictors. The architecture employs a patch-based Swin Transformer backbone with periodic convolutions to handle longitudinal continuity and integrates time and noise embeddings through conditional layer normalization. A dual-branch decoder predicts total precipitation and other variables, with targeted freezing of encoder-decoder pathways for specialized training. Training minimizes a hybrid loss combining the Continuous Ranked Probability Score (CRPS) and weighted log1p mean squared error (log1pMSE), balancing probabilistic accuracy and magnitude fidelity. During inference, the model ingests real-time Global Forecast System (GFS) initial conditions to generate 15-day forecasts autoregressively. Evaluation against GEFS using IMERG data demonstrates higher Critical Success Index (CSI) scores at precipitation thresholds of 0.1 mm, 1 mm, 10 mm, and 20 mm, highlighting improved performance for moderate to heavy rainfall.",
    "authors": [
      "Tianyi Xiong",
      "Haonan Chen"
    ],
    "categories": [
      "physics.ao-ph",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20769v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20769v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2510.20725v1",
    "title": "No-Regret Thompson Sampling for Finite-Horizon Markov Decision Processes   with Gaussian Processes",
    "summary": "Thompson sampling (TS) is a powerful and widely used strategy for sequential decision-making, with applications ranging from Bayesian optimization to reinforcement learning (RL). Despite its success, the theoretical foundations of TS remain limited, particularly in settings with complex temporal structure such as RL. We address this gap by establishing no-regret guarantees for TS using models with Gaussian marginal distributions. Specifically, we consider TS in episodic RL with joint Gaussian process (GP) priors over rewards and transitions. We prove a regret bound of $\\mathcal{\\tilde{O}}(\\sqrt{KH\\Gamma(KH)})$ over $K$ episodes of horizon $H$, where $\\Gamma(\\cdot)$ captures the complexity of the GP model. Our analysis addresses several challenges, including the non-Gaussian nature of value functions and the recursive structure of Bellman updates, and extends classical tools such as the elliptical potential lemma to multi-output settings. This work advances the understanding of TS in RL and highlights how structural assumptions and model uncertainty shape its performance in finite-horizon Markov Decision Processes.",
    "authors": [
      "Jasmine Bayrooti",
      "Sattar Vakili",
      "Amanda Prorok",
      "Carl Henrik Ek"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20725v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20725v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2510.20706v1",
    "title": "Real-Time Gait Adaptation for Quadrupeds using Model Predictive Control   and Reinforcement Learning",
    "summary": "Model-free reinforcement learning (RL) has enabled adaptable and agile quadruped locomotion; however, policies often converge to a single gait, leading to suboptimal performance. Traditionally, Model Predictive Control (MPC) has been extensively used to obtain task-specific optimal policies but lacks the ability to adapt to varying environments. To address these limitations, we propose an optimization framework for real-time gait adaptation in a continuous gait space, combining the Model Predictive Path Integral (MPPI) algorithm with a Dreamer module to produce adaptive and optimal policies for quadruped locomotion. At each time step, MPPI jointly optimizes the actions and gait variables using a learned Dreamer reward that promotes velocity tracking, energy efficiency, stability, and smooth transitions, while penalizing abrupt gait changes. A learned value function is incorporated as terminal reward, extending the formulation to an infinite-horizon planner. We evaluate our framework in simulation on the Unitree Go1, demonstrating an average reduction of up to 36.48\\% in energy consumption across varying target speeds, while maintaining accurate tracking and adaptive, task-appropriate gaits.",
    "authors": [
      "Ganga Nair B",
      "Prakrut Kotecha",
      "Shishir Kolathaya"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20706v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20706v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2510.20690v1",
    "title": "Neural Diversity Regularizes Hallucinations in Small Models",
    "summary": "Language models continue to hallucinate despite increases in parameters, compute, and data. We propose neural diversity -- decorrelated parallel representations -- as a principled mechanism that reduces hallucination rates at fixed parameter and data budgets. Inspired by portfolio theory, where uncorrelated assets reduce risk by $\\sqrt{P}$, we prove hallucination probability is bounded by representational correlation: $P(H) \\leq f(\\sigma^2((1-\\rho(P))/P + \\rho(P)), \\mu^2)$, which predicts that language models need an optimal amount of neurodiversity. To validate this, we introduce ND-LoRA (Neural Diversity Low-Rank Adaptation), combining parallel LoRA adapters with Barlow Twins regularization, and demonstrate that ND-LoRA reduces hallucinations by up to 25.6% (and 14.6% on average) without degrading general accuracy. Ablations show LoRA adapters and regularization act synergistically, causal interventions prove neurodiversity as the mediating factor and correlational analyses indicate scale: a 0.1% neural correlation increase is associated with a 3.8% hallucination increase. Finally, task-dependent optimality emerges: different tasks require different amounts of optimal neurodiversity. Together, our results highlight neural diversity as a third axis of scaling -- orthogonal to parameters and data -- to improve the reliability of language models at fixed budgets.",
    "authors": [
      "Kushal Chakrabarti",
      "Nirmal Balachundhar"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20690v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20690v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2510.20674v1",
    "title": "Analyticup E-commerce Product Search Competition Technical Report from   Team Tredence_AICOE",
    "summary": "This study presents the multilingual e-commerce search system developed by the Tredence_AICOE team. The competition features two multilingual relevance tasks: Query-Category (QC) Relevance, which evaluates how well a user's search query aligns with a product category, and Query-Item (QI) Relevance, which measures the match between a multilingual search query and an individual product listing. To ensure full language coverage, we performed data augmentation by translating existing datasets into languages missing from the development set, enabling training across all target languages. We fine-tuned Gemma-3 12B and Qwen-2.5 14B model for both tasks using multiple strategies. The Gemma-3 12B (4-bit) model achieved the best QC performance using original and translated data, and the best QI performance using original, translated, and minority class data creation. These approaches secured 4th place on the final leaderboard, with an average F1-score of 0.8857 on the private test set.",
    "authors": [
      "Rakshith R",
      "Shubham Sharma",
      "Mohammed Sameer Khan",
      "Ankush Chopra"
    ],
    "categories": [
      "cs.IR",
      "cs.CL"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20674v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20674v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2510.20661v1",
    "title": "UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale   High-Quality Dataset",
    "summary": "Ultra-high-resolution (UHR) text-to-image (T2I) generation has seen notable progress. However, two key challenges remain : 1) the absence of a large-scale high-quality UHR T2I dataset, and (2) the neglect of tailored training strategies for fine-grained detail synthesis in UHR scenarios. To tackle the first challenge, we introduce \\textbf{UltraHR-100K}, a high-quality dataset of 100K UHR images with rich captions, offering diverse content and strong visual fidelity. Each image exceeds 3K resolution and is rigorously curated based on detail richness, content complexity, and aesthetic quality. To tackle the second challenge, we propose a frequency-aware post-training method that enhances fine-detail generation in T2I diffusion models. Specifically, we design (i) \\textit{Detail-Oriented Timestep Sampling (DOTS)} to focus learning on detail-critical denoising steps, and (ii) \\textit{Soft-Weighting Frequency Regularization (SWFR)}, which leverages Discrete Fourier Transform (DFT) to softly constrain frequency components, encouraging high-frequency detail preservation. Extensive experiments on our proposed UltraHR-eval4K benchmarks demonstrate that our approach significantly improves the fine-grained detail quality and overall fidelity of UHR image generation. The code is available at \\href{https://github.com/NJU-PCALab/UltraHR-100k}{here}.",
    "authors": [
      "Chen Zhao",
      "En Ci",
      "Yunzhe Xu",
      "Tiehan Fan",
      "Shanyan Guan",
      "Yanhao Ge",
      "Jian Yang",
      "Ying Tai"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20661v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20661v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2510.20636v1",
    "title": "Fluidity Index: Next-Generation Super-intelligence Benchmarks",
    "summary": "This paper introduces the Fluidity Index (FI) to quantify model adaptability in dynamic, scaling environments. The benchmark evaluates response accuracy based on deviations in initial, current, and future environment states, assessing context switching and continuity. We distinguish between closed-ended and open-ended benchmarks, prioritizing closed-loop open-ended real-world benchmarks to test adaptability. The approach measures a model's ability to understand, predict, and adjust to state changes in scaling environments. A truly super-intelligent model should exhibit at least second-order adaptability, enabling self-sustained computation through digital replenishment for optimal fluidity.",
    "authors": [
      "Eric Ngoiya",
      "Tianshu Bao"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20636v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20636v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2510.20609v1",
    "title": "Practical Code RAG at Scale: Task-Aware Retrieval Design Choices under   Compute Budgets",
    "summary": "We study retrieval design for code-focused generation tasks under realistic compute budgets. Using two complementary tasks from Long Code Arena -- code completion and bug localization -- we systematically compare retrieval configurations across various context window sizes along three axes: (i) chunking strategy, (ii) similarity scoring, and (iii) splitting granularity. (1) For PL-PL, sparse BM25 with word-level splitting is the most effective and practical, significantly outperforming dense alternatives while being an order of magnitude faster. (2) For NL-PL, proprietary dense encoders (Voyager-3 family) consistently beat sparse retrievers, however requiring 100x larger latency. (3) Optimal chunk size scales with available context: 32-64 line chunks work best at small budgets, and whole-file retrieval becomes competitive at 16000 tokens. (4) Simple line-based chunking matches syntax-aware splitting across budgets. (5) Retrieval latency varies by up to 200x across configurations; BPE-based splitting is needlessly slow, and BM25 + word splitting offers the best quality-latency trade-off. Thus, we provide evidence-based recommendations for implementing effective code-oriented RAG systems based on task requirements, model constraints, and computational efficiency.",
    "authors": [
      "Timur Galimzyanov",
      "Olga Kolomyttseva",
      "Egor Bogomolov"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR",
      "cs.LG, cs.IR, cs.SE, cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20609v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20609v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2510.20568v1",
    "title": "Lost in Translation: Policymakers are not really listening to Citizen   Concerns about AI",
    "summary": "The worlds people have strong opinions about artificial intelligence (AI), and they want policymakers to listen. Governments are inviting public comment on AI, but as they translate input into policy, much of what citizens say is lost. Policymakers are missing a critical opportunity to build trust in AI and its governance. This paper compares three countries, Australia, Colombia, and the United States, that invited citizens to comment on AI risks and policies. Using a landscape analysis, the authors examined how each government solicited feedback and whether that input shaped governance. Yet in none of the three cases did citizens and policymakers establish a meaningful dialogue. Governments did little to attract diverse voices or publicize calls for comment, leaving most citizens unaware or unprepared to respond. In each nation, fewer than one percent of the population participated. Moreover, officials showed limited responsiveness to the feedback they received, failing to create an effective feedback loop. The study finds a persistent gap between the promise and practice of participatory AI governance. The authors conclude that current approaches are unlikely to build trust or legitimacy in AI because policymakers are not adequately listening or responding to public concerns. They offer eight recommendations: promote AI literacy; monitor public feedback; broaden outreach; hold regular online forums; use innovative engagement methods; include underrepresented groups; respond publicly to input; and make participation easier.",
    "authors": [
      "Susan Ariel Aaronson",
      "Michael Moreno"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20568v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20568v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2510.20482v1",
    "title": "Reliable and Reproducible Demographic Inference for Fairness in Face   Analysis",
    "summary": "Fairness evaluation in face analysis systems (FAS) typically depends on automatic demographic attribute inference (DAI), which itself relies on predefined demographic segmentation. However, the validity of fairness auditing hinges on the reliability of the DAI process. We begin by providing a theoretical motivation for this dependency, showing that improved DAI reliability leads to less biased and lower-variance estimates of FAS fairness. To address this, we propose a fully reproducible DAI pipeline that replaces conventional end-to-end training with a modular transfer learning approach. Our design integrates pretrained face recognition encoders with non-linear classification heads. We audit this pipeline across three dimensions: accuracy, fairness, and a newly introduced notion of robustness, defined via intra-identity consistency. The proposed robustness metric is applicable to any demographic segmentation scheme. We benchmark the pipeline on gender and ethnicity inference across multiple datasets and training setups. Our results show that the proposed method outperforms strong baselines, particularly on ethnicity, which is the more challenging attribute. To promote transparency and reproducibility, we will publicly release the training dataset metadata, full codebase, pretrained models, and evaluation toolkit. This work contributes a reliable foundation for demographic inference in fairness auditing.",
    "authors": [
      "Alexandre Fournier-Montgieux",
      "Herv\u00e9 Le Borgne",
      "Adrian Popescu",
      "Bertrand Luvison"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20482v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20482v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2510.20475v1",
    "title": "Mask and You Shall Receive: Optimizing Masked Language Modeling For   Pretraining BabyLMs",
    "summary": "We describe our strategy for the 2025 edition of the BabyLM Challenge. Our main contribution is that of an improved form of Masked Language Modeling (MLM), which adapts the probabilities of the tokens masked according to the model's ability to predict them. The results show a substantial increase in performance on (Super)GLUE tasks over the standard MLM. We also incorporate sub-token embeddings, finding that this increases the model's morphological generalization capabilities. Our submission beats the baseline in the strict-small track.",
    "authors": [
      "Lukas Edman",
      "Alexander Fraser"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20475v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20475v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2510.20454v1",
    "title": "Intransitive Player Dominance and Market Inefficiency in Tennis   Forecasting: A Graph Neural Network Approach",
    "summary": "Intransitive player dominance, where player A beats B, B beats C, but C beats A, is common in competitive tennis. Yet, there are few known attempts to incorporate it within forecasting methods. We address this problem with a graph neural network approach that explicitly models these intransitive relationships through temporal directed graphs, with players as nodes and their historical match outcomes as directed edges. We find the bookmaker Pinnacle Sports poorly handles matches with high intransitive complexity and posit that our graph-based approach is uniquely positioned to capture relational dynamics in these scenarios. When selectively betting on higher intransitivity matchups with our model (65.7% accuracy, 0.215 Brier Score), we achieve significant positive returns of 3.26% ROI with Kelly staking over 1903 bets, suggesting a market inefficiency in handling intransitive matchups that our approach successfully exploits.",
    "authors": [
      "Lawrence Clegg",
      "John Cartlidge"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20454v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20454v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2510.20350v1",
    "title": "What do AI-Generated Images Want?",
    "summary": "W.J.T. Mitchell's influential essay 'What do pictures want?' shifts the theoretical focus away from the interpretative act of understanding pictures and from the motivations of the humans who create them to the possibility that the picture itself is an entity with agency and wants. In this article, I reframe Mitchell's question in light of contemporary AI image generation tools to ask: what do AI-generated images want? Drawing from art historical discourse on the nature of abstraction, I argue that AI-generated images want specificity and concreteness because they are fundamentally abstract. Multimodal text-to-image models, which are the primary subject of this article, are based on the premise that text and image are interchangeable or exchangeable tokens and that there is a commensurability between them, at least as represented mathematically in data. The user pipeline that sees textual input become visual output, however, obscures this representational regress and makes it seem like one form transforms into the other -- as if by magic.",
    "authors": [
      "Amanda Wasielewski"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20350v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20350v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2510.20339v1",
    "title": "Multi-Task Deep Learning for Surface Metrology",
    "summary": "A reproducible deep learning framework is presented for surface metrology to predict surface texture parameters together with their reported standard uncertainties. Using a multi-instrument dataset spanning tactile and optical systems, measurement system type classification is addressed alongside coordinated regression of Ra, Rz, RONt and their uncertainty targets (Ra_uncert, Rz_uncert, RONt_uncert). Uncertainty is modelled via quantile and heteroscedastic heads with post-hoc conformal calibration to yield calibrated intervals. On a held-out set, high fidelity was achieved by single-target regressors (R2: Ra 0.9824, Rz 0.9847, RONt 0.9918), with two uncertainty targets also well modelled (Ra_uncert 0.9899, Rz_uncert 0.9955); RONt_uncert remained difficult (R2 0.4934). The classifier reached 92.85% accuracy and probability calibration was essentially unchanged after temperature scaling (ECE 0.00504 -> 0.00503 on the test split). Negative transfer was observed for naive multi-output trunks, with single-target models performing better. These results provide calibrated predictions suitable to inform instrument selection and acceptance decisions in metrological workflows.",
    "authors": [
      "D. Kucharski",
      "A. Gaska",
      "T. Kowaluk",
      "K. Stepien",
      "M. Repalska",
      "B. Gapinski",
      "M. Wieczorowski",
      "M. Nawotka",
      "P. Sobecki",
      "P. Sosinowski",
      "J. Tomasik",
      "A. Wojtowicz"
    ],
    "categories": [
      "physics.app-ph",
      "cs.AI",
      "cs.LG",
      "stat.AP",
      "stat.ML"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20339v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20339v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2510.20726v1",
    "title": "AutoScape: Geometry-Consistent Long-Horizon Scene Generation",
    "summary": "This paper proposes AutoScape, a long-horizon driving scene generation framework. At its core is a novel RGB-D diffusion model that iteratively generates sparse, geometrically consistent keyframes, serving as reliable anchors for the scene's appearance and geometry. To maintain long-range geometric consistency, the model 1) jointly handles image and depth in a shared latent space, 2) explicitly conditions on the existing scene geometry (i.e., rendered point clouds) from previously generated keyframes, and 3) steers the sampling process with a warp-consistent guidance. Given high-quality RGB-D keyframes, a video diffusion model then interpolates between them to produce dense and coherent video frames. AutoScape generates realistic and geometrically consistent driving videos of over 20 seconds, improving the long-horizon FID and FVD scores over the prior state-of-the-art by 48.6\\% and 43.0\\%, respectively.",
    "authors": [
      "Jiacheng Chen",
      "Ziyu Jiang",
      "Mingfu Liang",
      "Bingbing Zhuang",
      "Jong-Chyi Su",
      "Sparsh Garg",
      "Ying Wu",
      "Manmohan Chandraker"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20726v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20726v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2510.20668v1",
    "title": "From Masks to Worlds: A Hitchhiker's Guide to World Models",
    "summary": "This is not a typical survey of world models; it is a guide for those who want to build worlds. We do not aim to catalog every paper that has ever mentioned a ``world model\". Instead, we follow one clear road: from early masked models that unified representation learning across modalities, to unified architectures that share a single paradigm, then to interactive generative models that close the action-perception loop, and finally to memory-augmented systems that sustain consistent worlds over time. We bypass loosely related branches to focus on the core: the generative heart, the interactive loop, and the memory system. We show that this is the most promising path towards true world models.",
    "authors": [
      "Jinbin Bai",
      "Yu Lei",
      "Hecong Wu",
      "Yuchen Zhu",
      "Shufan Li",
      "Yi Xin",
      "Xiangtai Li",
      "Molei Tao",
      "Aditya Grover",
      "Ming-Hsuan Yang"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20668v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20668v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2510.20632v1",
    "title": "Towards Reliable Evaluation of Large Language Models for Multilingual   and Multimodal E-Commerce Applications",
    "summary": "Large Language Models (LLMs) excel on general-purpose NLP benchmarks, yet their capabilities in specialized domains remain underexplored. In e-commerce, existing evaluations-such as EcomInstruct, ChineseEcomQA, eCeLLM, and Shopping MMLU-suffer from limited task diversity (e.g., lacking product guidance and after-sales issues), limited task modalities (e.g., absence of multimodal data), synthetic or curated data, and a narrow focus on English and Chinese, leaving practitioners without reliable tools to assess models on complex, real-world shopping scenarios. We introduce EcomEval, a comprehensive multilingual and multimodal benchmark for evaluating LLMs in e-commerce. EcomEval covers six categories and 37 tasks (including 8 multimodal tasks), sourced primarily from authentic customer queries and transaction logs, reflecting the noisy and heterogeneous nature of real business interactions. To ensure both quality and scalability of reference answers, we adopt a semi-automatic pipeline in which large models draft candidate responses subsequently reviewed and modified by over 50 expert annotators with strong e-commerce and multilingual expertise. We define difficulty levels for each question and task category by averaging evaluation scores across models with different sizes and capabilities, enabling challenge-oriented and fine-grained assessment. EcomEval also spans seven languages-including five low-resource Southeast Asian languages-offering a multilingual perspective absent from prior work.",
    "authors": [
      "Shuyi Xie",
      "Ziqin Liew",
      "Hailing Zhang",
      "Haibo Zhang",
      "Ling Hu",
      "Zhiqiang Zhou",
      "Shuman Liu",
      "Anxiang Zeng"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20632v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20632v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2510.20590v1",
    "title": "Embedding the MLOps Lifecycle into OT Reference Models",
    "summary": "Machine Learning Operations (MLOps) practices are increas- ingly adopted in industrial settings, yet their integration with Opera- tional Technology (OT) systems presents significant challenges. This pa- per analyzes the fundamental obstacles in combining MLOps with OT en- vironments and proposes a systematic approach to embed MLOps prac- tices into established OT reference models. We evaluate the suitability of the Reference Architectural Model for Industry 4.0 (RAMI 4.0) and the International Society of Automation Standard 95 (ISA-95) for MLOps integration and present a detailed mapping of MLOps lifecycle compo- nents to RAMI 4.0 exemplified by a real-world use case. Our findings demonstrate that while standard MLOps practices cannot be directly transplanted to OT environments, structured adaptation using existing reference models can provide a pathway for successful integration.",
    "authors": [
      "Simon Schindler",
      "Christoph Binder",
      "Lukas L\u00fcrzer",
      "Stefan Huber"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20590v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20590v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2510.20453v1",
    "title": "Symbolic Regression and Differentiable Fits in Beyond the Standard Model   Physics",
    "summary": "We demonstrate the efficacy of symbolic regression (SR) to probe models of particle physics Beyond the Standard Model (BSM), by considering the so-called Constrained Minimal Supersymmetric Standard Model (CMSSM). Like many incarnations of BSM physics this model has a number (four) of arbitrary parameters, which determine the experimental signals, and cosmological observables such as the dark matter relic density. We show that analysis of the phenomenology can be greatly accelerated by using symbolic expressions derived for the observables in terms of the input parameters. Here we focus on the Higgs mass, the cold dark matter relic density, and the contribution to the anomalous magnetic moment of the muon. We find that SR can produce remarkably accurate expressions. Using them we make global fits to derive the posterior probability densities of the CMSSM input parameters which are in good agreement with those performed using conventional methods. Moreover, we demonstrate a major advantage of SR which is the ability to make fits using differentiable methods rather than sampling methods. We also compare the method with neural network (NN) regression. SR produces more globally robust results, while NNs require data that is focussed on the promising regions in order to be equally performant.",
    "authors": [
      "Shehu AbdusSalam",
      "Steven Abel",
      "Deaglan Bartlett",
      "Miguel Crispim Rom\u00e3o"
    ],
    "categories": [
      "hep-ph",
      "astro-ph.CO",
      "cs.AI",
      "cs.LG",
      "physics.comp-ph"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20453v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20453v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2510.20413v1",
    "title": "Why DPO is a Misspecified Estimator and How to Fix It",
    "summary": "Direct alignment algorithms such as Direct Preference Optimization (DPO) fine-tune models based on preference data, using only supervised learning instead of two-stage reinforcement learning with human feedback (RLHF). We show that DPO encodes a statistical estimation problem over reward functions induced by a parametric policy class. When the true reward function that generates preferences cannot be realized via the policy class, DPO becomes misspecified, resulting in failure modes such as preference order reversal, worsening of policy reward, and high sensitivity to the input preference data distribution. On the other hand, we study the local behavior of two-stage RLHF for a parametric class and relate it to a natural gradient step in policy space. Our fine-grained geometric characterization allows us to propose AuxDPO, which introduces additional auxiliary variables in the DPO loss function to help move towards the RLHF solution in a principled manner and mitigate the misspecification in DPO. We empirically demonstrate the superior performance of AuxDPO on didactic bandit settings as well as LLM alignment tasks.",
    "authors": [
      "Aditya Gopalan",
      "Sayak Ray Chowdhury",
      "Debangshu Banerjee"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20413v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20413v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2510.20408v1",
    "title": "Balancing Specialization and Centralization: A Multi-Agent Reinforcement   Learning Benchmark for Sequential Industrial Control",
    "summary": "Autonomous control of multi-stage industrial processes requires both local specialization and global coordination. Reinforcement learning (RL) offers a promising approach, but its industrial adoption remains limited due to challenges such as reward design, modularity, and action space management. Many academic benchmarks differ markedly from industrial control problems, limiting their transferability to real-world applications. This study introduces an enhanced industry-inspired benchmark environment that combines tasks from two existing benchmarks, SortingEnv and ContainerGym, into a sequential recycling scenario with sorting and pressing operations. We evaluate two control strategies: a modular architecture with specialized agents and a monolithic agent governing the full system, while also analyzing the impact of action masking. Our experiments show that without action masking, agents struggle to learn effective policies, with the modular architecture performing better. When action masking is applied, both architectures improve substantially, and the performance gap narrows considerably. These results highlight the decisive role of action space constraints and suggest that the advantages of specialization diminish as action complexity is reduced. The proposed benchmark thus provides a valuable testbed for exploring practical and robust multi-agent RL solutions in industrial automation, while contributing to the ongoing debate on centralization versus specialization.",
    "authors": [
      "Tom Maus",
      "Asma Atamna",
      "Tobias Glasmachers"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA",
      "cs.SY",
      "eess.SY"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20408v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20408v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2510.20375v1",
    "title": "The Impact of Negated Text on Hallucination with Large Language Models",
    "summary": "Recent studies on hallucination in large language models (LLMs) have been actively progressing in natural language processing. However, the impact of negated text on hallucination with LLMs remains largely unexplored. In this paper, we set three important yet unanswered research questions and aim to address them. To derive the answers, we investigate whether LLMs can recognize contextual shifts caused by negation and still reliably distinguish hallucinations comparable to affirmative cases. We also design the NegHalu dataset by reconstructing existing hallucination detection datasets with negated expressions. Our experiments demonstrate that LLMs struggle to detect hallucinations in negated text effectively, often producing logically inconsistent or unfaithful judgments. Moreover, we trace the internal state of LLMs as they process negated inputs at the token level and reveal the challenges of mitigating their unintended effects.",
    "authors": [
      "Jaehyung Seo",
      "Hyeonseok Moon",
      "Heuiseok Lim"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20375v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20375v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2510.20351v1",
    "title": "Evaluating Latent Knowledge of Public Tabular Datasets in Large Language   Models",
    "summary": "Large Language Models (LLMs) are increasingly evaluated on their ability to reason over structured data, yet such assessments often overlook a crucial confound: dataset contamination. In this work, we investigate whether LLMs exhibit prior knowledge of widely used tabular benchmarks such as Adult Income, Titanic, and others. Through a series of controlled probing experiments, we reveal that contamination effects emerge exclusively for datasets containing strong semantic cues-for instance, meaningful column names or interpretable value categories. In contrast, when such cues are removed or randomized, performance sharply declines to near-random levels. These findings suggest that LLMs' apparent competence on tabular reasoning tasks may, in part, reflect memorization of publicly available datasets rather than genuine generalization. We discuss implications for evaluation protocols and propose strategies to disentangle semantic leakage from authentic reasoning ability in future LLM assessments.",
    "authors": [
      "Matteo Silvestri",
      "Flavio Giorgi",
      "Fabrizio Silvestri",
      "Gabriele Tolomei"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20351v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20351v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2510.20808v1",
    "title": "The Reality Gap in Robotics: Challenges, Solutions, and Best Practices",
    "summary": "Machine learning has facilitated significant advancements across various robotics domains, including navigation, locomotion, and manipulation. Many such achievements have been driven by the extensive use of simulation as a critical tool for training and testing robotic systems prior to their deployment in real-world environments. However, simulations consist of abstractions and approximations that inevitably introduce discrepancies between simulated and real environments, known as the reality gap. These discrepancies significantly hinder the successful transfer of systems from simulation to the real world. Closing this gap remains one of the most pressing challenges in robotics. Recent advances in sim-to-real transfer have demonstrated promising results across various platforms, including locomotion, navigation, and manipulation. By leveraging techniques such as domain randomization, real-to-sim transfer, state and action abstractions, and sim-real co-training, many works have overcome the reality gap. However, challenges persist, and a deeper understanding of the reality gap's root causes and solutions is necessary. In this survey, we present a comprehensive overview of the sim-to-real landscape, highlighting the causes, solutions, and evaluation metrics for the reality gap and sim-to-real transfer.",
    "authors": [
      "Elie Aljalbout",
      "Jiaxu Xing",
      "Angel Romero",
      "Iretiayo Akinola",
      "Caelan Reed Garrett",
      "Eric Heiden",
      "Abhishek Gupta",
      "Tucker Hermans",
      "Yashraj Narang",
      "Dieter Fox",
      "Davide Scaramuzza",
      "Fabio Ramos"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "stat.ML",
      "I.2.6; I.2.8; I.2.9"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20808v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20808v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2510.20784v1",
    "title": "A Coherence-Based Measure of AGI",
    "summary": "Recent work by \\citet{hendrycks2025agidefinition} formalized \\textit{Artificial General Intelligence} (AGI) as the arithmetic mean of proficiencies across cognitive domains derived from the Cattell--Horn--Carroll (CHC) model of human cognition. While elegant, this definition assumes \\textit{compensability} -- that exceptional ability in some domains can offset failure in others. True general intelligence, however, should reflect \\textit{coherent sufficiency}: balanced competence across all essential domains. We propose a coherence-aware measure of AGI based on the integral of generalized means over a continuum of compensability exponents. This formulation spans arithmetic, geometric, and harmonic regimes, and the resulting \\textit{area under the curve} (AUC) quantifies robustness under varying compensability assumptions. Unlike the arithmetic mean, which rewards specialization, the AUC penalizes imbalance and captures inter-domain dependency. Applied to published CHC-based domain scores for GPT-4 and GPT-5, the coherence-adjusted AUC reveals that both systems remain far from general competence despite high arithmetic scores (e.g., GPT-5 at~24\\%). Integrating the generalized mean thus yields a principled, interpretable, and stricter foundation for measuring genuine progress toward AGI.",
    "authors": [
      "Fares Fourati"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20784v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20784v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2510.20721v1",
    "title": "User Perceptions of Privacy and Helpfulness in LLM Responses to   Privacy-Sensitive Scenarios",
    "summary": "Large language models (LLMs) have seen rapid adoption for tasks such as drafting emails, summarizing meetings, and answering health questions. In such uses, users may need to share private information (e.g., health records, contact details). To evaluate LLMs' ability to identify and redact such private information, prior work developed benchmarks (e.g., ConfAIde, PrivacyLens) with real-life scenarios. Using these benchmarks, researchers have found that LLMs sometimes fail to keep secrets private when responding to complex tasks (e.g., leaking employee salaries in meeting summaries). However, these evaluations rely on LLMs (proxy LLMs) to gauge compliance with privacy norms, overlooking real users' perceptions. Moreover, prior work primarily focused on the privacy-preservation quality of responses, without investigating nuanced differences in helpfulness. To understand how users perceive the privacy-preservation quality and helpfulness of LLM responses to privacy-sensitive scenarios, we conducted a user study with 94 participants using 90 scenarios from PrivacyLens. We found that, when evaluating identical responses to the same scenario, users showed low agreement with each other on the privacy-preservation quality and helpfulness of the LLM response. Further, we found high agreement among five proxy LLMs, while each individual LLM had low correlation with users' evaluations. These results indicate that the privacy and helpfulness of LLM responses are often specific to individuals, and proxy LLMs are poor estimates of how real users would perceive these responses in privacy-sensitive scenarios. Our results suggest the need to conduct user-centered studies on measuring LLMs' ability to help users while preserving privacy. Additionally, future research could investigate ways to improve the alignment between proxy LLMs and users for better estimation of users' perceived privacy and utility.",
    "authors": [
      "Xiaoyuan Wu",
      "Roshni Kaushik",
      "Wenkai Li",
      "Lujo Bauer",
      "Koichi Onoue"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20721v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20721v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2510.20567v1",
    "title": "Beyond Retrieval-Ranking: A Multi-Agent Cognitive Decision Framework for   E-Commerce Search",
    "summary": "The retrieval-ranking paradigm has long dominated e-commerce search, but its reliance on query-item matching fundamentally misaligns with multi-stage cognitive decision processes of platform users. This misalignment introduces critical limitations: semantic gaps in complex queries, high decision costs due to cross-platform information foraging, and the absence of professional shopping guidance. To address these issues, we propose a Multi-Agent Cognitive Decision Framework (MACDF), which shifts the paradigm from passive retrieval to proactive decision support. Extensive offline evaluations demonstrate MACDF's significant improvements in recommendation accuracy and user satisfaction, particularly for complex queries involving negation, multi-constraint, or reasoning demands. Online A/B testing on JD search platform confirms its practical efficacy. This work highlights the transformative potential of multi-agent cognitive systems in redefining e-commerce search.",
    "authors": [
      "Zhouwei Zhai",
      "Mengxiang Chen",
      "Haoyun Xia",
      "Jin Li",
      "Renquan Zhou",
      "Min Yang"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20567v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20567v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2510.20505v1",
    "title": "Hierarchical Sequence Iteration for Heterogeneous Question Answering",
    "summary": "Retrieval-augmented generation (RAG) remains brittle on multi-step questions and heterogeneous evidence sources, trading accuracy against latency and token/tool budgets. This paper introducesHierarchical Sequence (HSEQ) Iteration for Heterogeneous Question Answering, a unified framework that (i) linearize documents, tables, and knowledge graphs into a reversible hierarchical sequence with lightweight structural tags, and (ii) perform structure-aware iteration to collect just-enough evidence before answer synthesis. A Head Agent provides guidance that leads retrieval, while an Iteration Agent selects and expands HSeq via structure-respecting actions (e.g., parent/child hops, table row/column neighbors, KG relations); Finally the head agent composes canonicalized evidence to genearte the final answer, with an optional refinement loop to resolve detected contradictions. Experiments on HotpotQA (text), HybridQA/TAT-QA (table+text), and MetaQA (KG) show consistent EM/F1 gains over strong single-pass, multi-hop, and agentic RAG baselines with high efficiency. Besides, HSEQ exhibits three key advantages: (1) a format-agnostic unification that enables a single policy to operate across text, tables, and KGs without per-dataset specialization; (2) guided, budget-aware iteration that reduces unnecessary hops, tool calls, and tokens while preserving accuracy; and (3) evidence canonicalization for reliable QA, improving answers consistency and auditability.",
    "authors": [
      "Ruiyi Yang",
      "Hao Xue",
      "Imran Razzak",
      "Hakim Hacid",
      "Flora D. Salim"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20505v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20505v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2510.20469v1",
    "title": "Structures generated in a multiagent system performing information   fusion in peer-to-peer resource-constrained networks",
    "summary": "There has recently been a major advance with respect to how information fusion is performed. Information fusion has gone from being conceived as a purely hierarchical procedure, as is the case of traditional military applications, to now being regarded collaboratively, as holonic fusion, which is better suited for civil applications and edge organizations. The above paradigm shift is being boosted as information fusion gains ground in different non-military areas, and human-computer and machine-machine communications, where holarchies, which are more flexible structures than ordinary, static hierarchies, become more widespread. This paper focuses on showing how holonic structures tend to be generated when there are constraints on resources (energy, available messages, time, etc.) for interactions based on a set of fully intercommunicating elements (peers) whose components fuse information as a means of optimizing the impact of vagueness and uncertainty present message exchanges. Holon formation is studied generically based on a multiagent system model, and an example of its possible operation is shown. Holonic structures have a series of advantages, such as adaptability, to sudden changes in the environment or its composition, are somewhat autonomous and are capable of cooperating in order to achieve a common goal. This can be useful when the shortage of resources prevents communications or when the system components start to fail.",
    "authors": [
      "Horacio Paggi",
      "Juan A. Lara",
      "Javier Soriano"
    ],
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20469v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20469v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2510.20431v1",
    "title": "Partial Optimality in Cubic Correlation Clustering for General Graphs",
    "summary": "The higher-order correlation clustering problem for a graph $G$ and costs associated with cliques of $G$ consists in finding a clustering of $G$ so as to minimize the sum of the costs of those cliques whose nodes all belong to the same cluster. To tackle this NP-hard problem in practice, local search heuristics have been proposed and studied in the context of applications. Here, we establish partial optimality conditions for cubic correlation clustering, i.e., for the special case of at most 3-cliques. We define and implement algorithms for deciding these conditions and examine their effectiveness numerically, on two data sets.",
    "authors": [
      "David Stein",
      "Bjoern Andres",
      "Silvia Di Gregorio"
    ],
    "categories": [
      "cs.DM",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20431v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20431v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2510.20416v1",
    "title": "Learning Coupled Earth System Dynamics with GraphDOP",
    "summary": "Interactions between different components of the Earth System (e.g. ocean, atmosphere, land and cryosphere) are a crucial driver of global weather patterns. Modern Numerical Weather Prediction (NWP) systems typically run separate models of the different components, explicitly coupled across their interfaces to additionally model exchanges between the different components. Accurately representing these coupled interactions remains a major scientific and technical challenge of weather forecasting. GraphDOP is a graph-based machine learning model that learns to forecast weather directly from raw satellite and in-situ observations, without reliance on reanalysis products or traditional physics-based NWP models. GraphDOP simultaneously embeds information from diverse observation sources spanning the full Earth system into a shared latent space. This enables predictions that implicitly capture cross-domain interactions in a single model without the need for any explicit coupling. Here we present a selection of case studies which illustrate the capability of GraphDOP to forecast events where coupled processes play a particularly key role. These include rapid sea-ice freezing in the Arctic, mixing-induced ocean surface cooling during Hurricane Ian and the severe European heat wave of 2022. The results suggest that learning directly from Earth System observations can successfully characterise and propagate cross-component interactions, offering a promising path towards physically consistent end-to-end data-driven Earth System prediction with a single model.",
    "authors": [
      "Eulalie Boucher",
      "Mihai Alexe",
      "Peter Lean",
      "Ewan Pinnington",
      "Simon Lang",
      "Patrick Laloyaux",
      "Lorenzo Zampieri",
      "Patricia de Rosnay",
      "Niels Bormann",
      "Anthony McNally"
    ],
    "categories": [
      "physics.ao-ph",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20416v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20416v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2510.20549v1",
    "title": "Deep Learning-Powered Visual SLAM Aimed at Assisting Visually Impaired   Navigation",
    "summary": "Despite advancements in SLAM technologies, robust operation under challenging conditions such as low-texture, motion-blur, or challenging lighting remains an open challenge. Such conditions are common in applications such as assistive navigation for the visually impaired. These challenges undermine localization accuracy and tracking stability, reducing navigation reliability and safety. To overcome these limitations, we present SELM-SLAM3, a deep learning-enhanced visual SLAM framework that integrates SuperPoint and LightGlue for robust feature extraction and matching. We evaluated our framework using TUM RGB-D, ICL-NUIM, and TartanAir datasets, which feature diverse and challenging scenarios. SELM-SLAM3 outperforms conventional ORB-SLAM3 by an average of 87.84% and exceeds state-of-the-art RGB-D SLAM systems by 36.77%. Our framework demonstrates enhanced performance under challenging conditions, such as low-texture scenes and fast motion, providing a reliable platform for developing navigation aids for the visually impaired.",
    "authors": [
      "Marziyeh Bamdad",
      "Hans-Peter Hutter",
      "Alireza Darvishy"
    ],
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20549v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20549v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.51
  },
  {
    "arxiv_id": "2510.20381v1",
    "title": "VLSP 2025 MLQA-TSR Challenge: Vietnamese Multimodal Legal Question   Answering on Traffic Sign Regulation",
    "summary": "This paper presents the VLSP 2025 MLQA-TSR - the multimodal legal question answering on traffic sign regulation shared task at VLSP 2025. VLSP 2025 MLQA-TSR comprises two subtasks: multimodal legal retrieval and multimodal question answering. The goal is to advance research on Vietnamese multimodal legal text processing and to provide a benchmark dataset for building and evaluating intelligent systems in multimodal legal domains, with a focus on traffic sign regulation in Vietnam. The best-reported results on VLSP 2025 MLQA-TSR are an F2 score of 64.55% for multimodal legal retrieval and an accuracy of 86.30% for multimodal question answering.",
    "authors": [
      "Son T. Luu",
      "Trung Vo",
      "Hiep Nguyen",
      "Khanh Quoc Tran",
      "Kiet Van Nguyen",
      "Vu Tran",
      "Ngan Luu-Thuy Nguyen",
      "Le-Minh Nguyen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20381v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20381v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.51
  },
  {
    "arxiv_id": "2510.20335v1",
    "title": "Dino-Diffusion Modular Designs Bridge the Cross-Domain Gap in Autonomous   Parking",
    "summary": "Parking is a critical pillar of driving safety. While recent end-to-end (E2E) approaches have achieved promising in-domain results, robustness under domain shifts (e.g., weather and lighting changes) remains a key challenge. Rather than relying on additional data, in this paper, we propose Dino-Diffusion Parking (DDP), a domain-agnostic autonomous parking pipeline that integrates visual foundation models with diffusion-based planning to enable generalized perception and robust motion planning under distribution shifts. We train our pipeline in CARLA at regular setting and transfer it to more adversarial settings in a zero-shot fashion. Our model consistently achieves a parking success rate above 90% across all tested out-of-distribution (OOD) scenarios, with ablation studies confirming that both the network architecture and algorithmic design significantly enhance cross-domain performance over existing baselines. Furthermore, testing in a 3D Gaussian splatting (3DGS) environment reconstructed from a real-world parking lot demonstrates promising sim-to-real transfer.",
    "authors": [
      "Zixuan Wu",
      "Hengyuan Zhang",
      "Ting-Hsuan Chen",
      "Yuliang Guo",
      "David Paz",
      "Xinyu Huang",
      "Liu Ren"
    ],
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20335v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20335v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.51
  },
  {
    "arxiv_id": "2510.20608v1",
    "title": "Convergence Analysis of SGD under Expected Smoothness",
    "summary": "Stochastic gradient descent (SGD) is the workhorse of large-scale learning, yet classical analyses rely on assumptions that can be either too strong (bounded variance) or too coarse (uniform noise). The expected smoothness (ES) condition has emerged as a flexible alternative that ties the second moment of stochastic gradients to the objective value and the full gradient. This paper presents a self-contained convergence analysis of SGD under ES. We (i) refine ES with interpretations and sampling-dependent constants; (ii) derive bounds of the expectation of squared full gradient norm; and (iii) prove $O(1/K)$ rates with explicit residual errors for various step-size schedules. All proofs are given in full detail in the appendix. Our treatment unifies and extends recent threads (Khaled and Richt\\'arik, 2020; Umeda and Iiduka, 2025).",
    "authors": [
      "Yuta Kawamoto",
      "Hideaki Iiduka"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20608v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20608v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2510.20332v1",
    "title": "Bias by Design? How Data Practices Shape Fairness in AI Healthcare   Systems",
    "summary": "Artificial intelligence (AI) holds great promise for transforming healthcare. However, despite significant advances, the integration of AI solutions into real-world clinical practice remains limited. A major barrier is the quality and fairness of training data, which is often compromised by biased data collection practices. This paper draws on insights from the AI4HealthyAging project, part of Spain's national R&D initiative, where our task was to detect biases during clinical data collection. We identify several types of bias across multiple use cases, including historical, representation, and measurement biases. These biases manifest in variables such as sex, gender, age, habitat, socioeconomic status, equipment, and labeling. We conclude with practical recommendations for improving the fairness and robustness of clinical problem design and data collection. We hope that our findings and experience contribute to guiding future projects in the development of fairer AI systems in healthcare.",
    "authors": [
      "Anna Arias-Duart",
      "Maria Eugenia Cardello",
      "Atia Cort\u00e9s"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20332v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20332v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2510.20794v1",
    "title": "Radar-Camera Fused Multi-Object Tracking: Online Calibration and Common   Feature",
    "summary": "This paper presents a Multi-Object Tracking (MOT) framework that fuses radar and camera data to enhance tracking efficiency while minimizing manual interventions. Contrary to many studies that underutilize radar and assign it a supplementary role--despite its capability to provide accurate range/depth information of targets in a world 3D coordinate system--our approach positions radar in a crucial role. Meanwhile, this paper utilizes common features to enable online calibration to autonomously associate detections from radar and camera. The main contributions of this work include: (1) the development of a radar-camera fusion MOT framework that exploits online radar-camera calibration to simplify the integration of detection results from these two sensors, (2) the utilization of common features between radar and camera data to accurately derive real-world positions of detected objects, and (3) the adoption of feature matching and category-consistency checking to surpass the limitations of mere position matching in enhancing sensor association accuracy. To the best of our knowledge, we are the first to investigate the integration of radar-camera common features and their use in online calibration for achieving MOT. The efficacy of our framework is demonstrated by its ability to streamline the radar-camera mapping process and improve tracking precision, as evidenced by real-world experiments conducted in both controlled environments and actual traffic scenarios. Code is available at https://github.com/radar-lab/Radar_Camera_MOT",
    "authors": [
      "Lei Cheng",
      "Siyang Cao"
    ],
    "categories": [
      "cs.CV",
      "eess.SP"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20794v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20794v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.48
  },
  {
    "arxiv_id": "2510.20586v1",
    "title": "GenColorBench: A Color Evaluation Benchmark for Text-to-Image Generation   Models",
    "summary": "Recent years have seen impressive advances in text-to-image generation, with image generative or unified models producing high-quality images from text. Yet these models still struggle with fine-grained color controllability, often failing to accurately match colors specified in text prompts. While existing benchmarks evaluate compositional reasoning and prompt adherence, none systematically assess color precision. Color is fundamental to human visual perception and communication, critical for applications from art to design workflows requiring brand consistency. However, current benchmarks either neglect color or rely on coarse assessments, missing key capabilities such as interpreting RGB values or aligning with human expectations. To this end, we propose GenColorBench, the first comprehensive benchmark for text-to-image color generation, grounded in color systems like ISCC-NBS and CSS3/X11, including numerical colors which are absent elsewhere. With 44K color-focused prompts covering 400+ colors, it reveals models' true capabilities via perceptual and automated assessments. Evaluations of popular text-to-image models using GenColorBench show performance variations, highlighting which color conventions models understand best and identifying failure modes. Our GenColorBench assessments will guide improvements in precise color generation. The benchmark will be made public upon acceptance.",
    "authors": [
      "Muhammad Atif Butt",
      "Alexandra Gomez-Villa",
      "Tao Wu",
      "Javier Vazquez-Corral",
      "Joost Van De Weijer",
      "Kai Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20586v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20586v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.48
  },
  {
    "arxiv_id": "2510.20814v1",
    "title": "SpectraMorph: Structured Latent Learning for Self-Supervised   Hyperspectral Super-Resolution",
    "summary": "Hyperspectral sensors capture dense spectra per pixel but suffer from low spatial resolution, causing blurred boundaries and mixed-pixel effects. Co-registered companion sensors such as multispectral, RGB, or panchromatic cameras provide high-resolution spatial detail, motivating hyperspectral super-resolution through the fusion of hyperspectral and multispectral images (HSI-MSI). Existing deep learning based methods achieve strong performance but rely on opaque regressors that lack interpretability and often fail when the MSI has very few bands. We propose SpectraMorph, a physics-guided self-supervised fusion framework with a structured latent space. Instead of direct regression, SpectraMorph enforces an unmixing bottleneck: endmember signatures are extracted from the low-resolution HSI, and a compact multilayer perceptron predicts abundance-like maps from the MSI. Spectra are reconstructed by linear mixing, with training performed in a self-supervised manner via the MSI sensor's spectral response function. SpectraMorph produces interpretable intermediates, trains in under a minute, and remains robust even with a single-band (pan-chromatic) MSI. Experiments on synthetic and real-world datasets show SpectraMorph consistently outperforming state-of-the-art unsupervised/self-supervised baselines while remaining very competitive against supervised baselines.",
    "authors": [
      "Ritik Shah",
      "Marco F Duarte"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20814v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20814v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.47
  },
  {
    "arxiv_id": "2510.20404v1",
    "title": "Identification and Debiased Learning of Causal Effects with General   Instrumental Variables",
    "summary": "Instrumental variable methods are fundamental to causal inference when treatment assignment is confounded by unobserved variables. In this article, we develop a general nonparametric framework for identification and learning with multi-categorical or continuous instrumental variables. Specifically, we propose an additive instrumental variable framework to identify mean potential outcomes and the average treatment effect with a weighting function. Leveraging semiparametric theory, we derive efficient influence functions and construct consistent, asymptotically normal estimators via debiased machine learning. Extensions to longitudinal data, dynamic treatment regimes, and multiplicative instrumental variables are further developed. We demonstrate the proposed method by employing simulation studies and analyzing real data from the Job Training Partnership Act program.",
    "authors": [
      "Shuyuan Chen",
      "Peng Zhang",
      "Yifan Cui"
    ],
    "categories": [
      "stat.ME",
      "econ.EM",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20404v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20404v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.47
  },
  {
    "arxiv_id": "2510.20551v1",
    "title": "Time-series Random Process Complexity Ranking Using a Bound on   Conditional Differential Entropy",
    "summary": "Conditional differential entropy provides an intuitive measure for relatively ranking time-series complexity by quantifying uncertainty in future observations given past context. However, its direct computation for high-dimensional processes from unknown distributions is often intractable. This paper builds on the information theoretic prediction error bounds established by Fang et al. \\cite{fang2019generic}, which demonstrate that the conditional differential entropy \\textbf{$h(X_k \\mid X_{k-1},...,X_{k-m})$} is upper bounded by a function of the determinant of the covariance matrix of next-step prediction errors for any next step prediction model. We add to this theoretical framework by further increasing this bound by leveraging Hadamard's inequality and the positive semi-definite property of covariance matrices.   To see if these bounds can be used to rank the complexity of time series, we conducted two synthetic experiments: (1) controlled linear autoregressive processes with additive Gaussian noise, where we compare ordinary least squares prediction error entropy proxies to the true entropies of various additive noises, and (2) a complexity ranking task of bio-inspired synthetic audio data with unknown entropy, where neural network prediction errors are used to recover the known complexity ordering.   This framework provides a computationally tractable method for time-series complexity ranking using prediction errors from next-step prediction models, that maintains a theoretical foundation in information theory.",
    "authors": [
      "Jacob Ayers",
      "Richard Hahnloser",
      "Julia Ulrich",
      "Lothar Sebastian Krapp",
      "Remo Nitschke",
      "Sabine Stoll",
      "Balthasar Bickel",
      "Reinhard Furrer"
    ],
    "categories": [
      "eess.SP",
      "cs.IT",
      "eess.AS",
      "math.IT",
      "stat.ME",
      "stat.ML"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20551v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20551v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.44
  },
  {
    "arxiv_id": "2510.20550v1",
    "title": "From Cheap to Pro: A Learning-based Adaptive Camera Parameter Network   for Professional-Style Imaging",
    "summary": "Consumer-grade camera systems often struggle to maintain stable image quality under complex illumination conditions such as low light, high dynamic range, and backlighting, as well as spatial color temperature variation. These issues lead to underexposure, color casts, and tonal inconsistency, which degrade the performance of downstream vision tasks. To address this, we propose ACamera-Net, a lightweight and scene-adaptive camera parameter adjustment network that directly predicts optimal exposure and white balance from RAW inputs. The framework consists of two modules: ACamera-Exposure, which estimates ISO to alleviate underexposure and contrast loss, and ACamera-Color, which predicts correlated color temperature and gain factors for improved color consistency. Optimized for real-time inference on edge devices, ACamera-Net can be seamlessly integrated into imaging pipelines. Trained on diverse real-world data with annotated references, the model generalizes well across lighting conditions. Extensive experiments demonstrate that ACamera-Net consistently enhances image quality and stabilizes perception outputs, outperforming conventional auto modes and lightweight baselines without relying on additional image enhancement modules.",
    "authors": [
      "Fuchen Li",
      "Yansong Du",
      "Wenbo Cheng",
      "Xiaoxia Zhou",
      "Sen Yin"
    ],
    "categories": [
      "cs.CV",
      "cs.CV",
      "I.4.3; I.4.8; I.2.10"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20550v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20550v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.44
  },
  {
    "arxiv_id": "2510.20451v1",
    "title": "On Multiple Robustness of Proximal Dynamic Treatment Regimes",
    "summary": "Dynamic treatment regimes are sequential decision rules that adapt treatment according to individual time-varying characteristics and outcomes to achieve optimal effects, with applications in precision medicine, personalized recommendations, and dynamic marketing. Estimating optimal dynamic treatment regimes via sequential randomized trials might face costly and ethical hurdles, often necessitating the use of historical observational data. In this work, we utilize proximal causal inference framework for learning optimal dynamic treatment regimes when the unconfoundedness assumption fails. Our contributions are four-fold: (i) we propose three nonparametric identification methods for optimal dynamic treatment regimes; (ii) we establish the semiparametric efficiency bound for the value function of a given regime; (iii) we propose a (K+1)-robust method for learning optimal dynamic treatment regimes, where K is the number of stages; (iv) as a by-product for marginal structural models, we establish identification and estimation of counterfactual means under a static regime. Numerical experiments validate the efficiency and multiple robustness of our proposed methods.",
    "authors": [
      "Yuanshan Gao",
      "Yang Bai",
      "Yifan Cui"
    ],
    "categories": [
      "stat.ME",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20451v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20451v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.44
  },
  {
    "arxiv_id": "2510.20558v1",
    "title": "From Far and Near: Perceptual Evaluation of Crowd Representations Across   Levels of Detail",
    "summary": "In this paper, we investigate how users perceive the visual quality of crowd character representations at different levels of detail (LoD) and viewing distances. Each representation: geometric meshes, image-based impostors, Neural Radiance Fields (NeRFs), and 3D Gaussians, exhibits distinct trade-offs between visual fidelity and computational performance. Our qualitative and quantitative results provide insights to guide the design of perceptually optimized LoD strategies for crowd rendering.",
    "authors": [
      "Xiaohan Sun",
      "Carol O'Sullivan"
    ],
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.HC"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20558v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20558v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.4
  },
  {
    "model_id": "EpistemeAI/metatune-gpt20b",
    "author": "unknown",
    "title": "EpistemeAI/metatune-gpt20b",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "gpt_oss",
      "text-generation",
      "text-generation-inference",
      "unsloth",
      "conversational",
      "en",
      "base_model:unsloth/gpt-oss-20b-unsloth-bnb-4bit",
      "base_model:quantized:unsloth/gpt-oss-20b-unsloth-bnb-4bit",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "8-bit",
      "mxfp4",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "transformers",
    "created_at": "2025-10-24T18:05:10.000Z",
    "last_modified": "2025-10-24T20:07:36.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/EpistemeAI/metatune-gpt20b",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "asadkhan56/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-muscular_lightfooted_cockroach",
    "author": "unknown",
    "title": "asadkhan56/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-muscular_lightfooted_cockroach",
    "downloads": 489,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "rl-swarm",
      "genrl-swarm",
      "grpo",
      "gensyn",
      "I am muscular_lightfooted_cockroach",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "transformers",
    "created_at": "2025-10-22T20:11:39.000Z",
    "last_modified": "2025-10-24T20:07:19.000Z",
    "days_since_creation": 1,
    "url": "https://huggingface.co/asadkhan56/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-muscular_lightfooted_cockroach",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "sunemo/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-long_unseen_beaver",
    "author": "unknown",
    "title": "sunemo/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-long_unseen_beaver",
    "downloads": 937,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "rl-swarm",
      "genrl-swarm",
      "grpo",
      "gensyn",
      "I am long_unseen_beaver",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "transformers",
    "created_at": "2025-10-23T10:07:00.000Z",
    "last_modified": "2025-10-24T20:06:56.000Z",
    "days_since_creation": 1,
    "url": "https://huggingface.co/sunemo/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-long_unseen_beaver",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "usr256864/ee_gol",
    "author": "unknown",
    "title": "usr256864/ee_gol",
    "downloads": 39,
    "likes": 0,
    "tags": [
      "peft",
      "safetensors",
      "base_model:adapter:HiTZ/GoLLIE-7B",
      "grpo",
      "lora",
      "transformers",
      "trl",
      "text-generation",
      "arxiv:1910.09700",
      "base_model:HiTZ/GoLLIE-7B",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "peft",
    "created_at": "2025-10-23T15:16:31.000Z",
    "last_modified": "2025-10-24T20:06:42.000Z",
    "days_since_creation": 1,
    "url": "https://huggingface.co/usr256864/ee_gol",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "pittawat/qwen2.5-7b-instruct-new-math-1k-hard-grpo-cot-prompt",
    "author": "unknown",
    "title": "pittawat/qwen2.5-7b-instruct-new-math-1k-hard-grpo-cot-prompt",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "safetensors",
      "qwen2",
      "region:us"
    ],
    "pipeline_tag": "",
    "library": "",
    "created_at": "2025-10-24T20:04:59.000Z",
    "last_modified": "2025-10-24T20:06:40.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/pittawat/qwen2.5-7b-instruct-new-math-1k-hard-grpo-cot-prompt",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "DervlexVenice/chinese_swordswoman_lora-style-1.5",
    "author": "unknown",
    "title": "DervlexVenice/chinese_swordswoman_lora-style-1.5",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "lora",
      "text-to-image",
      "base_model:runwayml/stable-diffusion-v1-5",
      "base_model:adapter:runwayml/stable-diffusion-v1-5",
      "license:openrail++",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "library": "",
    "created_at": "2025-10-24T20:06:33.000Z",
    "last_modified": "2025-10-24T20:06:39.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/DervlexVenice/chinese_swordswoman_lora-style-1.5",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "Marcus-KO/ModernBERT-distil-optuna-clinc",
    "author": "unknown",
    "title": "Marcus-KO/ModernBERT-distil-optuna-clinc",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "modernbert",
      "text-classification",
      "generated_from_trainer",
      "base_model:answerdotai/ModernBERT-base",
      "base_model:finetune:answerdotai/ModernBERT-base",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-classification",
    "library": "transformers",
    "created_at": "2025-10-24T19:50:33.000Z",
    "last_modified": "2025-10-24T20:06:31.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/Marcus-KO/ModernBERT-distil-optuna-clinc",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "DervlexVenice/mix_dlc_for_fish_mix_23_3_14-style-1.5",
    "author": "unknown",
    "title": "DervlexVenice/mix_dlc_for_fish_mix_23_3_14-style-1.5",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "lora",
      "text-to-image",
      "base_model:runwayml/stable-diffusion-v1-5",
      "base_model:adapter:runwayml/stable-diffusion-v1-5",
      "license:openrail++",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "library": "",
    "created_at": "2025-10-24T20:06:25.000Z",
    "last_modified": "2025-10-24T20:06:28.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/DervlexVenice/mix_dlc_for_fish_mix_23_3_14-style-1.5",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "DervlexVenice/realistic_cum_illustrious-concept-illustrious",
    "author": "unknown",
    "title": "DervlexVenice/realistic_cum_illustrious-concept-illustrious",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "lora",
      "text-to-image",
      "nsfw",
      "license:openrail++",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "library": "",
    "created_at": "2025-10-24T20:06:17.000Z",
    "last_modified": "2025-10-24T20:06:22.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/DervlexVenice/realistic_cum_illustrious-concept-illustrious",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "IVDymer/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-polished_bold_anaconda",
    "author": "unknown",
    "title": "IVDymer/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-polished_bold_anaconda",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "rl-swarm",
      "genrl-swarm",
      "grpo",
      "gensyn",
      "I am polished_bold_anaconda",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "transformers",
    "created_at": "2025-10-24T18:23:46.000Z",
    "last_modified": "2025-10-24T20:06:12.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/IVDymer/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-polished_bold_anaconda",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "lukaskellerstein/tools-mistral-16bit-merged-unsloth",
    "author": "unknown",
    "title": "lukaskellerstein/tools-mistral-16bit-merged-unsloth",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "mistral",
      "text-generation",
      "text-generation-inference",
      "unsloth",
      "conversational",
      "en",
      "base_model:unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
      "base_model:finetune:unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "transformers",
    "created_at": "2025-10-24T20:02:36.000Z",
    "last_modified": "2025-10-24T20:06:02.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/lukaskellerstein/tools-mistral-16bit-merged-unsloth",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "walid0795/bert-Finetuned-IMDB",
    "author": "unknown",
    "title": "walid0795/bert-Finetuned-IMDB",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "bert",
      "text-classification",
      "generated_from_trainer",
      "base_model:google-bert/bert-base-uncased",
      "base_model:finetune:google-bert/bert-base-uncased",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-classification",
    "library": "transformers",
    "created_at": "2025-10-24T20:05:47.000Z",
    "last_modified": "2025-10-24T20:06:01.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/walid0795/bert-Finetuned-IMDB",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "rasmati96/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-vigilant_grazing_chimpanzee",
    "author": "unknown",
    "title": "rasmati96/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-vigilant_grazing_chimpanzee",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "rl-swarm",
      "genrl-swarm",
      "grpo",
      "gensyn",
      "I am vigilant_grazing_chimpanzee",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "transformers",
    "created_at": "2025-10-24T20:05:47.000Z",
    "last_modified": "2025-10-24T20:05:58.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/rasmati96/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-vigilant_grazing_chimpanzee",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "DervlexVenice/frieren_sousou_no_frieren_lora-style-1.5",
    "author": "unknown",
    "title": "DervlexVenice/frieren_sousou_no_frieren_lora-style-1.5",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "lora",
      "text-to-image",
      "base_model:runwayml/stable-diffusion-v1-5",
      "base_model:adapter:runwayml/stable-diffusion-v1-5",
      "license:openrail++",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "library": "",
    "created_at": "2025-10-24T20:05:54.000Z",
    "last_modified": "2025-10-24T20:05:57.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/DervlexVenice/frieren_sousou_no_frieren_lora-style-1.5",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "pawankumar20/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-snappy_pale_cobra",
    "author": "unknown",
    "title": "pawankumar20/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-snappy_pale_cobra",
    "downloads": 522,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "rl-swarm",
      "genrl-swarm",
      "grpo",
      "gensyn",
      "I am snappy_pale_cobra",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "transformers",
    "created_at": "2025-10-22T20:31:11.000Z",
    "last_modified": "2025-10-24T20:05:55.000Z",
    "days_since_creation": 1,
    "url": "https://huggingface.co/pawankumar20/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-snappy_pale_cobra",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "gambit16v/farm8",
    "author": "unknown",
    "title": "gambit16v/farm8",
    "downloads": 5773,
    "likes": 0,
    "tags": [
      "safetensors",
      "llama",
      "region:us"
    ],
    "pipeline_tag": "",
    "library": "",
    "created_at": "2025-10-23T22:50:31.000Z",
    "last_modified": "2025-10-24T20:05:53.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/gambit16v/farm8",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "DervlexVenice/luke_s_neon_clothing_and_accessories_illustrious_flux_pony_1.5_xl-concept-illustrious",
    "author": "unknown",
    "title": "DervlexVenice/luke_s_neon_clothing_and_accessories_illustrious_flux_pony_1.5_xl-concept-illustrious",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "lora",
      "text-to-image",
      "license:openrail++",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "library": "",
    "created_at": "2025-10-24T20:05:40.000Z",
    "last_modified": "2025-10-24T20:05:50.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/DervlexVenice/luke_s_neon_clothing_and_accessories_illustrious_flux_pony_1.5_xl-concept-illustrious",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "reichenbach/qwen_model_grpo_countdown_task_512_change_param",
    "author": "unknown",
    "title": "reichenbach/qwen_model_grpo_countdown_task_512_change_param",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "qwen3",
      "text-generation",
      "conversational",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "transformers",
    "created_at": "2025-10-24T20:05:11.000Z",
    "last_modified": "2025-10-24T20:05:39.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/reichenbach/qwen_model_grpo_countdown_task_512_change_param",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "tasikhan91/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-hoarse_dextrous_fox",
    "author": "unknown",
    "title": "tasikhan91/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-hoarse_dextrous_fox",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "rl-swarm",
      "genrl-swarm",
      "grpo",
      "gensyn",
      "I am hoarse_dextrous_fox",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "transformers",
    "created_at": "2025-10-24T20:05:18.000Z",
    "last_modified": "2025-10-24T20:05:29.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/tasikhan91/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-hoarse_dextrous_fox",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "DervlexVenice/waist_slider_xl_microwaist_xl-tool-sdxl",
    "author": "unknown",
    "title": "DervlexVenice/waist_slider_xl_microwaist_xl-tool-sdxl",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "lora",
      "text-to-image",
      "base_model:stabilityai/stable-diffusion-xl-base-1.0",
      "base_model:adapter:stabilityai/stable-diffusion-xl-base-1.0",
      "license:openrail++",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "library": "",
    "created_at": "2025-10-24T20:05:14.000Z",
    "last_modified": "2025-10-24T20:05:19.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/DervlexVenice/waist_slider_xl_microwaist_xl-tool-sdxl",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "yanghuattt/tha_JetBrains_Mellum-4b-base_24_ft_clm_best",
    "author": "unknown",
    "title": "yanghuattt/tha_JetBrains_Mellum-4b-base_24_ft_clm_best",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "llama",
      "text-generation",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "transformers",
    "created_at": "2025-10-24T19:05:47.000Z",
    "last_modified": "2025-10-24T20:05:15.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/yanghuattt/tha_JetBrains_Mellum-4b-base_24_ft_clm_best",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "DervlexVenice/chinchinpose-poses-1.5",
    "author": "unknown",
    "title": "DervlexVenice/chinchinpose-poses-1.5",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "lora",
      "text-to-image",
      "base_model:runwayml/stable-diffusion-v1-5",
      "base_model:adapter:runwayml/stable-diffusion-v1-5",
      "license:openrail++",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "library": "",
    "created_at": "2025-10-24T20:05:07.000Z",
    "last_modified": "2025-10-24T20:05:11.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/DervlexVenice/chinchinpose-poses-1.5",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "0xpop/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-burrowing_muscular_bison",
    "author": "unknown",
    "title": "0xpop/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-burrowing_muscular_bison",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "rl-swarm",
      "genrl-swarm",
      "grpo",
      "gensyn",
      "I am burrowing_muscular_bison",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "transformers",
    "created_at": "2025-10-24T06:50:15.000Z",
    "last_modified": "2025-10-24T20:05:10.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/0xpop/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-burrowing_muscular_bison",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "rajukhan8988/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-sniffing_aquatic_badger",
    "author": "unknown",
    "title": "rajukhan8988/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-sniffing_aquatic_badger",
    "downloads": 359,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "rl-swarm",
      "genrl-swarm",
      "grpo",
      "gensyn",
      "I am sniffing_aquatic_badger",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "transformers",
    "created_at": "2025-10-23T09:38:02.000Z",
    "last_modified": "2025-10-24T20:05:03.000Z",
    "days_since_creation": 1,
    "url": "https://huggingface.co/rajukhan8988/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-sniffing_aquatic_badger",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  }
]