[
  {
    "arxiv_id": "2510.23596v1",
    "title": "Think Twice: Branch-and-Rethink Reasoning Reward Model",
    "summary": "Large language models (LLMs) increasingly rely on thinking models that externalize intermediate steps and allocate extra test-time compute, with think-twice strategies showing that a deliberate second pass can elicit stronger reasoning. In contrast, most reward models (RMs) still compress many quality dimensions into a single scalar in one shot, a design that induces judgment diffusion: attention spreads across evaluation criteria, yielding diluted focus and shallow analysis. We introduce branch-and-rethink (BR-RM), a two-turn RM that transfers the think-twice principle to reward modeling. Turn 1 performs adaptive branching, selecting a small set of instance-critical dimensions (such as factuality and safety) and sketching concise, evidence-seeking hypotheses. Turn 2 executes branch-conditioned rethinking, a targeted reread that tests those hypotheses and scrutinizes only what matters most. We train with GRPO-style reinforcement learning over structured two-turn traces using a simple binary outcome reward with strict format checks, making the approach compatible with standard RLHF pipelines. By converting all-at-oncescoringintofocused, second-lookreasoning, BR-RMreducesjudgmentdiffusionandimproves sensitivity to subtle yet consequential errors while remaining practical and scalable. Experimental results demonstrate that our model achieves state-of-the-art performance on three challenging reward modeling benchmarks across diverse domains. The code and the model will be released soon.",
    "authors": [
      "Yizhu Jiao",
      "Jiaqi Zeng",
      "Julien Veron Vialard",
      "Oleksii Kuchaiev",
      "Jiawei Han",
      "Olivier Delalleau"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-10-27",
    "url": "https://arxiv.org/abs/2510.23596v1",
    "pdf_url": "https://arxiv.org/pdf/2510.23596v1.pdf",
    "date": "2025-10-28",
    "source": "arxiv",
    "research_score": 0.73
  },
  {
    "arxiv_id": "2510.23595v1",
    "title": "Multi-Agent Evolve: LLM Self-Improve through Co-evolution",
    "summary": "Reinforcement Learning (RL) has demonstrated significant potential in enhancing the reasoning capabilities of large language models (LLMs). However, the success of RL for LLMs heavily relies on human-curated datasets and verifiable rewards, which limit their scalability and generality. Recent Self-Play RL methods, inspired by the success of the paradigm in games and Go, aim to enhance LLM reasoning capabilities without human-annotated data. However, their methods primarily depend on a grounded environment for feedback (e.g., a Python interpreter or a game engine); extending them to general domains remains challenging. To address these challenges, we propose Multi-Agent Evolve (MAE), a framework that enables LLMs to self-evolve in solving diverse tasks, including mathematics, reasoning, and general knowledge Q&A. The core design of MAE is based on a triplet of interacting agents (Proposer, Solver, Judge) that are instantiated from a single LLM, and applies reinforcement learning to optimize their behaviors. The Proposer generates questions, the Solver attempts solutions, and the Judge evaluates both while co-evolving. Experiments on Qwen2.5-3B-Instruct demonstrate that MAE achieves an average improvement of 4.54% on multiple benchmarks. These results highlight MAE as a scalable, data-efficient method for enhancing the general reasoning abilities of LLMs with minimal reliance on human-curated supervision.",
    "authors": [
      "Yixing Chen",
      "Yiding Wang",
      "Siqi Zhu",
      "Haofei Yu",
      "Tao Feng",
      "Muhan Zhan",
      "Mostofa Patwary",
      "Jiaxuan You"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-27",
    "url": "https://arxiv.org/abs/2510.23595v1",
    "pdf_url": "https://arxiv.org/pdf/2510.23595v1.pdf",
    "date": "2025-10-28",
    "source": "arxiv",
    "research_score": 0.71
  },
  {
    "arxiv_id": "2510.23605v1",
    "title": "Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with   Progressive Texture Infilling",
    "summary": "Current 3D/4D generation methods are usually optimized for photorealism, efficiency, and aesthetics. However, they often fail to preserve the semantic identity of the subject across different viewpoints. Adapting generation methods with one or few images of a specific subject (also known as Personalization or Subject-driven generation) allows generating visual content that align with the identity of the subject. However, personalized 3D/4D generation is still largely underexplored. In this work, we introduce TIRE (Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation. It takes an initial 3D asset produced by an existing 3D generative model as input and uses video tracking to identify the regions that need to be modified. Then, we adopt a subject-driven 2D inpainting model for progressively infilling the identified regions. Finally, we resplat the modified 2D multi-view observations back to 3D while still maintaining consistency. Extensive experiments demonstrate that our approach significantly improves identity preservation in 3D/4D generation compared to state-of-the-art methods. Our project website is available at https://zsh2000.github.io/track-inpaint-resplat.github.io/.",
    "authors": [
      "Shuhong Zheng",
      "Ashkan Mirzaei",
      "Igor Gilitschenski"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG",
      "cs.RO"
    ],
    "published": "2025-10-27",
    "url": "https://arxiv.org/abs/2510.23605v1",
    "pdf_url": "https://arxiv.org/pdf/2510.23605v1.pdf",
    "date": "2025-10-28",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2510.23601v1",
    "title": "Alita-G: Self-Evolving Generative Agent for Agent Generation",
    "summary": "Large language models (LLMs) have been shown to perform better when scaffolded into agents with memory, tools, and feedback. Beyond this, self-evolving agents have emerged, but current work largely limits adaptation to prompt rewriting or failure retries. Therefore, we present ALITA-G, a self-evolution framework that transforms a general-purpose agent into a domain expert by systematically generating, abstracting, and curating Model Context Protocol (MCP) tools. In this framework, a generalist agent executes a curated suite of target-domain tasks and synthesizes candidate MCPs from successful trajectories. These are then abstracted to parameterized primitives and consolidated into an MCP Box. At inference time, ALITA-G performs retrieval-augmented MCP selection with the help of each tool's descriptions and use cases, before executing an agent equipped with the MCP Executor. Across several benchmarks GAIA, PathVQA, and Humanity's Last Exam, ALITA-G attains strong gains while reducing computation costs. On GAIA validation, it achieves 83.03% pass@1 and 89.09% pass@3, establishing a new state-of-the-art result while reducing mean tokens per example by approximately 15% relative to a strong baseline agent. ALITA-G thus provides a principled pathway from generalist capability to reusable, domain-specific competence, improving both accuracy and efficiency on complex reasoning tasks.",
    "authors": [
      "Jiahao Qiu",
      "Xuan Qi",
      "Hongru Wang",
      "Xinzhe Juan",
      "Yimin Wang",
      "Zelin Zhao",
      "Jiayi Geng",
      "Jiacheng Guo",
      "Peihang Li",
      "Jingzhe Shi",
      "Shilong Liu",
      "Mengdi Wang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-27",
    "url": "https://arxiv.org/abs/2510.23601v1",
    "pdf_url": "https://arxiv.org/pdf/2510.23601v1.pdf",
    "date": "2025-10-28",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2510.23606v1",
    "title": "Variational Masked Diffusion Models",
    "summary": "Masked diffusion models have recently emerged as a flexible framework for discrete generative modeling. However, a key limitation of standard masked diffusion is its inability to effectively capture dependencies among tokens that are predicted concurrently, leading to degraded generation quality when dependencies among tokens are important. To explicitly model dependencies among tokens, we propose Variational Masked Diffusion (VMD), a framework that introduces latent variables into the masked diffusion process. Through controlled experiments on synthetic datasets, we demonstrate that VMD successfully learns dependencies that conventional masked diffusion fails to capture. We further validate the effectiveness of our approach on Sudoku puzzles and text datasets, where learning of dependencies among tokens improves global consistency. Across these domains, VMD enhances both generation quality and dependency awareness, highlighting the value of integrating variational inference into masked diffusion. Our code is available at: https://riccizz.github.io/VMD.",
    "authors": [
      "Yichi Zhang",
      "Alex Schwing",
      "Zhizhen Zhao"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-10-27",
    "url": "https://arxiv.org/abs/2510.23606v1",
    "pdf_url": "https://arxiv.org/pdf/2510.23606v1.pdf",
    "date": "2025-10-28",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2510.23590v1",
    "title": "Lightweight Robust Direct Preference Optimization",
    "summary": "Direct Preference Optimization (DPO) has become a popular method for fine-tuning large language models (LLMs) due to its stability and simplicity. However, it is also known to be sensitive to noise in the data and prone to overfitting. Recent works have proposed using distributionally robust optimization (DRO) to address potential noise and distributional shift in the data. However, these methods often suffer from excessive conservatism and high computational cost. We propose DPO-PRO (DPO with Preference Robustness), a robust fine-tuning algorithm based on DPO which accounts for uncertainty in the preference distribution through a lightweight DRO formulation. Unlike prior DRO-based variants, DPO-PRO focuses solely on uncertainty in preferences, avoiding unnecessary conservatism and incurring negligible computational overhead. We further show that DPO-PRO is equivalent to a regularized DPO objective that penalizes model overconfidence under weak preference signals. We evaluate DPO-PRO on standard alignment benchmarks and a real-world public health task. Experimental results show that our method consistently improves robustness to noisy preference signals compared to existing DPO variants.",
    "authors": [
      "Cheol Woo Kim",
      "Shresth Verma",
      "Mauricio Tec",
      "Milind Tambe"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-27",
    "url": "https://arxiv.org/abs/2510.23590v1",
    "pdf_url": "https://arxiv.org/pdf/2510.23590v1.pdf",
    "date": "2025-10-28",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2510.23607v1",
    "title": "Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial   Representations",
    "summary": "Humans learn abstract concepts through multisensory synergy, and once formed, such representations can often be recalled from a single modality. Inspired by this principle, we introduce Concerto, a minimalist simulation of human concept learning for spatial cognition, combining 3D intra-modal self-distillation with 2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more coherent and informative spatial features, as demonstrated by zero-shot visualizations. It outperforms both standalone SOTA 2D and 3D self-supervised models by 14.2% and 4.8%, respectively, as well as their feature concatenation, in linear probing for 3D scene perception. With full fine-tuning, Concerto sets new SOTA results across multiple scene understanding benchmarks (e.g., 80.7% mIoU on ScanNet). We further present a variant of Concerto tailored for video-lifted point cloud spatial understanding, and a translator that linearly projects Concerto representations into CLIP's language space, enabling open-world perception. These results highlight that Concerto emerges spatial representations with superior fine-grained geometric and semantic consistency.",
    "authors": [
      "Yujia Zhang",
      "Xiaoyang Wu",
      "Yixing Lao",
      "Chengyao Wang",
      "Zhuotao Tian",
      "Naiyan Wang",
      "Hengshuang Zhao"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-27",
    "url": "https://arxiv.org/abs/2510.23607v1",
    "pdf_url": "https://arxiv.org/pdf/2510.23607v1.pdf",
    "date": "2025-10-28",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2510.23603v1",
    "title": "PixelRefer: A Unified Framework for Spatio-Temporal Object Referring   with Arbitrary Granularity",
    "summary": "Multimodal large language models (MLLMs) have demonstrated strong general-purpose capabilities in open-world visual comprehension. However, most existing MLLMs primarily focus on holistic, scene-level understanding, often overlooking the need for fine-grained, object-centric reasoning. In this paper, we present PixelRefer, a unified region-level MLLM framework that enables advanced fine-grained understanding over user-specified regions across both images and videos. Motivated by the observation that LLM attention predominantly focuses on object-level tokens, we propose a Scale-Adaptive Object Tokenizer (SAOT) to generate compact and semantically rich object representations from free-form regions. Our analysis reveals that global visual tokens contribute mainly in early LLM layers, inspiring the design of PixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion module to pre-fuse global context into object tokens. This yields a lightweight Object-Only Framework that substantially reduces computational cost while maintaining high semantic fidelity. To facilitate fine-grained instruction tuning, we curate PixelRefer-2.2M, a high-quality object-centric instruction dataset. Extensive experiments across a range of benchmarks validate that PixelRefer achieves leading performance with fewer training samples, while PixelRefer-Lite offers competitive accuracy with notable gains in efficiency.",
    "authors": [
      "Yuqian Yuan",
      "Wenqiao Zhang",
      "Xin Li",
      "Shihao Wang",
      "Kehan Li",
      "Wentong Li",
      "Jun Xiao",
      "Lei Zhang",
      "Beng Chin Ooi"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-27",
    "url": "https://arxiv.org/abs/2510.23603v1",
    "pdf_url": "https://arxiv.org/pdf/2510.23603v1.pdf",
    "date": "2025-10-28",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2510.23594v1",
    "title": "PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error   Detection",
    "summary": "We introduce \\textbf{PRISM-Bench}, a benchmark of puzzle-based visual challenges designed to evaluate not only whether models can solve problems, but how their reasoning unfolds. Unlike prior evaluations that measure only final-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual puzzle and a step-by-step chain-of-thought (CoT) containing exactly one error, models must identify the first incorrect step. This setting enables fine-grained assessment of logical consistency, error detection, and visual reasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric, and analogical reasoning, resisting shortcuts based on superficial pattern matching. Evaluations across state-of-the-art MLLMs reveal a persistent gap between fluent generation and faithful reasoning: models that produce plausible CoTs often fail to locate simple logical faults. By disentangling answer generation from reasoning verification, PRISM-Bench offers a sharper lens on multimodal reasoning competence and underscores the need for diagnostic evaluation protocols in the development of trustworthy MLLMs.",
    "authors": [
      "Yusu Qian",
      "Cheng Wan",
      "Chao Jia",
      "Yinfei Yang",
      "Qingyu Zhao",
      "Zhe Gan"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-27",
    "url": "https://arxiv.org/abs/2510.23594v1",
    "pdf_url": "https://arxiv.org/pdf/2510.23594v1.pdf",
    "date": "2025-10-28",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2510.23589v1",
    "title": "InFlux: A Benchmark for Self-Calibration of Dynamic Intrinsics of Video   Cameras",
    "summary": "Accurately tracking camera intrinsics is crucial for achieving 3D understanding from 2D video. However, most 3D algorithms assume that camera intrinsics stay constant throughout a video, which is often not true for many real-world in-the-wild videos. A major obstacle in this field is a lack of dynamic camera intrinsics benchmarks--existing benchmarks typically offer limited diversity in scene content and intrinsics variation, and none provide per-frame intrinsic changes for consecutive video frames. In this paper, we present Intrinsics in Flux (InFlux), a real-world benchmark that provides per-frame ground truth intrinsics annotations for videos with dynamic intrinsics. Compared to prior benchmarks, InFlux captures a wider range of intrinsic variations and scene diversity, featuring 143K+ annotated frames from 386 high-resolution indoor and outdoor videos with dynamic camera intrinsics. To ensure accurate per-frame intrinsics, we build a comprehensive lookup table of calibration experiments and extend the Kalibr toolbox to improve its accuracy and robustness. Using our benchmark, we evaluate existing baseline methods for predicting camera intrinsics and find that most struggle to achieve accurate predictions on videos with dynamic intrinsics. For the dataset, code, videos, and submission, please visit https://influx.cs.princeton.edu/.",
    "authors": [
      "Erich Liang",
      "Roma Bhattacharjee",
      "Sreemanti Dey",
      "Rafael Moschopoulos",
      "Caitlin Wang",
      "Michel Liao",
      "Grace Tan",
      "Andrew Wang",
      "Karhan Kayan",
      "Stamatis Alexandropoulos",
      "Jia Deng"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-27",
    "url": "https://arxiv.org/abs/2510.23589v1",
    "pdf_url": "https://arxiv.org/pdf/2510.23589v1.pdf",
    "date": "2025-10-28",
    "source": "arxiv",
    "research_score": 0.44
  },
  {
    "model_id": "jorgedelpozolerida/Llama-2-13b-chat-hf-Q8_0-GGUF",
    "author": "unknown",
    "title": "jorgedelpozolerida/Llama-2-13b-chat-hf-Q8_0-GGUF",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "gguf",
      "facebook",
      "meta",
      "pytorch",
      "llama",
      "llama-2",
      "llama-cpp",
      "gguf-my-repo",
      "text-generation",
      "en",
      "base_model:meta-llama/Llama-2-13b-chat-hf",
      "base_model:quantized:meta-llama/Llama-2-13b-chat-hf",
      "license:llama2",
      "endpoints_compatible",
      "region:us",
      "conversational"
    ],
    "pipeline_tag": "text-generation",
    "library": "",
    "created_at": "2025-10-28T16:19:36.000Z",
    "last_modified": "2025-10-28T16:20:29.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/jorgedelpozolerida/Llama-2-13b-chat-hf-Q8_0-GGUF",
    "date": "2025-10-28",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "lhkhiem28/zephyr-7b-sft-iter1",
    "author": "unknown",
    "title": "lhkhiem28/zephyr-7b-sft-iter1",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "transformers",
      "tensorboard",
      "safetensors",
      "qwen2",
      "text-generation",
      "generated_from_trainer",
      "trl",
      "hf_jobs",
      "sft",
      "conversational",
      "base_model:Qwen/Qwen2.5-3B-Instruct",
      "base_model:finetune:Qwen/Qwen2.5-3B-Instruct",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "transformers",
    "created_at": "2025-10-28T16:10:59.000Z",
    "last_modified": "2025-10-28T16:20:09.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/lhkhiem28/zephyr-7b-sft-iter1",
    "date": "2025-10-28",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "moyixiao/Qwen3-1.7B-gspo7-120",
    "author": "unknown",
    "title": "moyixiao/Qwen3-1.7B-gspo7-120",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "qwen3",
      "text-generation",
      "conversational",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "transformers",
    "created_at": "2025-10-28T16:18:20.000Z",
    "last_modified": "2025-10-28T16:20:02.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/moyixiao/Qwen3-1.7B-gspo7-120",
    "date": "2025-10-28",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "mlx-community/granite-4.0-1b-base-6bit",
    "author": "unknown",
    "title": "mlx-community/granite-4.0-1b-base-6bit",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "mlx",
      "safetensors",
      "granitemoehybrid",
      "language",
      "granite-4.0",
      "text-generation",
      "base_model:ibm-granite/granite-4.0-1b-base",
      "base_model:quantized:ibm-granite/granite-4.0-1b-base",
      "license:apache-2.0",
      "6-bit",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "mlx",
    "created_at": "2025-10-28T16:17:45.000Z",
    "last_modified": "2025-10-28T16:19:53.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/mlx-community/granite-4.0-1b-base-6bit",
    "date": "2025-10-28",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "jrkenny/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-tawny_wise_wombat",
    "author": "unknown",
    "title": "jrkenny/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-tawny_wise_wombat",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "rl-swarm",
      "genrl-swarm",
      "grpo",
      "gensyn",
      "I am tawny_wise_wombat",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "transformers",
    "created_at": "2025-10-28T15:13:06.000Z",
    "last_modified": "2025-10-28T16:19:37.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/jrkenny/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-tawny_wise_wombat",
    "date": "2025-10-28",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "colesmcintosh/llama-3.2-3B-gsm8k",
    "author": "unknown",
    "title": "colesmcintosh/llama-3.2-3B-gsm8k",
    "downloads": 24,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "llama",
      "text-generation",
      "text-generation-inference",
      "unsloth",
      "grpo",
      "gsm8k",
      "conversational",
      "en",
      "base_model:unsloth/Llama-3.2-3B-Instruct",
      "base_model:finetune:unsloth/Llama-3.2-3B-Instruct",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "transformers",
    "created_at": "2025-10-27T01:48:00.000Z",
    "last_modified": "2025-10-28T16:19:25.000Z",
    "days_since_creation": 1,
    "url": "https://huggingface.co/colesmcintosh/llama-3.2-3B-gsm8k",
    "date": "2025-10-28",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "hzmichaelvu/Qwen2.5-1.5B-Instruct-Gensyn-Swarm-stinging_mute_cheetah",
    "author": "unknown",
    "title": "hzmichaelvu/Qwen2.5-1.5B-Instruct-Gensyn-Swarm-stinging_mute_cheetah",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "rl-swarm",
      "genrl-swarm",
      "grpo",
      "gensyn",
      "I am stinging_mute_cheetah",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "transformers",
    "created_at": "2025-10-28T15:55:45.000Z",
    "last_modified": "2025-10-28T16:19:21.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/hzmichaelvu/Qwen2.5-1.5B-Instruct-Gensyn-Swarm-stinging_mute_cheetah",
    "date": "2025-10-28",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "danggia/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-pesty_ferocious_fish",
    "author": "unknown",
    "title": "danggia/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-pesty_ferocious_fish",
    "downloads": 564,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "rl-swarm",
      "genrl-swarm",
      "grpo",
      "gensyn",
      "I am pesty_ferocious_fish",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "transformers",
    "created_at": "2025-10-27T15:58:59.000Z",
    "last_modified": "2025-10-28T16:19:11.000Z",
    "days_since_creation": 1,
    "url": "https://huggingface.co/danggia/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-pesty_ferocious_fish",
    "date": "2025-10-28",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "bungamawar/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-armored_lively_albatross",
    "author": "unknown",
    "title": "bungamawar/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-armored_lively_albatross",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "rl-swarm",
      "genrl-swarm",
      "grpo",
      "gensyn",
      "I am armored_lively_albatross",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "transformers",
    "created_at": "2025-10-28T13:56:34.000Z",
    "last_modified": "2025-10-28T16:18:48.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/bungamawar/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-armored_lively_albatross",
    "date": "2025-10-28",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "jetx/sn38-v11-1",
    "author": "unknown",
    "title": "jetx/sn38-v11-1",
    "downloads": 1504,
    "likes": 0,
    "tags": [
      "safetensors",
      "llama",
      "region:us"
    ],
    "pipeline_tag": "",
    "library": "",
    "created_at": "2025-10-26T16:51:12.000Z",
    "last_modified": "2025-10-28T16:18:25.000Z",
    "days_since_creation": 1,
    "url": "https://huggingface.co/jetx/sn38-v11-1",
    "date": "2025-10-28",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "moyixiao/Qwen3-1.7B-gspo7-90",
    "author": "unknown",
    "title": "moyixiao/Qwen3-1.7B-gspo7-90",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "qwen3",
      "text-generation",
      "conversational",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "transformers",
    "created_at": "2025-10-28T16:16:29.000Z",
    "last_modified": "2025-10-28T16:18:11.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/moyixiao/Qwen3-1.7B-gspo7-90",
    "date": "2025-10-28",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "mlx-community/granite-4.0-1b-base-5bit",
    "author": "unknown",
    "title": "mlx-community/granite-4.0-1b-base-5bit",
    "downloads": 0,
    "likes": 1,
    "tags": [
      "mlx",
      "safetensors",
      "granitemoehybrid",
      "language",
      "granite-4.0",
      "text-generation",
      "base_model:ibm-granite/granite-4.0-1b-base",
      "base_model:quantized:ibm-granite/granite-4.0-1b-base",
      "license:apache-2.0",
      "5-bit",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "mlx",
    "created_at": "2025-10-28T16:15:52.000Z",
    "last_modified": "2025-10-28T16:17:28.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/mlx-community/granite-4.0-1b-base-5bit",
    "date": "2025-10-28",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  }
]